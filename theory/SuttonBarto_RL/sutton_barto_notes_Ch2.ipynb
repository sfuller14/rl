{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1841a00e",
   "metadata": {},
   "source": [
    "# Summary of Notation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402c54e",
   "metadata": {},
   "source": [
    "(full notation in Appendix A)\n",
    "\n",
    "> Whenever you see the word \"value\", think \"action's Expected Value of reward\" (AEVOR). This is usually a $P(win)$ or (in the bandit case) probability-weighted-avg $R_t$, given that that action is taken. It can be either a \"True\" value (i.e. the population mean of the action's distribution) or an estimate (i.e. sample statistic) of it.\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| $\\Pr\\{X = x\\}$ | Probability that random variable $X$ takes value $x$ |\n",
    "| $X \\sim p$ | $X$ is drawn from distribution $p$ |\n",
    "| $\\mathbb{E}[X]$ | Expectation of random variable $X$ |\n",
    "| $\\arg\\max_a f(a)$ | Value of $a$ that maximizes $f(a)$ |\n",
    "| $\\alpha$, $\\beta$, $\\epsilon$ | Step-size, decay-rate, and exploration parameters |\n",
    "| $s, s'$ | States |\n",
    "| $a$ | Action |\n",
    "| $r$ | Reward |\n",
    "| $S, A(s), R$ | Set of states, available actions, rewards |\n",
    "| $t, T$ | Discrete time step, final step |\n",
    "| $S_t, A_t, R_t$ | State, action, and reward at time $t$ |\n",
    "| $\\pi$ | Policy (decision-making rule) |\n",
    "| $\\pi(s)$ | action taken in state $s$ under _deterministic_ $\\pi$ |\n",
    "| $\\pi(a \\mid s)$ | Probability of taking action $a$ in state $s$ under _stochastic_ $\\pi$ |\n",
    "| $G_t$ | Return from time $t$ |\n",
    "| $h$ | horizon (the timestep one looks up to in a forward view) |\n",
    "| $v_\\pi(s)$ | Value of state $s$ under policy $\\pi$ |\n",
    "| $q_\\pi(s, a)$ | Value of state-action pair $(s,a)$ under $\\pi$ |\n",
    "| $p(s', r \\mid s, a)$ | Transition dynamics |\n",
    "| $\\hat{v}(s, \\mathbf{w})$ | Approximate value function using weight vector $\\mathbf{w}$ |\n",
    "| $\\delta_t$ | Temporal-difference error at time $t$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ac0f7",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402856b7",
   "metadata": {},
   "source": [
    "# Part I: Tabular Solution Methods\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64caf4a",
   "metadata": {},
   "source": [
    "<img src=\"../img/6.png\" alt=\"tabularsolutionmethods\" width=\"40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc6fe0e",
   "metadata": {},
   "source": [
    "# Chapter 2: Multi-armed Bandits\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b4b44c",
   "metadata": {},
   "source": [
    "> The most important feature distinguishing reinforcement learning from other types of learning is that it **uses training information that evaluates the actions taken rather than instructs by giving correct actions**. This is what creates the need for active exploration, for an explicit search for good behavior. Purely evaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action possible. Purely instructive feedback, on the other hand, indicates the correct action to take, independently of the action actually taken....Evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken\n",
    "\n",
    "**Bandit problems** model decision-making under uncertainty with *no state transitions*.  \n",
    "Bandit problems in general are a special case of RL where there is only a single state.\n",
    "\n",
    "Bandit problems model a **nonassociative setting**, in which the agent must learn (from evaluative feedback) in just one situation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccd7c84",
   "metadata": {},
   "source": [
    "## 2.1 A $k$-Armed Bandit Problem\n",
    "---\n",
    "\n",
    "Unlike supervised learning, RL uses *evaluative* feedback—it tells you how good your chosen action was, but not what the best action would have been.\n",
    "\n",
    "- **Setting**: At each time $t$, choose one of $k$ actions (“arms”), then observe reward $R_t$ drawn from a stationary distribution for that arm. So:\n",
    "\t- $k$: The number of actions (arms) to choose from.\n",
    "\t- $A_t$: The action (arm) selected at timestep $t$.\n",
    "\t\t- This is a RV. The observation of $A_t$ (i.e. the actual arm chosen at $t$) is denoted $a$.\n",
    "\t- $R_t$: Reward received from $A_t$.\n",
    "- **Objective**: Maximize expected cumulative reward over time (e.g., $T=1000$ rounds).\n",
    "\t- In the \"k-armed slot machine\" example, the objective is to maximize your winnings by concentrating your actions on the best levers.\n",
    "- **Approach**: For each arm $a$ of the $k$ arms, maintain a \"$\\text{value}$ estimate\" $Q_t (a)$ equal to your estimate of the expected reward from choosing that arm.\n",
    "\t- $Q_t (a) = \\text{value}_{A_t = a} = \\hat{\\mathbb{E}}[R_t \\mid A_t = a]$\n",
    "- **True action value** (_Equation 2.0_): For an arbitrary action, the <u>ACTUAL</u> expected reward given that $a$ is selected. We don't know this (population parameter).\n",
    "  $$\n",
    "  q_*(a) = \\mathbb{E}[R_t \\mid A_t = a]\n",
    "  $$\n",
    "\t- If we knew the true action values, we'd just choose the highest $q_*(a)$ every time.\n",
    "\t- We don't. So we want to make $Q_t (a)$ as close to $q_*(a)$ as possible.\n",
    "- **Exploration vs. exploitation**:\n",
    "  - *Exploitation*: Choose the action you *currently* think is best.\n",
    "\t- Good for this step.\n",
    "  - *Exploration*: Try other actions to discover if they might be better.\n",
    "\t- Good for the long run.\n",
    "  - Balancing these is a *central challenge*—no single action achieves both at once.\n",
    "\t- E.g. say you have certainty on a greedy action's $value$ but there are several other actions with close-but-uncertain $value$s. Say the uncertainty is such that at least one of these other actions is probably actually better than the greedy action, but you don't know which one. \n",
    "\t\t- In this scenario, given the nature of the uncertainty - \n",
    "\t\t\t- If you have many timesteps left, you should probably explore.\n",
    "\t\t\t- If you don't, you should probably exploit.\n",
    "\t- In general, balancing explore vs. exploit = f($\\quad Q_t (a)\\quad \\forall \\quad a \\in K$, uncertainties associated with each $Q_t (a)$, and remaining timesteps)\n",
    "\t- <u>HOWEVER, most sophisticated methods for balancing explore vs. exploit make strong, unrealistic assumptions around distribution stationarity and prior knowledge, SO this book focuses only on balancing them <i>at all</i>. We now look at simple methods for balancing explore vs. exploit with the point of showing that they are all better than \"always exploit\".</u>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2 Action-Value Methods\n",
    "\n",
    "**Action-Value Methods**: Methods for estimating the values of actions and using those action value estimates to make action selection decisions.\n",
    "\n",
    "### Value Estimation\n",
    "\n",
    "- **Goal**: Estimate the \"true value\" $q_*(a)$ for each arm $a$.\n",
    "- **Sample-average estimator** (_Equation 2.1_): Estimate \"true value\" of each arm as the average reward actually received from selecting that arm so far.\n",
    "  $$\n",
    "  Q_t(a) \\doteq \\frac{\\text { sum of rewards when } a \\text { taken prior to } t}{\\text { number of times } a \\text { taken prior to } t}=\\frac{\\displaystyle\\sum_{i=1}^{t-1} R_i \\cdot \\mathbf{1}_{A_i=a}}{\\displaystyle\\sum_{i=1}^{t-1} \\mathbf{1}_{A_i=a}}\n",
    "  $$\n",
    "\n",
    "  - where $\\mathbf{1}_\\text{predicate}$ is the indicator function (= 1 if predicate is True, else 0)\n",
    "  - If denominator is 0, define $Q_t(a)$ to be some default value (e.g. 0)\n",
    "  - As denominator $\\to \\infty$, $Q_t(a) \\to q_*(a)$.\n",
    "\t- Note that this is simply a value estimation. So this only holds assuming your action selection strategy actually selects $a$ with some non-zero probability. \n",
    "\t- i.o.w. this formula never even \"runs\" for arms that just get ignored (e.g. by purely greedy strategies).\n",
    "\n",
    "Note that the sample-average method for estimating values is simple & just one of many value estimation methods. But we stick with it for the next sections.\n",
    "\n",
    "### Action Selection Strategies\n",
    "\n",
    "- **Greedy Action Selection method** (_Equation 2.2_): Always pick the arm with the highest value estimate.\n",
    "  $$\n",
    "  A_t \\doteq \\arg\\max_a Q_t(a)\n",
    "  $$\n",
    "  - where $\\arg\\max_a$ denotes \"select action $a$ for which the expression that follows is maximized (with ties broken arbitrarily, perhaps randomly)\"\n",
    "  - *Limitation*: Purely greedy strategies may permanently overlook arms with higher true values. \n",
    "  \t- i.o.w. No guarantee that all $Q_t(a)$ actually converge to their respective $q_*(a)$. May get stuck exploiting a suboptimal action (resulting in a non-maximal cumulative reward) if your initial samples (and thus value estimates) of the highest true value arms happened to produce lower rewards.\n",
    "- **$\\epsilon$-greedy**: Be greedy most of the time, but select an action randomly $\\epsilon$% of the time (where $\\epsilon$ is some small probability)\n",
    "  - With probability $1 - \\epsilon$, pick greedy action.\n",
    "  - With probability $\\epsilon$, pick a random action.\n",
    "  - *Guarantees*: As $t \\to \\infty$, all actions will be sampled infinitely often, ensuring $Q_t(a) \\to q_*(a)$ by Law of Large Numbers.\n",
    "  \t- > This of course implies that the probability of selecting the optimal action converges to greater than $1 - \\epsilon$, that is, to near certainty. \n",
    "\n",
    "**Key Point**: Simple random exploration (via $\\epsilon$-greedy) is enough to outperform pure exploitation in most nontrivial problems.\n",
    "\n",
    "### Exercise 2.1 Solution\n",
    "**Q**: In $\\epsilon$-greedy action selection with 2 actions and $\\epsilon = 0.5$, what's the probability the greedy action is selected?\n",
    "\n",
    "**A**: \n",
    "- With probability $1 - \\epsilon = 0.5$, select greedy action deterministically.\n",
    "- With probability $\\epsilon = 0.5$, select randomly between 2 actions (each with probability 0.25).\n",
    "- **Total probability of greedy action** = $0.5 + 0.5 \\times 0.5 = 0.75$\n",
    "\n",
    "---\n",
    "\n",
    "## 2.3 The 10-Armed Testbed (how to test different action selection strategies)\n",
    "\n",
    "**Purpose**: Create a standardized empirical benchmark for comparing bandit algorithms and understanding the exploration-exploitation tradeoff.\n",
    "\n",
    "### Experimental Setup\n",
    "\n",
    "- **Standard empirical benchmark** for comparing bandit algorithms.\n",
    "- **Test suite**: 2000 randomly generated 10-armed bandit problems.\n",
    "- **For each individual bandit problem**:\n",
    "  - $k = 10$ arms (actions available).\n",
    "  - **True action values** $q_*(a)$ for each arm $a = 1, 2, ..., 10$ sampled from $\\mathcal{N}(0, 1)$.\n",
    "    - This means the \"best\" arm varies randomly across problems.\n",
    "    - Action values are distributed around 0 with unit variance.\n",
    "  - **Reward generation**: When action $A_t$ is selected at time $t$, reward $R_t \\sim \\mathcal{N}(q_*(A_t), 1)$.\n",
    "    - Rewards are noisy: normally distributed around the true action value with unit variance.\n",
    "    - This noise makes it challenging to identify the best action quickly.\n",
    "\n",
    "<img src=\"../img/fig2_1.png\" alt=\"true value & reward distribution\" width=\"50%\"/>\n",
    "\n",
    "### Methodology\n",
    "\n",
    "- **One run**: Apply a learning algorithm to one bandit problem for 1000 time steps.\n",
    "- **Performance measurement**: Track reward and optimal action selection over time.\n",
    "- **Statistical reliability**: Average results over 2000 independent runs (each with different bandit problems).\n",
    "- **Algorithms compared**: \n",
    "  - Pure greedy ($\\epsilon = 0$)\n",
    "  - $\\epsilon$-greedy with $\\epsilon = 0.01$ \n",
    "  - $\\epsilon$-greedy with $\\epsilon = 0.1$\n",
    "- **Value estimation**: All methods use sample-average technique with initial estimates $Q_1(a) = 0$.\n",
    "\n",
    "### Results (Figure 2.2)\n",
    "\n",
    "<img src=\"../img/fig2_2.png\" alt=\"testbed results\" width=\"50%\"/>\n",
    "\n",
    "**Upper graph (Average Reward over Time)**:\n",
    "- **Greedy method**: \n",
    "  - Improves slightly faster initially (exploits early good estimates).\n",
    "  - Plateaus at lower performance (~1.0 average reward vs. ~1.54 optimal).\n",
    "  - Gets \"stuck\" performing suboptimal actions.\n",
    "- **$\\epsilon$-greedy methods**:\n",
    "  - Initially perform worse due to random exploration.\n",
    "  - Eventually surpass greedy method due to continued learning.\n",
    "  - $\\epsilon = 0.1$: Faster initial improvement, higher exploration.\n",
    "  - $\\epsilon = 0.01$: Slower but more precise convergence.\n",
    "\n",
    "**Lower graph (% Optimal Action Selection)**:\n",
    "- **Greedy**: Finds optimal action only ~33% of the time.\n",
    "  - In 2/3 of problems, early disappointing samples from optimal action cause permanent abandonment.\n",
    "- **$\\epsilon = 0.1$**: Explores more, finds optimal action earlier, but caps at ~91% (due to 10% random selection).\n",
    "- **$\\epsilon = 0.01$**: Slower improvement but eventually achieves higher optimal action percentage.\n",
    "\n",
    "### Key Insights and Implications\n",
    "\n",
    "**Why Greedy Fails**:\n",
    "- **Early sampling bias**: If the truly optimal action gives disappointing early rewards (due to noise), greedy method abandons it permanently.\n",
    "- **No recovery mechanism**: Once a suboptimal action appears best, greedy never reconsiders abandoned actions.\n",
    "- **Premature convergence**: Settles on locally optimal choice based on limited early experience.\n",
    "\n",
    "**Why $\\epsilon$-greedy Succeeds**:\n",
    "- **Guaranteed exploration**: Every action will be sampled infinitely often as $t \\to \\infty$.\n",
    "- **Convergence guarantee**: By Law of Large Numbers, $Q_t(a) \\to q_*(a)$ for all actions.\n",
    "- **Probability of optimal convergence**: $\\lim_{t \\to \\infty} P(\\text{select optimal action}) \\geq 1 - \\epsilon$.\n",
    "\n",
    "**Parameter Tradeoffs**:\n",
    "- **Larger $\\epsilon$ (0.1)**: \n",
    "  - Faster exploration and discovery of optimal action.\n",
    "  - Lower asymptotic performance due to continued random actions.\n",
    "- **Smaller $\\epsilon$ (0.01)**:\n",
    "  - Slower initial learning.\n",
    "  - Better long-term performance with less \"wasted\" exploration.\n",
    "\n",
    "### When Different Methods Excel\n",
    "\n",
    "**Favor more exploration when**:\n",
    "- **Higher reward variance**: Noisier rewards require more samples to identify true values.\n",
    "- **Nonstationary environments**: True action values change over time, requiring ongoing exploration.\n",
    "- **Uncertain initial conditions**: When starting knowledge is poor.\n",
    "\n",
    "**Favor less exploration when**:\n",
    "- **Deterministic rewards**: True values can be learned quickly (in extreme case, greedy optimal after one sample per action).\n",
    "- **High confidence in estimates**: When you have good prior knowledge.\n",
    "- **Short time horizons**: When there's insufficient time to benefit from exploration.\n",
    "\n",
    "### Key Takeaway\n",
    "The 10-armed testbed demonstrates that **even simple exploration can dramatically outperform pure exploitation** in realistic noisy environments. The optimal balance between exploration and exploitation depends on problem characteristics, but some exploration is almost always beneficial.\n",
    "\n",
    "---\n",
    "---\n",
    "### Exercise 2.2 Solution\n",
    "**Q:** Consider a 4-armed bandit with ε-greedy action selection, sample-average estimates, and initial estimates Q₁(a) = 0 for all a. Given the sequence: A₁ = 1, R₁ = -1; A₂ = 2, R₂ = 1; A₃ = 2, R₃ = -2; A₄ = 2, R₄ = 2; A₅ = 3, R₅ = 0. On which time steps did exploration definitely occur? On which could it have occurred?\n",
    "\n",
    "**A:** Let's trace through the Q-value updates:\n",
    "\n",
    "**Initial:** Q₁(1) = Q₁(2) = Q₁(3) = Q₁(4) = 0\n",
    "\n",
    "**Step 1:** A₁ = 1, R₁ = -1\n",
    "- All actions tied at Q = 0, so action 1 **could be greedy or exploratory**\n",
    "- Update: Q₂(1) = -1, others remain 0\n",
    "\n",
    "**Step 2:** A₂ = 2, R₂ = 1  \n",
    "- Q-values: Q(1) = -1, Q(2) = Q(3) = Q(4) = 0\n",
    "- Action 2 tied for greedy (highest at 0), so **could be greedy or exploratory**\n",
    "- Update: Q₃(2) = 1, others unchanged\n",
    "\n",
    "**Step 3:** A₃ = 2, R₃ = -2\n",
    "- Q-values: Q(1) = -1, Q(2) = 1, Q(3) = Q(4) = 0\n",
    "- Action 2 is greedy (highest at 1), so **could be greedy or exploratory**\n",
    "- Update: Q₄(2) = (1 + (-2))/2 = -0.5\n",
    "\n",
    "**Step 4:** A₄ = 2, R₄ = 2\n",
    "- Q-values: Q(1) = -1, Q(2) = -0.5, Q(3) = Q(4) = 0\n",
    "- Actions 3 and 4 tied for greedy (0), action 2 is not greedy (-0.5)\n",
    "- **Definitely exploratory**\n",
    "- Update: Q₅(2) = (1 + (-2) + 2)/3 = 1/3\n",
    "\n",
    "**Step 5:** A₅ = 3, R₅ = 0\n",
    "- Q-values: Q(1) = -1, Q(2) = 1/3, Q(3) = Q(4) = 0\n",
    "- Action 2 is greedy (1/3), action 3 is not greedy (0)\n",
    "- **Definitely exploratory**\n",
    "\n",
    "**Answer:** \n",
    "- **Definitely exploratory:** Steps 4 and 5\n",
    "- **Could have been exploratory:** Steps 1, 2, and 3\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 2.3 Solution\n",
    "**Q:** In Figure 2.2, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be?\n",
    "\n",
    "**A:** The **ε = 0.01 method** will perform best in the long run on both measures.\n",
    "\n",
    "**Probability of selecting optimal action:**\n",
    "- **ε = 0.1:** Asymptotes at ~91% (due to 10% random exploration)\n",
    "- **ε = 0.01:** Will asymptote at ~99% (due to only 1% random exploration)\n",
    "- **Improvement:** ~8 percentage points higher optimal action selection\n",
    "\n",
    "**Average reward:**\n",
    "- **ε = 0.1:** Limited by continued exploration reducing average reward\n",
    "- **ε = 0.01:** Higher asymptotic reward due to more exploitation of learned optimal action\n",
    "\n",
    "**Quantitative reasoning:**\n",
    "- As t → ∞, both methods learn the true action values perfectly\n",
    "- ε = 0.01 selects optimal action with probability ≥ 1 - 0.01 = 0.99\n",
    "- ε = 0.1 selects optimal action with probability ≤ 1 - 0.1 = 0.90\n",
    "- Given optimal action has highest expected reward, ε = 0.01 achieves ~9% more optimal selections\n",
    "- This translates directly to higher long-run average reward proportional to the reward difference between optimal and suboptimal actions\n",
    "\n",
    "**Trade-off:** ε = 0.01 learns more slowly initially but achieves better asymptotic performance.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## 2.4 Incremental Implementation (how to actually implement action-value methods efficiently)\n",
    "\n",
    "**Core Problem**: Naive implementation of sample averaging requires storing all past rewards and recomputing the full average each time—this scales poorly in both memory and computation.\n",
    "\n",
    "**Relationship to episodic learning**: Unlike the tic-tac-toe example (which used batch updates after episodes), bandit problems typically update **after each action selection**. However, the incremental computation method shown here works for both cases—it's simply a more efficient way to compute sample averages whenever updates occur.\n",
    "\n",
    "### The Memory and Computation Problem\n",
    "\n",
    "**Naive sample-average implementation**:\n",
    "- Store all rewards: $R_1, R_2, ..., R_{n-1}$ for each action\n",
    "- Compute: $Q_n = \\frac{R_1 + R_2 + \\cdots + R_{n-1}}{n-1}$\n",
    "- **Memory**: $O(n)$ - grows linearly with number of samples\n",
    "- **Computation**: $O(n)$ per update - must sum all rewards each time\n",
    "\n",
    "This becomes impractical for long-running systems or problems with many actions.\n",
    "\n",
    "### The Incremental Solution\n",
    "\n",
    "**Key insight**: We can update estimates incrementally using only the new reward and current estimate.\n",
    "\n",
    "**Mathematical derivation** for the $n$-th reward $R_n$ for action $a$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q_{n+1} &= \\frac{1}{n}\\sum_{i=1}^{n} R_i \\\\\n",
    "&= \\frac{1}{n}\\left[R_n + \\sum_{i=1}^{n-1} R_i\\right] \\\\\n",
    "&= \\frac{1}{n}\\left[R_n + (n-1) \\frac{1}{n-1}\\sum_{i=1}^{n-1} R_i\\right] \\\\\n",
    "&= \\frac{1}{n}\\left[R_n + (n-1)Q_n\\right] \\\\\n",
    "&= \\frac{1}{n}\\left[R_n + nQ_n - Q_n\\right] \\\\\n",
    "&= Q_n + \\frac{1}{n}\\left[R_n - Q_n\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Incremental update rule** (_Equation 2.3_):\n",
    "$$\n",
    "Q_{n+1}(a) = Q_n(a) + \\frac{1}{n}\\left[R_n - Q_n(a)\\right]\n",
    "$$\n",
    "\n",
    "**Efficiency gains**:\n",
    "- **Memory**: $O(1)$ - only store $Q_n(a)$ and count $n$ for each action\n",
    "- **Computation**: $O(1)$ per update - single arithmetic operation\n",
    "\n",
    "### The General Update Form\n",
    "\n",
    "**Universal pattern** (_Equation 2.4_):\n",
    "$$\n",
    "\\text{NewEstimate} \\leftarrow \\text{OldEstimate} + \\text{StepSize} \\cdot \\left[\\text{Target} - \\text{OldEstimate}\\right]\n",
    "$$\n",
    "\n",
    "**Components**:\n",
    "- **Target**: $R_n$ (the new reward observed)\n",
    "- **StepSize**: $\\frac{1}{n}$ (decreases as more samples are collected)\n",
    "- **Error**: $\\left[\\text{Target} - \\text{OldEstimate}\\right]$ (how far off our current estimate is)\n",
    "\n",
    "**Interpretation**:\n",
    "- We \"step toward\" the target by an amount proportional to the error\n",
    "- Step size determines how much we trust the new information vs. old estimate\n",
    "- This pattern appears throughout RL, not just in bandits\n",
    "\n",
    "### Bandit vs. Episodic Update Timing\n",
    "\n",
    "**Bandit context (this chapter)**:\n",
    "- Updates happen **immediately after each action** (online/continuing learning)\n",
    "- No natural episode boundaries—learning continues indefinitely\n",
    "- Incremental updates enable real-time learning without memory growth\n",
    "\n",
    "**Episodic context (like tic-tac-toe)**:\n",
    "- Updates might happen **after episodes complete** (batch updates)\n",
    "- Same incremental formula applies, just applied at different times\n",
    "- Whether online or batch, the computational efficiency benefits remain\n",
    "\n",
    "**Key point**: Section 2.4 focuses on **computational efficiency of the update calculation itself**, regardless of when those updates occur in the learning process.\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "**Step-size parameter**: In the incremental method, step-size $\\alpha_t(a) = \\frac{1}{n}$ where $n$ is the number of times action $a$ has been selected.\n",
    "\n",
    "**Pseudocode** (from the book):\n",
    "\n",
    "<img src=\"../img/banditalgo.png\" alt=\"bandit algo\" width=\"40%\"/>\n",
    "\n",
    "See [Bandit Implementation in python, here](./code_examples/bandit_implementation.py).\n",
    "\n",
    "---\n",
    "\n",
    "## 2.5 Tracking a Nonstationary Problem\n",
    "\n",
    "**Context shift**: Previous methods assume **stationary** bandit problems where reward probabilities don't change over time. In practice, many RL problems are **nonstationary**—the optimal action changes over time.\n",
    "\n",
    "**Key insight**: For nonstationary problems, recent rewards should matter more than old rewards since old information may be outdated.\n",
    "\n",
    "### The Problem with Sample Averages in Nonstationary Environments\n",
    "\n",
    "**Sample-average method limitations**:\n",
    "- Treats all rewards equally: $Q_n = \\frac{1}{n}\\sum_{i=1}^{n} R_i$\n",
    "- Old rewards from 1000 steps ago get same weight as recent rewards\n",
    "- **Cannot adapt** when true action values change over time\n",
    "- **Slow response** to changes in optimal action\n",
    "\n",
    "**Solution**: Use **constant step-size parameter** to emphasize recent experience.\n",
    "\n",
    "### Constant Step-Size Method\n",
    "\n",
    "**Modified incremental update rule** (_Equation 2.5_):\n",
    "$$Q_{n+1} = Q_n + \\alpha[R_n - Q_n]$$\n",
    "\n",
    "where **step-size parameter** $\\alpha \\in (0, 1]$ is constant.\n",
    "\n",
    "**Key differences from sample-average**:\n",
    "- Step-size doesn't decrease with more samples ($\\alpha$ vs. $\\frac{1}{n}$)\n",
    "- More recent rewards get higher effective weight\n",
    "- Enables tracking of changing environments\n",
    "\n",
    "### Exponential Recency-Weighted Average\n",
    "\n",
    "**Mathematical expansion** (_Equation 2.6_):\n",
    "$$\n",
    "\\begin{align}\n",
    "Q_{n+1} &= Q_n + \\alpha[R_n - Q_n] \\\\\n",
    "&= \\alpha R_n + (1-\\alpha)Q_n \\\\\n",
    "&= \\alpha R_n + (1-\\alpha)[\\alpha R_{n-1} + (1-\\alpha)Q_{n-1}] \\\\\n",
    "&= \\alpha R_n + (1-\\alpha)\\alpha R_{n-1} + (1-\\alpha)^2 Q_{n-1} \\\\\n",
    "&= \\alpha R_n + (1-\\alpha)\\alpha R_{n-1} + (1-\\alpha)^2\\alpha R_{n-2} + \\cdots \\\\\n",
    "&\\quad + (1-\\alpha)^{n-1}\\alpha R_1 + (1-\\alpha)^n Q_1 \\\\\n",
    "&= (1-\\alpha)^n Q_1 + \\sum_{i=1}^{n} \\alpha(1-\\alpha)^{n-i} R_i\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Interpretation**:\n",
    "- **Sum of weights**: $(1-\\alpha)^n + \\sum_{i=1}^{n} \\alpha(1-\\alpha)^{n-i} = 1$ ✓\n",
    "- **Exponential decay**: Weight for reward $R_i$ is $\\alpha(1-\\alpha)^{n-i}$\n",
    "- **Recency bias**: More recent rewards get exponentially higher weight\n",
    "- **Decay rate**: $(1-\\alpha)$ controls how quickly old information is forgotten\n",
    "\n",
    "**Special cases**:\n",
    "- $\\alpha = 1$: Only most recent reward matters ($Q_{n+1} = R_n$)\n",
    "- $\\alpha \\to 0$: Approaches sample average\n",
    "- $1-\\alpha = 0$: All weight on most recent reward\n",
    "\n",
    "### Step-Size Sequence Convergence Conditions\n",
    "\n",
    "**General step-size sequence** $\\{\\alpha_n(a)\\}$ **convergence conditions** (_Equation 2.7_):\n",
    "$$\\sum_{n=1}^{\\infty} \\alpha_n(a) = \\infty \\quad \\text{and} \\quad \\sum_{n=1}^{\\infty} \\alpha_n^2(a) < \\infty$$\n",
    "\n",
    "**Condition interpretations**:\n",
    "1. **First condition**: Steps large enough to overcome initial conditions and random fluctuations\n",
    "2. **Second condition**: Steps eventually become small enough to ensure convergence\n",
    "\n",
    "**Method comparisons**:\n",
    "- **Sample-average** ($\\alpha_n = \\frac{1}{n}$): ✓ Both conditions satisfied → **guaranteed convergence**\n",
    "- **Constant step-size** ($\\alpha_n = \\alpha$): ✓ First condition, ✗ Second condition → **no convergence guarantee**\n",
    "\n",
    "### Why Non-Convergence Can Be Good\n",
    "\n",
    "**For nonstationary problems**:\n",
    "- **Lack of convergence is desirable**—estimates should track changing values\n",
    "- **Continued adaptation** to most recent rewards\n",
    "- **Responsiveness** to environmental changes\n",
    "\n",
    "**Trade-offs**:\n",
    "- **Larger $\\alpha$**: Faster adaptation, higher variance in estimates\n",
    "- **Smaller $\\alpha$**: Slower adaptation, lower variance, more stable\n",
    "\n",
    "**Practical note**: Theoretical convergence guarantees often require impractically slow learning rates. Constant step-sizes with good empirical performance are preferred in practice.\n",
    "\n",
    "### When to Use Each Method\n",
    "\n",
    "**Sample-average ($\\alpha = \\frac{1}{n}$)**:\n",
    "- **Stationary environments** where true values don't change\n",
    "- When you want **guaranteed convergence** to true values\n",
    "- **Long-term accuracy** more important than adaptation speed\n",
    "\n",
    "**Constant step-size ($\\alpha = $ constant)**:\n",
    "- **Nonstationary environments** where values change over time\n",
    "- When **responsiveness** to recent changes is crucial\n",
    "- **Online learning** scenarios with ongoing environmental shifts\n",
    "\n",
    "### Exercise 2.4 Solution\n",
    "**Q:** If step-size parameters αₙ are not constant, what is the weighting on each prior reward for the general case?\n",
    "\n",
    "**A:** For general step-size sequence {α₁, α₂, ..., αₙ}, the weight on reward Rᵢ in estimate Qₙ₊₁ is:\n",
    "\n",
    "$$w_i = \\alpha_i \\prod_{j=i+1}^{n} (1 - \\alpha_j)$$\n",
    "\n",
    "**Derivation:**\n",
    "- $Q_{n+1} = Q_n + \\alpha_n[R_n - Q_n] = \\alpha_n R_n + (1-\\alpha_n)Q_n$\n",
    "- $Q_n = \\alpha_{n-1} R_{n-1} + (1-\\alpha_{n-1})Q_{n-1}$\n",
    "- Substituting recursively gives the weight formula above\n",
    "\n",
    "**Verification:** For constant α, this reduces to $w_i = \\alpha(1-\\alpha)^{n-i}$, matching equation (2.6).\n",
    "\n",
    "### See [nonstationary testbed implementation, here](./code_examples/nonstationary_testbed.py) for Exercise 2.5.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Environment type determines method choice**: Stationary → sample-average, Nonstationary → constant step-size\n",
    "2. **Step-size controls adaptation speed**: Larger α = faster adaptation but higher variance\n",
    "3. **Exponential weighting emerges naturally** from constant step-size updates\n",
    "4. **Non-convergence can be beneficial** when tracking changing environments\n",
    "5. **This pattern generalizes** beyond bandits to full RL problems\n",
    "\n",
    "---\n",
    "\n",
    "## 2.6 Optimistic Initial Values\n",
    "\n",
    "**Core idea**: Set initial action-value estimates $Q_1(a)$ to optimistically high values to encourage exploration, even with greedy action selection.\n",
    "\n",
    "### The Technique\n",
    "\n",
    "**Standard approach**: Initialize $Q_1(a) = 0$ for all actions\n",
    "**Optimistic approach**: Initialize $Q_1(a) = +5$ (when true values $q_*(a) \\sim \\mathcal{N}(0,1)$)\n",
    "\n",
    "**Why it works**:\n",
    "- All actions start with unrealistically high value estimates\n",
    "- Whichever action is selected first will yield disappointing reward (less than +5)\n",
    "- Agent becomes \"disappointed\" and tries other actions\n",
    "- Continues until all actions tried multiple times\n",
    "- **Automatic exploration** even with purely greedy selection\n",
    "\n",
    "### Mechanism and Limitations\n",
    "\n",
    "**How disappointment drives exploration**:\n",
    "1. Agent selects action with highest $Q(a)$ (initially all tied at +5)\n",
    "2. Receives actual reward (much less than +5 in expectation)\n",
    "3. Updates $Q(a)$ downward via sample averaging\n",
    "4. Other actions still have optimistic values, so agent tries them\n",
    "5. Process continues until realistic estimates emerge\n",
    "\n",
    "**When it works well**:\n",
    "- **Stationary problems** where true values don't change\n",
    "- **Short-term exploration boost** at beginning of learning\n",
    "- Problems where initial exploration is most valuable\n",
    "\n",
    "**Limitations**:\n",
    "- **Not suitable for nonstationary problems**: Drive for exploration is temporary\n",
    "- **Beginning-focused**: Only encourages exploration early in learning\n",
    "- **Task-dependent**: Requires setting appropriate optimistic level\n",
    "- **One-time effect**: Once estimates converge, no further exploration benefit\n",
    "\n",
    "**Key insight**: Simple tricks can be surprisingly effective, but they're not universally applicable.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.7 Upper-Confidence-Bound Action Selection\n",
    "\n",
    "**Motivation**: $\\epsilon$-greedy explores indiscriminately. Better approach: explore actions based on their **potential** to be optimal, considering both current estimates and uncertainty.\n",
    "\n",
    "### The UCB Algorithm\n",
    "\n",
    "**UCB action selection** (_Equation 2.10_):\n",
    "$$A_t = \\arg\\max_a \\left[Q_t(a) + c\\sqrt{\\frac{\\ln t}{N_t(a)}}\\right]$$\n",
    "\n",
    "**Components**:\n",
    "- $Q_t(a)$: Current value estimate (exploitation term)\n",
    "- $c > 0$: Confidence level parameter  \n",
    "- $\\sqrt{\\frac{\\ln t}{N_t(a)}}$: Uncertainty/confidence bound (exploration term)\n",
    "- $N_t(a)$: Number of times action $a$ selected by time $t$\n",
    "\n",
    "**Interpretation**: Select action with highest **upper confidence bound** on its true value.\n",
    "\n",
    "### Why the Natural Logarithm?\n",
    "\n",
    "**Mathematical origin**: The $\\ln t$ term comes from **concentration inequalities** (particularly Hoeffding bounds) that provide probabilistic guarantees on estimation error.\n",
    "\n",
    "**Key properties of $\\ln t$**:\n",
    "- **Grows unbounded**: Ensures all actions eventually selected\n",
    "- **Grows slowly**: Doesn't dominate value estimates too quickly  \n",
    "- **Time-dependent**: Accounts for overall experience level\n",
    "- **Theoretical optimality**: Provides optimal regret bounds in certain settings\n",
    "\n",
    "**Intuitive behavior**:\n",
    "- As $t$ increases, pressure to explore all actions grows (but slowly)\n",
    "- As $N_t(a)$ increases, confidence in estimate grows (exploration bonus decreases)\n",
    "- Actions with fewer samples get higher exploration bonuses\n",
    "\n",
    "### UCB Mechanism\n",
    "\n",
    "**How uncertainty drives exploration**:\n",
    "1. **Rarely-selected actions**: Large $\\sqrt{\\frac{\\ln t}{N_t(a)}}$ term → high selection probability\n",
    "2. **Frequently-selected actions**: Small uncertainty term → selection based mainly on $Q_t(a)$  \n",
    "3. **Optimal actions**: Eventually dominate due to both high value and sufficient confidence\n",
    "\n",
    "**Adaptive exploration**: UCB automatically balances exploitation and exploration without requiring manual parameter tuning for exploration rate.\n",
    "\n",
    "### Performance and Limitations\n",
    "\n",
    "**Advantages**:\n",
    "- **Strong empirical performance** on stationary bandits\n",
    "- **Principled approach** based on statistical confidence\n",
    "- **Parameter-insensitive**: Less tuning required than $\\epsilon$-greedy\n",
    "\n",
    "**Limitations**:\n",
    "- **Difficult to extend** beyond bandits to full RL settings\n",
    "- **Nonstationary problems**: More complex methods needed\n",
    "- **Large state spaces**: Not practical with function approximation\n",
    "- **Computational complexity**: More complex than $\\epsilon$-greedy\n",
    "\n",
    "**Exercise 2.8 Solution**: The spike at step 11 occurs because after 10 steps, UCB has tried each action once. At step 11, $\\ln t$ jumps significantly while $N_t(a)$ values are still small, creating large exploration bonuses that temporarily override value estimates. The spike decreases afterward as $N_t(a)$ values grow and exploration bonuses shrink.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.8 Gradient Bandit Algorithms\n",
    "\n",
    "**Paradigm shift**: Instead of estimating action **values**, learn action **preferences** and select actions probabilistically.\n",
    "\n",
    "### Preference-Based Action Selection\n",
    "\n",
    "**Action preferences**: $H_t(a) \\in \\mathbb{R}$ for each action $a$\n",
    "- **Not interpretable as values**: Only relative preferences matter\n",
    "- **Scale invariant**: Adding constant to all $H_t(a)$ doesn't change behavior\n",
    "- **Learned through experience**: Start with $H_1(a) = 0$ (equal preferences)\n",
    "\n",
    "**Action selection via softmax** (_Equation 2.11_):\n",
    "$$\\pi_t(a) = \\Pr\\{A_t = a\\} = \\frac{e^{H_t(a)}}{\\sum_{b=1}^{k} e^{H_t(b)}}$$\n",
    "\n",
    "### Softmax Function Deep Dive\n",
    "\n",
    "**Mathematical formulation**: \n",
    "$$\\text{softmax}(\\mathbf{h})_i = \\frac{e^{h_i}}{\\sum_{j=1}^{k} e^{h_j}}$$\n",
    "\n",
    "**Why exponential function?**:\n",
    "- **Always positive**: $e^x > 0$ for all $x$, ensuring valid probabilities\n",
    "- **Monotonic**: Higher preferences → higher probabilities\n",
    "- **Differentiable**: Enables gradient-based learning\n",
    "- **Amplifies differences**: Small preference differences become larger probability differences\n",
    "\n",
    "**Temperature interpretation**: Implicit temperature $T = 1$. General form:\n",
    "$$\\pi_t(a) = \\frac{e^{H_t(a)/T}}{\\sum_{b=1}^{k} e^{H_t(b)/T}}$$\n",
    "- **High T**: More uniform (exploratory)\n",
    "- **Low T**: More concentrated on best action (exploitative)\n",
    "\n",
    "**Usage in ML**:\n",
    "- **Neural networks**: Final layer for classification\n",
    "- **Multinomial logistic regression**: Link function\n",
    "- **Reinforcement learning**: Policy parameterization\n",
    "- **Attention mechanisms**: Weighting scheme\n",
    "\n",
    "**Why appropriate for bandits**:\n",
    "- **Smooth selection**: Avoids discrete exploration decisions\n",
    "- **Differentiable**: Enables policy gradient methods\n",
    "- **Probabilistic**: Natural for stochastic policies\n",
    "- **Scale-free**: Relative preferences matter, not absolute values\n",
    "\n",
    "### Gradient Bandit Update Rules\n",
    "\n",
    "**Preference updates** (_Equation 2.12_):\n",
    "\n",
    "**For selected action** $A_t$:\n",
    "$$H_{t+1}(A_t) = H_t(A_t) + \\alpha(R_t - \\bar{R}_t)[1 - \\pi_t(A_t)]$$\n",
    "\n",
    "**For non-selected actions** $a \\neq A_t$:\n",
    "$$H_{t+1}(a) = H_t(a) - \\alpha(R_t - \\bar{R}_t)\\pi_t(a)$$\n",
    "\n",
    "**Parameters**:\n",
    "- $\\alpha > 0$: Step-size parameter\n",
    "- $\\bar{R}_t$: Average reward baseline (computed incrementally)\n",
    "\n",
    "### Understanding the Updates\n",
    "\n",
    "**Reward above baseline** ($R_t > \\bar{R}_t$):\n",
    "- **Selected action**: Preference increases by $\\alpha(R_t - \\bar{R}_t)[1 - \\pi_t(A_t)]$\n",
    "- **Non-selected actions**: Preferences decrease by $\\alpha(R_t - \\bar{R}_t)\\pi_t(a)$\n",
    "- **Effect**: Higher reward → increase probability of repeating selected action\n",
    "\n",
    "**Reward below baseline** ($R_t < \\bar{R}_t$):\n",
    "- **Selected action**: Preference decreases\n",
    "- **Non-selected actions**: Preferences increase  \n",
    "- **Effect**: Poor reward → decrease probability of repeating selected action\n",
    "\n",
    "**Why baseline matters**:\n",
    "- **Variance reduction**: Reduces update noise, faster learning\n",
    "- **Reference point**: Provides context for \"good\" vs \"bad\" rewards\n",
    "- **Figure 2.5 evidence**: Without baseline, performance degrades significantly\n",
    "\n",
    "### Theoretical Foundation: Stochastic Gradient Ascent\n",
    "\n",
    "**Objective**: Maximize expected reward $\\mathbb{E}[R_t] = \\sum_a \\pi_t(a)q_*(a)$\n",
    "\n",
    "**True gradient**:\n",
    "$$\\frac{\\partial \\mathbb{E}[R_t]}{\\partial H_t(a)} = \\sum_x q_*(x) \\frac{\\partial \\pi_t(x)}{\\partial H_t(a)}$$\n",
    "\n",
    "**Key mathematical result**:\n",
    "$$\\frac{\\partial \\pi_t(x)}{\\partial H_t(a)} = \\pi_t(x)[\\mathbf{1}_{a=x} - \\pi_t(a)]$$\n",
    "\n",
    "where $\\mathbf{1}_{a=x}$ is indicator function.\n",
    "\n",
    "**Sample-based approximation**: The update rules (2.12) are unbiased estimates of the true gradient, making this algorithm an instance of **stochastic gradient ascent**.\n",
    "\n",
    "**Convergence guarantee**: As a stochastic gradient method, has robust convergence properties to local optima.\n",
    "\n",
    "**Exercise 2.9 Solution**: For two actions with preferences $H_t(1)$ and $H_t(2)$:\n",
    "$$\\pi_t(1) = \\frac{e^{H_t(1)}}{e^{H_t(1)} + e^{H_t(2)}} = \\frac{1}{1 + e^{H_t(2) - H_t(1)}}$$\n",
    "\n",
    "This is the **logistic (sigmoid) function** with input $H_t(1) - H_t(2)$, commonly used in binary classification and neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.9 Associative Search (Contextual Bandits)\n",
    "\n",
    "**Bridge to full RL**: Introduces the concept of **state** while maintaining the single-step reward structure of bandits.\n",
    "\n",
    "### The Associative Search Problem\n",
    "\n",
    "**Setup**: Multiple bandit problems, with observable context indicating which problem you're facing.\n",
    "\n",
    "**Example scenario**:\n",
    "- Multiple slot machines with different colored displays\n",
    "- Color indicates machine's current reward mapping\n",
    "- Goal: Learn policy mapping colors (contexts) to optimal actions\n",
    "\n",
    "**Formal definition**:\n",
    "- **Context**: Observable state $s_t$ that indicates current bandit problem\n",
    "- **Policy**: Mapping $\\pi(s) : \\mathcal{S} \\to \\mathcal{A}$ from contexts to actions\n",
    "- **Objective**: Learn optimal action for each context\n",
    "\n",
    "### Comparison to Other Problems\n",
    "\n",
    "**vs. Standard k-armed bandit**:\n",
    "- **Bandit**: Single stationary problem, learn one optimal action\n",
    "- **Associative search**: Multiple problems, learn optimal action for each context\n",
    "\n",
    "**vs. Full RL**:\n",
    "- **Similar**: Both involve learning policies mapping states to actions\n",
    "- **Different**: Actions only affect immediate reward, not next state\n",
    "- **Associative search**: $s_{t+1}$ independent of $A_t$\n",
    "- **Full RL**: $s_{t+1}$ depends on both $s_t$ and $A_t$\n",
    "\n",
    "### Why Context Matters\n",
    "\n",
    "**Without context information**:\n",
    "- Appears as single nonstationary bandit\n",
    "- Optimal strategy: Use methods from Section 2.5\n",
    "- Performance limited by need to adapt to changing optimal action\n",
    "\n",
    "**With context information**:\n",
    "- Can learn separate policy for each context\n",
    "- Much better performance possible\n",
    "- Exploit structure in the environment\n",
    "\n",
    "**Key insight**: **Information about state** dramatically improves learning performance when environment has structure.\n",
    "\n",
    "### Solution Approaches\n",
    "\n",
    "**Context-specific learning**:\n",
    "- Maintain separate value estimates $Q_t(s,a)$ for each state-action pair\n",
    "- Apply bandit algorithms within each context\n",
    "- Use any method: $\\epsilon$-greedy, UCB, gradient bandit, etc.\n",
    "\n",
    "**Function approximation**:\n",
    "- Learn general mapping from (context, action) to values\n",
    "- Important when context space is large or continuous\n",
    "\n",
    "**Exercise 2.10 Solution**:\n",
    "\n",
    "**Without context knowledge**:\n",
    "- Expected values: Action 1 gives $0.5 \\times 10 + 0.5 \\times 90 = 50$\n",
    "- Action 2 gives $0.5 \\times 20 + 0.5 \\times 80 = 50$\n",
    "- **Best strategy**: Either action, expected reward = 50\n",
    "\n",
    "**With context knowledge**:\n",
    "- **Case A**: Choose action 2 (20 > 10), expected reward = 20\n",
    "- **Case B**: Choose action 1 (90 > 80), expected reward = 90  \n",
    "- **Overall**: $0.5 \\times 20 + 0.5 \\times 90 = 55$\n",
    "- **Improvement**: 10% better performance just from observing context\n",
    "\n",
    "---\n",
    "\n",
    "## 2.10 Summary\n",
    "\n",
    "**Chapter overview**: Exploration vs. exploitation in the simplest RL setting—bandits provide foundation for understanding this fundamental tradeoff.\n",
    "\n",
    "### Method Comparison and Performance\n",
    "\n",
    "**Key algorithms covered**:\n",
    "\n",
    "| Method | Core Idea | Best For | Limitations |\n",
    "|--------|-----------|----------|-------------|\n",
    "| **Greedy** | Always exploit current best estimate | Deterministic environments | Gets stuck in suboptimal choices |\n",
    "| **$\\epsilon$-greedy** | Random exploration with probability $\\epsilon$ | General-purpose, robust | Indiscriminate exploration |\n",
    "| **Optimistic initialization** | Start with overly high value estimates | Stationary problems | Temporary exploration only |\n",
    "| **UCB** | Confidence-based exploration | Stationary bandits | Hard to extend to full RL |\n",
    "| **Gradient bandit** | Learn action preferences, not values | When relative preferences matter | More complex than value-based |\n",
    "\n",
    "**Performance insights from Figure 2.6**:\n",
    "- **Inverted-U shape**: All methods have optimal parameter ranges\n",
    "- **UCB**: Generally best performance on 10-armed testbed\n",
    "- **Parameter sensitivity**: All methods fairly robust across parameter ranges\n",
    "- **No universal winner**: Performance depends on problem characteristics\n",
    "\n",
    "### Advanced Topics and Future Directions\n",
    "\n",
    "**Bayesian approaches**:\n",
    "- **Gittins indices**: Theoretically optimal for specific cases\n",
    "- **Thompson sampling**: Sample from posterior distributions\n",
    "- **Computational complexity**: Often intractable for large problems\n",
    "\n",
    "**Theoretical guarantees**:\n",
    "- **Regret bounds**: How quickly algorithms approach optimal performance\n",
    "- **Sample complexity**: Number of steps needed to learn near-optimal policy\n",
    "- **Exploration efficiency**: Formal measures of exploration quality\n",
    "\n",
    "**Limitations of current methods**:\n",
    "- **Distributional assumptions**: Most methods assume specific reward distributions\n",
    "- **Prior knowledge**: Often require domain-specific parameter tuning\n",
    "- **Scalability**: Simple methods may not extend to complex RL problems\n",
    "\n",
    "### Comprehensive Symbol Reference\n",
    "\n",
    "| Symbol | Meaning | Context |\n",
    "|--------|---------|---------|\n",
    "| **Problem Setup** | | |\n",
    "| $k$ | Number of actions (arms) | Multi-armed bandit |\n",
    "| $A_t$ | Action selected at time $t$ | Random variable |\n",
    "| $R_t$ | Reward received at time $t$ | Random variable |\n",
    "| $q_*(a)$ | True value of action $a$ | $\\mathbb{E}[R_t \\mid A_t = a]$ |\n",
    "| **Value Estimation** | | |\n",
    "| $Q_t(a)$ | Estimated value of action $a$ at time $t$ | Sample average or other estimate |\n",
    "| $N_t(a)$ | Number of times action $a$ selected by time $t$ | Action count |\n",
    "| $\\alpha$ | Step-size parameter | Learning rate |\n",
    "| $\\bar{R}_t$ | Average reward baseline up to time $t$ | Variance reduction |\n",
    "| **Action Selection** | | |\n",
    "| $\\epsilon$ | Exploration probability | $\\epsilon$-greedy parameter |\n",
    "| $\\pi_t(a)$ | Probability of selecting action $a$ at time $t$ | Stochastic policy |\n",
    "| $c$ | Confidence level parameter | UCB exploration control |\n",
    "| **Gradient Bandits** | | |\n",
    "| $H_t(a)$ | Preference for action $a$ at time $t$ | Not interpretable as value |\n",
    "| **Mathematical Operators** | | |\n",
    "| $\\arg\\max_a f(a)$ | Action $a$ that maximizes $f(a)$ | Greedy selection |\n",
    "| $\\mathbf{1}_{\\text{condition}}$ | Indicator function | 1 if condition true, 0 otherwise |\n",
    "| $\\ln t$ | Natural logarithm of $t$ | UCB confidence bound |\n",
    "| **Update Patterns** | | |\n",
    "| Target $-$ OldEstimate | Error signal | Universal RL update pattern |\n",
    "| StepSize $\\times$ Error | Update magnitude | Learning rate control |\n",
    "\n",
    "### Key Insights for Full RL\n",
    "\n",
    "**Foundational concepts**:\n",
    "1. **Exploration-exploitation tradeoff** appears in all RL problems\n",
    "2. **Incremental updates** are computationally essential\n",
    "3. **Step-size choice** affects adaptation vs. stability\n",
    "4. **Baseline techniques** reduce variance in learning\n",
    "5. **Context information** dramatically improves performance\n",
    "\n",
    "**Limitations requiring more advanced methods**:\n",
    "- **Sequential decision making**: Actions affect future states\n",
    "- **Credit assignment**: Rewards may be delayed\n",
    "- **Large state spaces**: Function approximation needed\n",
    "- **Continuous actions**: Discrete methods don't apply\n",
    "\n",
    "**Bridge to Chapter 3**: Associative search introduces states but maintains immediate rewards. Next: full RL where actions affect both immediate rewards and future states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87706ebe",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cf86eb",
   "metadata": {},
   "source": [
    "# Appendix A: Full Notation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c0755",
   "metadata": {},
   "source": [
    "### General Notation\n",
    "\n",
    "> Capital letters are used for random variables.\n",
    "\n",
    "> Lower case letters are used for the values of random variables and for scalar functions.\n",
    "\n",
    "> Quantities that are required to be real-valued vectors are written in bold and in lower case (even if random variables). \n",
    "\n",
    "> Matrices are bold capitals.\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **Standard Operators** |  |\n",
    "| $\\doteq$ | Equality relationship that is true by definition |\n",
    "| $\\approx$ | Approximately equal |\n",
    "| $\\propto$ | Proportional to |\n",
    "| $\\Pr\\{X = x\\}$ | Probability that random variable $X$ takes value $x$ |\n",
    "| $X \\sim p$ | $X$ drawn from distribution $p$ |\n",
    "| $\\mathbb{E}[X]$ | Expectation of random variable $X$ |\n",
    "| $\\arg\\max_a f(a)$ | Value of $a$ that maximizes $f(a)$ |\n",
    "| $\\ln x$ | Natural logarithm of $x$ |\n",
    "| $e^x$, $\\exp(x)$ | The base of the natural logarithm, $e \\approx 2.71828$, carried to power $x$ |\n",
    "| $\\mathbb{R}$ | Set of real numbers |\n",
    "| $f: \\mathcal{X} \\to \\mathcal{Y}$ | Function from elements of set $\\mathcal{X}$ to elements of set $\\mathcal{Y}$ |\n",
    "| $\\leftarrow$ | Assignment |\n",
    "| $(a, b]$ | Real interval between $a$ and $b$ including $b$ but not $a$ |\n",
    "| **Standard RL Algorithm Parameters** |  |\n",
    "| $\\epsilon$ | Probability of taking a random action in an $\\epsilon$-greedy policy |\n",
    "| $\\alpha, \\beta$ | Step-size parameters |\n",
    "| $\\gamma$ | Discount-rate parameter |\n",
    "| $\\lambda$ | Decay-rate parameter for eligibility traces |\n",
    "| $\\mathbf{1}_\\text{predicate}$ | Indicator function (1 if predicate is true, else 0) |\n",
    "\n",
    "---\n",
    "\n",
    "### Multi-Armed Bandit Notation\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| $k$ | Number of actions (arms) |\n",
    "| $t$ | Discrete time step or play number |\n",
    "| $q_*(a)$ | True value (expected reward) of action $a$ |\n",
    "| $Q_t(a)$ | Estimate at time $t$ of $q_*(a)$ |\n",
    "| $N_t(a)$ | Number of times action $a$ has been selected up to time $t$ |\n",
    "| $H_t(a)$ | Learned preference for selecting action $a$ at time $t$ |\n",
    "| $\\pi_t(a)$ | Probability of selecting action $a$ at time $t$ |\n",
    "| $\\bar{R}_t$ | Estimate at time $t$ of the expected reward given $\\pi_t$ |\n",
    "\n",
    "---\n",
    "\n",
    "### Markov Decision Process Notation\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **State & Action Sets** |  |\n",
    "| $s, s'$ | States |\n",
    "| $a$ | An action |\n",
    "| $r$ | A reward |\n",
    "| $\\mathcal{S}$ | Set of all nonterminal states |\n",
    "| $\\mathcal{S}^+$ | Set of all states, incl. terminal state |\n",
    "| $\\mathcal{A}(s)$ | Set of all actions available in state $s$ |\n",
    "| $\\mathcal{R}$ | Set of all possible rewards, a finite subset of $\\mathbb{R}$ |\n",
    "| $\\mathcal{C}$ | Subset of (e.g., $\\mathcal{R} \\subset \\mathbb{R}$) |\n",
    "| $\\in$ | Is an element of (e.g. $s \\in \\mathcal{S}$, $r \\in \\mathcal{R}$) |\n",
    "| $\\lvert\\mathcal{S}\\rvert$ | Number of elements in set $\\mathcal{S}$ |\n",
    "| **Time & Policy** |  |\n",
    "| $t$ | Discrete time step |\n",
    "| $T, T(t)$ | Final time step of episode, or including $t$ |\n",
    "| $A_t$ | Action at time $t$ |\n",
    "| $S_t$ | State at time $t$ |\n",
    "| $R_t$ | Reward at time $t$ |\n",
    "| $\\pi$ | Policy (decision-making rule) |\n",
    "| $\\pi(s)$ | Action taken in state $s$ under deterministic $\\pi$ |\n",
    "| $\\pi(a \\mid s)$ | Probability of taking action $a$ in state $s$ under stochastic $\\pi$ |\n",
    "| **Returns** |  |\n",
    "| $G_t$ | Return following time $t$ |\n",
    "| $h$ | Horizon (timestep looked up to in forward view) |\n",
    "| $G_{t:t+n}, G_{t:h}$ | $n$-step return from $t+1$ to $t+n$ or $h$ (discounted/corrected) |\n",
    "| $G_{t:h}$ | Flat return (undiscounted/unadjusted) from $t+1$ to $h$ |\n",
    "| $G^\\lambda_t$ | $\\lambda$-return |\n",
    "| $G^{\\wedge}_t$, $G^{\\wedge a}_t$ | Truncated, corrected $\\lambda$-return |\n",
    "| **Transition & Reward Probabilities** |  |\n",
    "| $p(s', r \\mid s, a)$ | Probability of transition to $s'$ with reward $r$ from $s, a$ |\n",
    "| $p(s' \\mid s, a)$ | Probability of transition to $s'$ from $s$ taking $a$ |\n",
    "| $r(s, a)$ | Expected immediate reward from $s$ after $a$ |\n",
    "| $r(s, a, s')$ | Expected reward on transition $s \\to s'$ under $a$ |\n",
    "| **Value Functions** |  |\n",
    "| $v_\\pi(s)$ | Value of $s$ under policy $\\pi$ (expected return) |\n",
    "| $v_*(s)$ | Value of $s$ under optimal policy |\n",
    "| $q_\\pi(s, a)$ | Value of taking $a$ in $s$ under $\\pi$ |\n",
    "| $q_*(s, a)$ | Value of taking $a$ in $s$ under optimal policy |\n",
    "| **Estimators & TD Error** |  |\n",
    "| $V, V_t$ | Array estimates of $v_\\pi$ or $v_*$ |\n",
    "| $Q, Q_t$ | Array estimates of $q_\\pi$ or $q_*$ |\n",
    "| $\\hat{V}(s)$ | Expected approximate action value; e.g. $\\hat{V}_t(s) \\doteq \\sum_a \\pi(a|s) Q_t(s, a)$ |\n",
    "| $U_t$ | Target for estimate at time $t$ |\n",
    "| $\\delta_t$ | Temporal-difference (TD) error at $t$ (a random variable) |\n",
    "| $\\delta_t^s$, $\\delta_t^a$ | State- and action-specific forms of TD error |\n",
    "| $n$ | In $n$-step methods, $n$ is number of steps of bootstrapping |\n",
    "---\n",
    "\n",
    "### Function Approximation, Policy Gradient, Advanced Notation\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **Weight & Parameter Notation** |  |\n",
    "| $d$ | Dimensionality—number of components of $\\mathbf{w}$ |\n",
    "| $d'$ | Alternate dimensionality—number of components of $\\theta$ |\n",
    "| $\\mathbf{w}, \\mathbf{w}_t$ | $d$-vector of weights underlying approximate value function |\n",
    "| $w_i, w_{t,i}$ | $i$-th component of learnable weight vector |\n",
    "| $\\hat{v}(s, \\mathbf{w})$ | Approximate value of $s$ given $\\mathbf{w}$ |\n",
    "| $v_\\mathbf{w}(s)$ | Alternate notation for $\\hat{v}(s, \\mathbf{w})$ |\n",
    "| $\\hat{q}(s, a, \\mathbf{w})$ | Approximate value of $(s, a)$ given $\\mathbf{w}$ |\n",
    "| $\\nabla \\hat{v}(s, \\mathbf{w})$ | Column vector of partial derivatives w.r.t. $\\mathbf{w}$ |\n",
    "| $\\nabla \\hat{q}(s, a, \\mathbf{w})$ | Column vector of partial derivatives w.r.t. $\\mathbf{w}$ |\n",
    "| **Feature Vectors & Eligibility Traces** |  |\n",
    "| $\\mathbf{x}(s)$ | Feature vector visible in $s$ |\n",
    "| $\\mathbf{x}(s, a)$ | Feature vector visible in $s$ taking $a$ |\n",
    "| $x_i(s), x_i(s, a)$ | $i$-th component of $\\mathbf{x}(s)$ or $\\mathbf{x}(s, a)$ |\n",
    "| $\\mathbf{x}_t$ | Shorthand for $\\mathbf{x}(S_t)$ or $\\mathbf{x}(S_t, A_t)$ |\n",
    "| $\\mathbf{w}^\\top \\mathbf{x}$ | Inner product of vectors |\n",
    "| $\\mathbf{v}, \\mathbf{v}_t$ | Secondary $d$-vector of weights, used to learn $\\mathbf{w}$ |\n",
    "| $\\mathbf{z}_t$ | $d$-vector of eligibility traces at $t$ |\n",
    "| **Policy Gradient Notation** |  |\n",
    "| $\\theta, \\theta_t$ | Parameter vector of target policy |\n",
    "| $\\pi(a \\mid s, \\theta)$ | Probability of taking $a$ in $s$ given $\\theta$ |\n",
    "| $\\pi_\\theta$ | Policy corresponding to parameter $\\theta$ |\n",
    "| $\\nabla \\pi(a \\mid s, \\theta)$ | Partial derivatives of $\\pi(a \\mid s, \\theta)$ w.r.t. $\\theta$ |\n",
    "| $J(\\theta)$ | Performance measure for policy $\\pi_\\theta$ |\n",
    "| $\\nabla J(\\theta)$ | Partial derivatives of $J(\\theta)$ w.r.t. $\\theta$ |\n",
    "| $h(s, a, \\theta)$ | Preference for $a$ in $s$ based on $\\theta$ |\n",
    "| **Behavior Policy, Baselines, Importance Sampling** |  |\n",
    "| $b(a \\mid s)$ | Behavior policy used to select actions while learning target $\\pi$ |\n",
    "| $b(s)$ | Baseline function $b: \\mathcal{S} \\to \\mathbb{R}$ for policy-gradient methods |\n",
    "| $b$ | Branching factor for MDP/search tree |\n",
    "| $\\rho_{t:h}$ | Importance sampling ratio for $t$ through $h$ |\n",
    "| $\\rho_t$ | Importance sampling ratio for time $t$ alone, $\\rho_t \\doteq \\rho_{t:t}$ |\n",
    "| $r(\\pi)$ | Average reward (reward rate) for policy $\\pi$ |\n",
    "| $\\bar{R}_t$ | Estimate of $r(\\pi)$ at time $t$ |\n",
    "| **State Distributions & Operators** |  |\n",
    "| $\\mu(s)$ | On-policy distribution over states |\n",
    "| $\\mu$ | $\\lvert\\mathcal{S}\\rvert$-vector of the $\\mu(s)$ for $s \\in \\mathcal{S}$ |\n",
    "| $\\|v\\|^2_\\mu$ | $\\mu$-weighted squared norm of $v$, i.e., $\\|v\\|^2_\\mu \\doteq \\sum_{s \\in \\mathcal{S}} \\mu(s)v(s)^2$ |\n",
    "| $\\eta(s)$ | Expected number of visits to $s$ per episode |\n",
    "| $\\Pi$ | Projection operator for value functions |\n",
    "| $B_\\pi$ | Bellman operator for value functions |\n",
    "\n",
    "---\n",
    "\n",
    "### Matrices, Bellman Error, & Error Metrics\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **Matrix Notation & Linear Algebra** |  |\n",
    "| $\\mathbf{A}$ | $d \\times d$ matrix: $\\mathbf{A} \\doteq \\mathbb{E}\\left[ \\mathbf{x}_t(\\mathbf{x}_t - \\gamma \\mathbf{x}_{t+1})^\\top \\right]$ |\n",
    "| $\\mathbf{b}$ | $d$-dimensional vector: $\\mathbf{b} \\doteq \\mathbb{E}[R_{t+1} \\mathbf{x}_t]$ |\n",
    "| $\\mathbf{w}_{TD}$ | TD fixed point: $\\mathbf{w}_{TD} \\doteq \\mathbf{A}^{-1}\\mathbf{b}$ |\n",
    "| $\\mathbf{I}$ | Identity matrix |\n",
    "| $\\mathbf{P}$ | $\\lvert\\mathcal{S}\\rvert \\times \\lvert\\mathcal{S}\\rvert$ matrix of state-transition probabilities under $\\pi$ |\n",
    "| $\\mathbf{D}$ | $\\lvert\\mathcal{S}\\rvert \\times \\lvert\\mathcal{S}\\rvert$ diagonal matrix with $\\mu$ on its diagonal |\n",
    "| $\\mathbf{X}$ | $\\lvert\\mathcal{S}\\rvert \\times d$ matrix with the $\\mathbf{x}(s)$ as its rows |\n",
    "| **Bellman Error & Value Error Metrics** |  |\n",
    "| $\\bar{\\delta}_\\mathbf{w}(s)$ | Bellman error (expected TD error) for $v_\\mathbf{w}$ at $s$ |\n",
    "| $\\bar{\\delta}_\\mathbf{w}$, BE | Bellman error vector (with components $\\bar{\\delta}_\\mathbf{w}(s)$) |\n",
    "| $\\text{VE}(\\mathbf{w})$ | Mean square value error: $\\text{VE}(\\mathbf{w}) \\doteq \\|v_\\mathbf{w} - v_\\pi\\|^2_\\mu$ |\n",
    "| $\\text{BE}(\\mathbf{w})$ | Mean square Bellman error: $\\text{BE}(\\mathbf{w}) \\doteq \\|\\bar{\\delta}_\\mathbf{w}\\|^2_\\mu$ |\n",
    "| $\\text{PBE}(\\mathbf{w})$ | Mean square projected Bellman error: $\\text{PBE}(\\mathbf{w}) \\doteq \\|\\Pi \\bar{\\delta}_\\mathbf{w}\\|^2_\\mu$ |\n",
    "| $\\text{TDE}(\\mathbf{w})$ | Mean square temporal-difference error: $\\text{TDE}(\\mathbf{w}) \\doteq \\mathbb{E}_b[\\rho_t \\delta_t^2]$ |\n",
    "| $\\text{RE}(\\mathbf{w})$ | Mean square return error |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
