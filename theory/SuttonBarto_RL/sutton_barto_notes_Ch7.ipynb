{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1841a00e",
   "metadata": {},
   "source": [
    "# Summary of Notation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402c54e",
   "metadata": {},
   "source": [
    "(full notation in Appendix A)\n",
    "\n",
    "> Whenever you see the word \"value\", think \"action's Expected Value of reward\" (AEVOR). This is usually a $P(win)$ or (in the bandit case) probability-weighted-avg $R_t$, given that that action is taken. It can be either a \"True\" value (i.e. the population mean of the action's distribution) or an estimate (i.e. sample statistic) of it.\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| $\\Pr\\{X = x\\}$ | Probability that random variable $X$ takes value $x$ |\n",
    "| $X \\sim p$ | $X$ is drawn from distribution $p$ |\n",
    "| $\\mathbb{E}[X]$ | Expectation of random variable $X$ |\n",
    "| $\\arg\\max_a f(a)$ | Value of $a$ that maximizes $f(a)$ |\n",
    "| $\\alpha$, $\\beta$, $\\epsilon$ | Step-size, decay-rate, and exploration parameters |\n",
    "| $s, s'$ | States |\n",
    "| $a$ | Action |\n",
    "| $r$ | Reward |\n",
    "| $S, A(s), R$ | Set of states, available actions, rewards |\n",
    "| $t, T$ | Discrete time step, final step |\n",
    "| $S_t, A_t, R_t$ | State, action, and reward at time $t$ |\n",
    "| $\\pi$ | Policy (decision-making rule) |\n",
    "| $\\pi(s)$ | action taken in state $s$ under _deterministic_ $\\pi$ |\n",
    "| $\\pi(a \\mid s)$ | Probability of taking action $a$ in state $s$ under _stochastic_ $\\pi$ |\n",
    "| $G_t$ | Return from time $t$ |\n",
    "| $h$ | horizon (the timestep one looks up to in a forward view) |\n",
    "| $v_\\pi(s)$ | Value of state $s$ under policy $\\pi$ |\n",
    "| $q_\\pi(s, a)$ | Value of state-action pair $(s,a)$ under $\\pi$ |\n",
    "| $p(s', r \\mid s, a)$ | Transition dynamics |\n",
    "| $\\hat{v}(s, \\mathbf{w})$ | Approximate value function using weight vector $\\mathbf{w}$ |\n",
    "| $\\delta_t$ | Temporal-difference error at time $t$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402856b7",
   "metadata": {},
   "source": [
    "# Part I: Tabular Solution Methods\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64caf4a",
   "metadata": {},
   "source": [
    "<img src=\"../img/6.png\" alt=\"tabularsolutionmethods\" width=\"40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab66ed8",
   "metadata": {},
   "source": [
    "# Chapter 7: n-step Bootstrapping\n",
    "---\n",
    "\n",
    "**Chapter Overview**: This chapter bridges Monte Carlo (MC) methods (Chapter 5) and one-step temporal-difference (TD) methods (Chapter 6) by introducing n-step TD methods. These generalize both approaches, allowing a smooth shift between them based on task needs. n-step methods form a spectrum: MC at one end (full episode backups) and one-step TD at the other (single-step backups). Intermediate n often performs best, balancing bias-variance trade-offs.\n",
    "\n",
    "**Key Insight**: Neither pure MC (high variance, low bias) nor one-step TD (low variance, higher bias) is always optimal. n-step methods tune the bootstrapping amount for better performance.\n",
    "\n",
    "**Sidenote**: Bootstrapping means updating estimates based on other estimates (like TD), reducing variance but introducing bias if estimates are poor. MC avoids bootstrapping by waiting for actual returns, but suffers from high variance in long episodes.\n",
    "\n",
    "---\n",
    "\n",
    "## 7.1 n-step TD Prediction\n",
    "\n",
    "**Core Idea**: Estimate value function $v_\\pi$ using n-step updatesâ€”look ahead n steps, then bootstrap from the estimated value at step n.\n",
    "\n",
    "**Spectrum of Methods**:\n",
    "- One-step TD: Update based on immediate reward + bootstrapped next state value.\n",
    "- MC: Update based on all rewards until episode end (no bootstrapping).\n",
    "- n-step TD: Update based on n rewards + bootstrapped value at n steps ahead.\n",
    "\n",
    "![Backup Diagrams for n-step Methods](../img/fig7_1.png)\n",
    "\n",
    "**n-step Return** (target for update, _Equation 7.1_):\n",
    "$$G_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1} R_{t+n} + \\gamma^n V_{t+n-1}(S_{t+n})$$\n",
    "- For $t + n < T$; if $t + n \\geq T$, $G_{t:t+n} = G_t$ (full return).\n",
    "- Components:\n",
    "  - $R_{t+1}$ to $R_{t+n}$: Actual rewards over n steps.\n",
    "  - $\\gamma^n V_{t+n-1}(S_{t+n})$: Bootstrapped estimate correcting for truncation.\n",
    "- Intuition: Approximation to full return $G_t$; truncates after n steps and corrects with current value estimate.\n",
    "\n",
    "**Update Rule** (_Equation 7.2_):\n",
    "$$V_{t+n}(S_t) \\doteq V_{t+n-1}(S_t) + \\alpha [G_{t:t+n} - V_{t+n-1}(S_t)]$$\n",
    "- For $0 \\leq t < T$; values unchanged for unvisited states.\n",
    "- $\\alpha$: Step size.\n",
    "- Intuition: Moves $V(S_t)$ toward the n-step return target.\n",
    "\n",
    "**ðŸ† FOUNDATIONAL FORMULA**: The n-step return blends sampling (actual rewards) and bootstrapping, central to unifying TD and MC.\n",
    "\n",
    "**Algorithm**: Pseudocode for n-step TD (state-value prediction under policy $\\pi$).\n",
    "- Initialize $V(s)$ arbitrarily.\n",
    "- For each episode: Store states/rewards mod $n+1$; compute $G$ and update after n steps (or at end).\n",
    "\n",
    "**Error Reduction Property**: n-step returns reduce error relative to true expected value more than shorter returns (proven for fixed $V$).\n",
    "\n",
    "**Example: Random Walk** (19 states, undiscounted, rewards -1/0/+1 at ends).\n",
    "- Intermediate n (e.g., 4-8) outperforms extremes for various $\\alpha$.\n",
    "\n",
    "![Performance on Random Walk](../img/fig7_2.png)\n",
    "\n",
    "**Sidenote**: Larger tasks favor intermediate n as variance grows with episode length in MC, while one-step TD bootstraps too aggressively early on.\n",
    "\n",
    "---\n",
    "\n",
    "## 7.2 n-step Sarsa\n",
    "\n",
    "**Core Idea**: Extend n-step to on-policy control for $q_\\pi$ estimation.\n",
    "\n",
    "**Backup Diagrams**:\n",
    "\n",
    "![Backup Diagrams for n-step Action-Value Methods](../img/fig7_3.png)\n",
    "\n",
    "**n-step Return for Action Values** (_Equation 7.4_):\n",
    "$$G_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1} R_{t+n} + \\gamma^n Q_{t+n-1}(S_{t+n}, A_{t+n})$$\n",
    "- $t + n < T$; else $G_{t:t+n} = G_t$.\n",
    "\n",
    "**Update Rule** (_Equation 7.5_):\n",
    "$$Q_{t+n}(S_t, A_t) \\doteq Q_{t+n-1}(S_t, A_t) + \\alpha [G_{t:t+n} - Q_{t+n-1}(S_t, A_t)]$$\n",
    "\n",
    "**Algorithm**: Pseudocode for n-step Sarsa.\n",
    "- Similar to n-step TD but for $Q(s,a)$; actions from $\\epsilon$-greedy policy.\n",
    "- Speeds policy improvement by propagating rewards faster (e.g., cliff-walking example).\n",
    "\n",
    "**n-step Expected Sarsa**: Uses expected value over actions at step n (_Equation 7.7_):\n",
    "$$G_{t:t+n} = R_{t+1} + \\cdots + \\gamma^{n-1} R_{t+n} + \\gamma^n \\bar{V}_{t+n-1}(S_{t+n})$$\n",
    "- $\\bar{V}_t(s) = \\sum_a \\pi(a|s) Q_t(s,a)$.\n",
    "- Often better than Sarsa due to lower variance.\n",
    "\n",
    "**Sidenote**: Expected Sarsa generalizes Q-learning (one-step case) to multi-step.\n",
    "\n",
    "---\n",
    "\n",
    "## 7.3 n-step Off-policy Learning\n",
    "\n",
    "**Core Idea**: Learn target policy $\\pi$ from behavior policy $b$ data using importance sampling (IS).\n",
    "\n",
    "**Importance Sampling Ratio** (_Equation 7.8_):\n",
    "$$\\rho_{t:h} = \\prod_{k=t}^{h} \\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}$$\n",
    "- Corrects for action selection differences.\n",
    "\n",
    "**Off-policy n-step TD Update** (_Equation 7.9_):\n",
    "$$V_{t+n}(S_t) \\doteq V_{t+n-1}(S_t) + \\alpha \\rho_{t+1:t+n} [G_{t:t+n} - V_{t+n-1}(S_t)]$$\n",
    "- Generalizes on-policy (ratio=1).\n",
    "\n",
    "**Off-policy n-step Sarsa** (_Equation 7.10_):\n",
    "$$Q_{t+n}(S_t, A_t) \\doteq Q_{t+n-1}(S_t, A_t) + \\alpha \\rho_{t+1:t+n} [G_{t:t+n} - Q_{t+n-1}(S_t, A_t)]$$\n",
    "- Ratio starts at $t+1$ (action $A_t$ already taken).\n",
    "\n",
    "**Algorithm**: Pseudocode for off-policy n-step Sarsa.\n",
    "\n",
    "**Sidenote**: IS can cause high variance if $\\pi$ and $b$ differ much; zero ratio ignores bad samples, high ratio amplifies good ones.\n",
    "\n",
    "---\n",
    "\n",
    "## 7.4 *Per-decision Methods with Control Variates\n",
    "\n",
    "**Core Idea**: Refine off-policy with per-decision IS and control variates (CV) for lower variance.\n",
    "\n",
    "**Recursive n-step Return** (_Equation 7.12_):\n",
    "$$G_{t:h} = R_{t+1} + \\gamma G_{t+1:h}$$\n",
    "\n",
    "**Off-policy Version with CV** (_Equation 7.13_):\n",
    "$$G_{t:h} = R_{t+1} + \\gamma \\rho_t (G_{t+1:h} - Q_t(S_{t+1}, A_{t+1})) + \\gamma \\bar{V}_t(S_{t+1})$$\n",
    "- CV term: Doesn't change expectation but reduces variance by ignoring unlikely branches.\n",
    "- Intuition: If $\\rho_t=0$, target = immediate reward + bootstrapped value (ignore sample).\n",
    "\n",
    "**For Action Values** (_Equation 7.14_):\n",
    "$$G_{t:h} = R_{t+1} + \\gamma (\\rho_{t+1} G_{t+1:h} + (1 - \\rho_{t+1}) Q_t(S_{t+1}, A_{t+1})) + \\gamma (1 - \\rho_{t+1}) (\\bar{V}_t(S_{t+1}) - Q_t(S_{t+1}, A_{t+1}))$$\n",
    "- More complex; first action not importance-sampled.\n",
    "\n",
    "**Sidenote**: CV exploits that unlikely actions under $\\pi$ shouldn't bias updates; expected update unchanged.\n",
    "\n",
    "---\n",
    "\n",
    "## 7.5 Off-policy Learning Without Importance Sampling: The n-step Tree Backup Algorithm\n",
    "\n",
    "**Core Idea**: Multi-step generalization of Expected Sarsa/Q-learning; branches over all actions (no IS).\n",
    "\n",
    "**Backup Diagram**: Tree structureâ€”sample spine, expected branches off states.\n",
    "\n",
    "**n-step Return** (_Equation 7.15_ for 1-step, general recursive _Equation 7.17_):\n",
    "$$G_{t:t+n} = R_{t+1} + \\gamma \\sum_{a \\neq A_{t+1}} \\pi(a|S_{t+1}) Q_{t+n-1}(S_{t+1}, a) + \\gamma \\pi(A_{t+1}|S_{t+1}) G_{t+1:t+n}$$\n",
    "- Weights by $\\pi$; samples only taken action.\n",
    "\n",
    "**Update**: Same as Expected Sarsa but with tree return.\n",
    "\n",
    "**Algorithm**: Pseudocode for n-step Tree Backup.\n",
    "\n",
    "**Sidenote**: Avoids IS variance; effective when policies differ, but bootstraps less if $\\pi(A_t|S_t)$ small.\n",
    "\n",
    "---\n",
    "\n",
    "## 7.6 *A Unifying Algorithm: n-step $Q(\\sigma)$\n",
    "\n",
    "**Core Idea**: Generalizes previous methods with per-step sampling degree $\\sigma_k \\in [0,1]$.\n",
    "\n",
    "**Backup Diagrams**:\n",
    "\n",
    "![Backup Diagrams for Q(sigma)](../img/fig7_5.png)\n",
    "\n",
    "**n-step Return** (_Equation 7.18_):\n",
    "$$G_{t:h} = R_{t+1} + \\gamma [\\sigma_{t+1} \\rho_{t+1} (G_{t+1:h} - Q_{t}(S_{t+1}, A_{t+1})) + (1 - \\sigma_{t+1} \\rho_{t+1}) \\bar{V}_{t}(S_{t+1})] + \\gamma \\sigma_{t+1} \\rho_{t+1} Q_{t}(S_{t+1}, A_{t+1})$$\n",
    "- $\\sigma=1$: Full sampling (like Sarsa with IS).\n",
    "- $\\sigma=0$: Expectation (like Tree Backup).\n",
    "- Intuition: Blends sampling (high variance, low bias) and expectation flexibly.\n",
    "\n",
    "**Algorithm**: Pseudocode for off-policy n-step $Q(\\sigma)$.\n",
    "\n",
    "**ðŸ† FOUNDATIONAL**: Unifies on/off-policy, TD/MC, Sarsa/Q-learning; adaptable via $\\sigma$.\n",
    "\n",
    "---\n",
    "\n",
    "## 7.7 Summary\n",
    "\n",
    "**Key Concepts**:\n",
    "- n-step methods: Tune bootstrapping degree for optimal bias-variance.\n",
    "- On-policy: n-step TD/Sarsa/Expected Sarsa.\n",
    "- Off-policy: IS-based (simple but high variance), Tree Backup (no IS), $Q(\\sigma)$ (unifying).\n",
    "- Advantages: Faster propagation, better than extremes.\n",
    "- Trade-offs: More computation/memory than one-step; variance issues in off-policy.\n",
    "\n",
    "**Fundamental Takeaways**:\n",
    "1. Multi-step bootstrapping accelerates learning by balancing immediacy and completeness.\n",
    "2. Off-policy needs correction (IS or trees) for policy mismatch.\n",
    "3. $Q(\\sigma)$: Flexible framework for future algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 7.1 Solution\n",
    "**Q**: In learning curves such as those shown in Figure 7.2, asymptote is offset slightly above 0 due to the off-by-0.5-step problem (random walk starts in center state 10, true values range 1/20 to 19/20). Can you explain why? What would the asymptotic error be if $\\gamma=0.9$?\n",
    "\n",
    "**A**: \n",
    "- Asymptotic RMS error isn't zero because learned values approximate $v_\\pi$, but true values are offset (e.g., from center, expected return is average of left/right paths).\n",
    "- With $\\gamma=0.9$, error scales by discount; asymptotic error would be $0.5 \\times (1 - \\gamma) = 0.05$ (general undiscounted bias persists, but discounted reduces effective offset).\n",
    "\n",
    "### Exercise 7.2 Solution\n",
    "**Q**: Why does off-policy MC control require $\\pi$ is greedy wrt $Q$, while on-policy MC does not?\n",
    "\n",
    "**A**: Off-policy separates behavior ($b$, exploratory) and target ($\\pi$, greedy). Greedy $\\pi$ ensures optimality; on-policy uses single improving policy, converging to optimal without explicit greediness.\n",
    "\n",
    "### Exercise 7.3 Solution\n",
    "**Q**: Why larger random walk (19 vs 5 states)? Effect of smaller walk or left reward -1 vs 0 on best n?\n",
    "\n",
    "**A**: \n",
    "- Larger: Amplifies variance in MC (longer episodes), shifts optimum n rightward.\n",
    "- Smaller: Favors larger n (closer to MC).\n",
    "- -1 left: Increases variance asymmetry; may favor slightly smaller n for stability.\n",
    "\n",
    "### Exercise 7.4 Solution\n",
    "**Q**: Prove n-step Sarsa return equals novel TD error form.\n",
    "\n",
    "**A**:\n",
    "Start with $G_{t:t+n} = Q_{t-1}(S_t, A_t) + \\sum_{k=t}^{t+n-1} \\gamma^{k-t} [R_{k+1} + \\gamma Q_k(S_{k+1}, A_{k+1}) - Q_{k-1}(S_k, A_k)]$.\n",
    "- By induction: Base $n=1$ holds; assume for n-1, expand recursively.\n",
    "\n",
    "### Exercise 7.5 Solution\n",
    "**Q**: Pseudocode for off-policy state-value prediction with CV.\n",
    "\n",
    "**A**:\n",
    "```python\n",
    "# Off-policy n-step TD with per-decision IS and CV for V â‰ˆ v_Ï€\n",
    "# Input: policies Ï€, b; Î±; n\n",
    "# Initialize V(s) arbitrarily\n",
    "# For each episode:\n",
    "#   Generate episode using b: S0, R1, S1, ..., RT\n",
    "#   For t = 0 to T-n:\n",
    "#       G = V(St+n)  # or adjust if terminal\n",
    "#       For k = t+n-1 downto t:\n",
    "#           Ï = Ï€(Ak|Sk) / b(Ak|Sk)\n",
    "#           G = R_{k+1} + Î³ * Ï * (G - V(S_{k+1})) + Î³ * V(S_{k+1})\n",
    "#       V(St) += Î± * (G - V(St))\n",
    "```\n",
    "\n",
    "### Exercise 7.6 Solution\n",
    "Q: Why per-decision methods better than ordinary IS for off-policy MC?\n",
    "\n",
    "A: Per-decision applies IS only to relevant steps, adding CV to reduce variance (ignores low-prob branches without zeroing entire return).\n",
    "\n",
    "### Exercise 7.7 Solution\n",
    "Q: Prove per-decision off-policy return expectation equals on-policy return.\n",
    "\n",
    "A: By induction on horizon h:\n",
    "\n",
    "Base: $E[G_{h-1:h}] = E[R_h] = v_Ï€(S_{h-1})$.\n",
    "Assume for t+1:h; then $E[G_{t:h}] = E[R_{t+1} + Î³ Ï_t G_{t+1:h} + Î³ (1 - Ï_t) V(S_{t+1})] = ... = v_Ï€(S_t)$, since $E[Ï_t]=1$, CV expectation 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cf86eb",
   "metadata": {},
   "source": [
    "# Appendix A: Full Notation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c0755",
   "metadata": {},
   "source": [
    "### General Notation\n",
    "\n",
    "> Capital letters are used for random variables.\n",
    "\n",
    "> Lower case letters are used for the values of random variables and for scalar functions.\n",
    "\n",
    "> Quantities that are required to be real-valued vectors are written in bold and in lower case (even if random variables). \n",
    "\n",
    "> Matrices are bold capitals.\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **Standard Operators** |  |\n",
    "| $\\doteq$ | Equality relationship that is true by definition |\n",
    "| $\\approx$ | Approximately equal |\n",
    "| $\\propto$ | Proportional to |\n",
    "| $\\Pr\\{X = x\\}$ | Probability that random variable $X$ takes value $x$ |\n",
    "| $X \\sim p$ | $X$ drawn from distribution $p$ |\n",
    "| $\\mathbb{E}[X]$ | Expectation of random variable $X$ |\n",
    "| $\\arg\\max_a f(a)$ | Value of $a$ that maximizes $f(a)$ |\n",
    "| $\\ln x$ | Natural logarithm of $x$ |\n",
    "| $e^x$, $\\exp(x)$ | The base of the natural logarithm, $e \\approx 2.71828$, carried to power $x$ |\n",
    "| $\\mathbb{R}$ | Set of real numbers |\n",
    "| $f: \\mathcal{X} \\to \\mathcal{Y}$ | Function from elements of set $\\mathcal{X}$ to elements of set $\\mathcal{Y}$ |\n",
    "| $\\leftarrow$ | Assignment |\n",
    "| $(a, b]$ | Real interval between $a$ and $b$ including $b$ but not $a$ |\n",
    "| **Standard RL Algorithm Parameters** |  |\n",
    "| $\\epsilon$ | Probability of taking a random action in an $\\epsilon$-greedy policy |\n",
    "| $\\alpha, \\beta$ | Step-size parameters |\n",
    "| $\\gamma$ | Discount-rate parameter |\n",
    "| $\\lambda$ | Decay-rate parameter for eligibility traces |\n",
    "| $\\mathbf{1}_\\text{predicate}$ | Indicator function (1 if predicate is true, else 0) |\n",
    "\n",
    "---\n",
    "\n",
    "### Multi-Armed Bandit Notation\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| $k$ | Number of actions (arms) |\n",
    "| $t$ | Discrete time step or play number |\n",
    "| $q_*(a)$ | True value (expected reward) of action $a$ |\n",
    "| $Q_t(a)$ | Estimate at time $t$ of $q_*(a)$ |\n",
    "| $N_t(a)$ | Number of times action $a$ has been selected up to time $t$ |\n",
    "| $H_t(a)$ | Learned preference for selecting action $a$ at time $t$ |\n",
    "| $\\pi_t(a)$ | Probability of selecting action $a$ at time $t$ |\n",
    "| $\\bar{R}_t$ | Estimate at time $t$ of the expected reward given $\\pi_t$ |\n",
    "\n",
    "---\n",
    "\n",
    "### Markov Decision Process Notation\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **State & Action Sets** |  |\n",
    "| $s, s'$ | States |\n",
    "| $a$ | An action |\n",
    "| $r$ | A reward |\n",
    "| $\\mathcal{S}$ | Set of all nonterminal states |\n",
    "| $\\mathcal{S}^+$ | Set of all states, incl. terminal state |\n",
    "| $\\mathcal{A}(s)$ | Set of all actions available in state $s$ |\n",
    "| $\\mathcal{R}$ | Set of all possible rewards, a finite subset of $\\mathbb{R}$ |\n",
    "| $\\mathcal{C}$ | Subset of (e.g., $\\mathcal{R} \\subset \\mathbb{R}$) |\n",
    "| $\\in$ | Is an element of (e.g. $s \\in \\mathcal{S}$, $r \\in \\mathcal{R}$) |\n",
    "| $\\lvert\\mathcal{S}\\rvert$ | Number of elements in set $\\mathcal{S}$ |\n",
    "| **Time & Policy** |  |\n",
    "| $t$ | Discrete time step |\n",
    "| $T, T(t)$ | Final time step of episode, or including $t$ |\n",
    "| $A_t$ | Action at time $t$ |\n",
    "| $S_t$ | State at time $t$ |\n",
    "| $R_t$ | Reward at time $t$ |\n",
    "| $\\pi$ | Policy (decision-making rule) |\n",
    "| $\\pi(s)$ | Action taken in state $s$ under deterministic $\\pi$ |\n",
    "| $\\pi(a \\mid s)$ | Probability of taking action $a$ in state $s$ under stochastic $\\pi$ |\n",
    "| **Returns** |  |\n",
    "| $G_t$ | Return following time $t$ |\n",
    "| $h$ | Horizon (timestep looked up to in forward view) |\n",
    "| $G_{t:t+n}, G_{t:h}$ | $n$-step return from $t+1$ to $t+n$ or $h$ (discounted/corrected) |\n",
    "| $G_{t:h}$ | Flat return (undiscounted/unadjusted) from $t+1$ to $h$ |\n",
    "| $G^\\lambda_t$ | $\\lambda$-return |\n",
    "| $G^{\\wedge}_t$, $G^{\\wedge a}_t$ | Truncated, corrected $\\lambda$-return |\n",
    "| **Transition & Reward Probabilities** |  |\n",
    "| $p(s', r \\mid s, a)$ | Probability of transition to $s'$ with reward $r$ from $s, a$ |\n",
    "| $p(s' \\mid s, a)$ | Probability of transition to $s'$ from $s$ taking $a$ |\n",
    "| $r(s, a)$ | Expected immediate reward from $s$ after $a$ |\n",
    "| $r(s, a, s')$ | Expected reward on transition $s \\to s'$ under $a$ |\n",
    "| **Value Functions** |  |\n",
    "| $v_\\pi(s)$ | Value of $s$ under policy $\\pi$ (expected return) |\n",
    "| $v_*(s)$ | Value of $s$ under optimal policy |\n",
    "| $q_\\pi(s, a)$ | Value of taking $a$ in $s$ under $\\pi$ |\n",
    "| $q_*(s, a)$ | Value of taking $a$ in $s$ under optimal policy |\n",
    "| **Estimators & TD Error** |  |\n",
    "| $V, V_t$ | Array estimates of $v_\\pi$ or $v_*$ |\n",
    "| $Q, Q_t$ | Array estimates of $q_\\pi$ or $q_*$ |\n",
    "| $\\hat{V}(s)$ | Expected approximate action value; e.g. $\\hat{V}_t(s) \\doteq \\sum_a \\pi(a|s) Q_t(s, a)$ |\n",
    "| $U_t$ | Target for estimate at time $t$ |\n",
    "| $\\delta_t$ | Temporal-difference (TD) error at $t$ (a random variable) |\n",
    "| $\\delta_t^s$, $\\delta_t^a$ | State- and action-specific forms of TD error |\n",
    "| $n$ | In $n$-step methods, $n$ is number of steps of bootstrapping |\n",
    "---\n",
    "\n",
    "### Function Approximation, Policy Gradient, Advanced Notation\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **Weight & Parameter Notation** |  |\n",
    "| $d$ | Dimensionalityâ€”number of components of $\\mathbf{w}$ |\n",
    "| $d'$ | Alternate dimensionalityâ€”number of components of $\\theta$ |\n",
    "| $\\mathbf{w}, \\mathbf{w}_t$ | $d$-vector of weights underlying approximate value function |\n",
    "| $w_i, w_{t,i}$ | $i$-th component of learnable weight vector |\n",
    "| $\\hat{v}(s, \\mathbf{w})$ | Approximate value of $s$ given $\\mathbf{w}$ |\n",
    "| $v_\\mathbf{w}(s)$ | Alternate notation for $\\hat{v}(s, \\mathbf{w})$ |\n",
    "| $\\hat{q}(s, a, \\mathbf{w})$ | Approximate value of $(s, a)$ given $\\mathbf{w}$ |\n",
    "| $\\nabla \\hat{v}(s, \\mathbf{w})$ | Column vector of partial derivatives w.r.t. $\\mathbf{w}$ |\n",
    "| $\\nabla \\hat{q}(s, a, \\mathbf{w})$ | Column vector of partial derivatives w.r.t. $\\mathbf{w}$ |\n",
    "| **Feature Vectors & Eligibility Traces** |  |\n",
    "| $\\mathbf{x}(s)$ | Feature vector visible in $s$ |\n",
    "| $\\mathbf{x}(s, a)$ | Feature vector visible in $s$ taking $a$ |\n",
    "| $x_i(s), x_i(s, a)$ | $i$-th component of $\\mathbf{x}(s)$ or $\\mathbf{x}(s, a)$ |\n",
    "| $\\mathbf{x}_t$ | Shorthand for $\\mathbf{x}(S_t)$ or $\\mathbf{x}(S_t, A_t)$ |\n",
    "| $\\mathbf{w}^\\top \\mathbf{x}$ | Inner product of vectors |\n",
    "| $\\mathbf{v}, \\mathbf{v}_t$ | Secondary $d$-vector of weights, used to learn $\\mathbf{w}$ |\n",
    "| $\\mathbf{z}_t$ | $d$-vector of eligibility traces at $t$ |\n",
    "| **Policy Gradient Notation** |  |\n",
    "| $\\theta, \\theta_t$ | Parameter vector of target policy |\n",
    "| $\\pi(a \\mid s, \\theta)$ | Probability of taking $a$ in $s$ given $\\theta$ |\n",
    "| $\\pi_\\theta$ | Policy corresponding to parameter $\\theta$ |\n",
    "| $\\nabla \\pi(a \\mid s, \\theta)$ | Partial derivatives of $\\pi(a \\mid s, \\theta)$ w.r.t. $\\theta$ |\n",
    "| $J(\\theta)$ | Performance measure for policy $\\pi_\\theta$ |\n",
    "| $\\nabla J(\\theta)$ | Partial derivatives of $J(\\theta)$ w.r.t. $\\theta$ |\n",
    "| $h(s, a, \\theta)$ | Preference for $a$ in $s$ based on $\\theta$ |\n",
    "| **Behavior Policy, Baselines, Importance Sampling** |  |\n",
    "| $b(a \\mid s)$ | Behavior policy used to select actions while learning target $\\pi$ |\n",
    "| $b(s)$ | Baseline function $b: \\mathcal{S} \\to \\mathbb{R}$ for policy-gradient methods |\n",
    "| $b$ | Branching factor for MDP/search tree |\n",
    "| $\\rho_{t:h}$ | Importance sampling ratio for $t$ through $h$ |\n",
    "| $\\rho_t$ | Importance sampling ratio for time $t$ alone, $\\rho_t \\doteq \\rho_{t:t}$ |\n",
    "| $r(\\pi)$ | Average reward (reward rate) for policy $\\pi$ |\n",
    "| $\\bar{R}_t$ | Estimate of $r(\\pi)$ at time $t$ |\n",
    "| **State Distributions & Operators** |  |\n",
    "| $\\mu(s)$ | On-policy distribution over states |\n",
    "| $\\mu$ | $\\lvert\\mathcal{S}\\rvert$-vector of the $\\mu(s)$ for $s \\in \\mathcal{S}$ |\n",
    "| $\\|v\\|^2_\\mu$ | $\\mu$-weighted squared norm of $v$, i.e., $\\|v\\|^2_\\mu \\doteq \\sum_{s \\in \\mathcal{S}} \\mu(s)v(s)^2$ |\n",
    "| $\\eta(s)$ | Expected number of visits to $s$ per episode |\n",
    "| $\\Pi$ | Projection operator for value functions |\n",
    "| $B_\\pi$ | Bellman operator for value functions |\n",
    "\n",
    "---\n",
    "\n",
    "### Matrices, Bellman Error, & Error Metrics\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **Matrix Notation & Linear Algebra** |  |\n",
    "| $\\mathbf{A}$ | $d \\times d$ matrix: $\\mathbf{A} \\doteq \\mathbb{E}\\left[ \\mathbf{x}_t(\\mathbf{x}_t - \\gamma \\mathbf{x}_{t+1})^\\top \\right]$ |\n",
    "| $\\mathbf{b}$ | $d$-dimensional vector: $\\mathbf{b} \\doteq \\mathbb{E}[R_{t+1} \\mathbf{x}_t]$ |\n",
    "| $\\mathbf{w}_{TD}$ | TD fixed point: $\\mathbf{w}_{TD} \\doteq \\mathbf{A}^{-1}\\mathbf{b}$ |\n",
    "| $\\mathbf{I}$ | Identity matrix |\n",
    "| $\\mathbf{P}$ | $\\lvert\\mathcal{S}\\rvert \\times \\lvert\\mathcal{S}\\rvert$ matrix of state-transition probabilities under $\\pi$ |\n",
    "| $\\mathbf{D}$ | $\\lvert\\mathcal{S}\\rvert \\times \\lvert\\mathcal{S}\\rvert$ diagonal matrix with $\\mu$ on its diagonal |\n",
    "| $\\mathbf{X}$ | $\\lvert\\mathcal{S}\\rvert \\times d$ matrix with the $\\mathbf{x}(s)$ as its rows |\n",
    "| **Bellman Error & Value Error Metrics** |  |\n",
    "| $\\bar{\\delta}_\\mathbf{w}(s)$ | Bellman error (expected TD error) for $v_\\mathbf{w}$ at $s$ |\n",
    "| $\\bar{\\delta}_\\mathbf{w}$, BE | Bellman error vector (with components $\\bar{\\delta}_\\mathbf{w}(s)$) |\n",
    "| $\\text{VE}(\\mathbf{w})$ | Mean square value error: $\\text{VE}(\\mathbf{w}) \\doteq \\|v_\\mathbf{w} - v_\\pi\\|^2_\\mu$ |\n",
    "| $\\text{BE}(\\mathbf{w})$ | Mean square Bellman error: $\\text{BE}(\\mathbf{w}) \\doteq \\|\\bar{\\delta}_\\mathbf{w}\\|^2_\\mu$ |\n",
    "| $\\text{PBE}(\\mathbf{w})$ | Mean square projected Bellman error: $\\text{PBE}(\\mathbf{w}) \\doteq \\|\\Pi \\bar{\\delta}_\\mathbf{w}\\|^2_\\mu$ |\n",
    "| $\\text{TDE}(\\mathbf{w})$ | Mean square temporal-difference error: $\\text{TDE}(\\mathbf{w}) \\doteq \\mathbb{E}_b[\\rho_t \\delta_t^2]$ |\n",
    "| $\\text{RE}(\\mathbf{w})$ | Mean square return error |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
