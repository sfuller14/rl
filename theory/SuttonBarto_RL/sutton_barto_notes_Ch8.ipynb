{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c5e439",
   "metadata": {},
   "source": [
    "# Chapter 8: Planning and Learning with Tabular Methods\n",
    "\n",
    "**Chapter Overview**: This chapter unifies model-based methods (like dynamic programming, which use planning with an environment model) and model-free methods (like Monte Carlo and TD learning, which rely directly on experience). It introduces integrated architectures like Dyna that combine learning and planning. Key themes include using models to simulate experience, the trade-offs between expected and sample updates, focusing backups efficiently, and decision-time planning techniques like MCTS. As the capstone of Part I (tabular methods), it recaps core dimensions of RL methods and bridges to Part II's focus on scaling via function approximation.\n",
    "\n",
    "**Key Unification Insight**: All RL methods compute value functions via backups (looking ahead to future events). Planning uses simulated experience from a model; learning uses real experience. This allows seamless integration: apply learning algorithms to model-generated data for planning.\n",
    "\n",
    "**Sidenote on Planning vs. Learning**: Planning is \"deliberative\" (model-based foresight), learning is \"reactive\" (experience-based adaptation). But they're not opposites—many methods blend them. Intuitively, planning is like mentally rehearsing scenarios; learning is trial-and-error in the real world.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.1 Models and Planning\n",
    "\n",
    "**Models**: Anything an agent uses to predict environment responses to actions.\n",
    "\n",
    "- **Given**: State $s$, action $a$\n",
    "- **Predicts**: Next state $s'$ and reward $r$\n",
    "\n",
    "**Types of Models**:\n",
    "- **Distribution model**: Gives all possible $s', r$ and their probabilities (e.g., DP's $p(s', r|s, a)$ from Equation 3.2).\n",
    "- **Sample model**: Samples one possible $s', r$ according to probabilities (e.g., blackjack simulation in Chapter 5).\n",
    "\n",
    "**Intuition**: Distribution models are \"stronger\" (can generate samples and probabilities) but harder to obtain. Sample models are easier (e.g., simulate dice rolls) but may introduce sampling variance.\n",
    "\n",
    "**Sidenote**: Models simulate experience. From a state and policy, generate episodes (trajectories). This \"simulated experience\" can train RL methods just like real experience.\n",
    "\n",
    "**Planning**: Computation to improve policy/value function using a model.\n",
    "- **State-space planning**: Focus of RL (vs. plan-space planning like evolutionary methods).\n",
    "- **Core Structure**: Compute values via backups on simulated experience.\n",
    "\n",
    "**Diagram Insight**: All state-space planning fits: (1) Compute values as intermediate step; (2) Use backups on simulated experience.\n",
    "\n",
    "![Common Structure of Planning and Learning](../img/fig8_1.png)\n",
    "\n",
    "**Unification with Learning**: Replace real experience in learning methods (e.g., TD) with model-simulated experience for planning. Both estimate values via backups; difference is experience source.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.2 Dyna: Integrated Planning, Acting, and Learning\n",
    "\n",
    "**Dyna Architecture**: Online agent integrating planning, acting, model-learning, and direct RL (all in parallel).\n",
    "\n",
    "- **Direct RL**: Improve value/policy from real experience (e.g., Q-learning).\n",
    "- **Model-learning**: Update model from real experience.\n",
    "- **Planning**: Indirect RL—improve value/policy from simulated experience.\n",
    "\n",
    "**Diagram**:\n",
    "\n",
    "![Dyna Architecture](../img/fig8_1.png)  <!-- Note: This is Figure 8.1 from the text -->\n",
    "\n",
    "**Relationships** (see diagram on page 163):\n",
    "- Experience → Model (model-learning)\n",
    "- Experience → Value/Policy (direct RL)\n",
    "- Model → Simulated Experience → Value/Policy (planning)\n",
    "\n",
    "**Search Control**: Process selecting starting states/actions for simulated experiences (e.g., random from experienced pairs).\n",
    "\n",
    "**Tabular Dyna-Q Algorithm** (pseudocode on page 164):\n",
    "- Initialize $Q(s,a)$ and Model$(s,a)$\n",
    "- Loop forever:\n",
    "  - (a) $S$ ← current state\n",
    "  - (b) $A$ ← ε-greedy$(S, Q)$\n",
    "  - (c) Execute $A$, observe $R, S'$\n",
    "  - (d) $Q(S,A) \\leftarrow Q(S,A) + \\alpha [R + \\gamma \\max_{a'} Q(S',a') - Q(S,A)]$  (direct RL, one-step tabular Q-learning)\n",
    "  - (e) Model$(S,A) \\leftarrow R, S'$  (assuming deterministic env)\n",
    "  - (f) For $n$ planning steps: Random experienced $S_{rand}, A_{rand}$; Simulate $R, S'$ from Model; Update $Q$ as in (d)\n",
    "\n",
    "**Key Parameters**:\n",
    "- $n$: Planning steps per real step (balances computation).\n",
    "- Assumes deterministic model; queries only experienced pairs.\n",
    "\n",
    "**Example: Dyna Maze** (Figure 8.2):\n",
    "- Task: From S to G quickly.\n",
    "- Dyna-Q with more $n$ learns faster (e.g., $n=50$ builds extensive policy in one episode).\n",
    "\n",
    "![Dyna Maze Learning Curves](../img/fig8_2.png)\n",
    "\n",
    "![Policies During Learning](../img/fig8_3.png)\n",
    "\n",
    "**Intuition**: Planning builds \"mental models\" to accelerate learning. Without planning ($n=0$), policy grows one step per episode; with planning, it expands rapidly via backups.\n",
    "\n",
    "**Sidenote**: Dyna-Q unifies: Same Q-update for real/simulated experience. Agent is always acting, planning, learning—model improves as experience grows.\n",
    "\n",
    "**Exercise Insight**: Multi-step methods (Ch. 7) might match Dyna if model accurate, but Dyna's one-step simplicity + many simulations often wins.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.3 When the Model Is Wrong\n",
    "\n",
    "**Issue**: Models can be inaccurate (initially empty or environment changes).\n",
    "\n",
    "**Example: Blocking Maze** (Figure 8.4):\n",
    "- Path blocks after 1000 steps; new path opens.\n",
    "- Dyna-Q recovers but wanders initially.\n",
    "\n",
    "**Worse Case: Shortcut Maze** (Figure 8.5):\n",
    "- Shortcut opens; Dyna-Q misses it (model says impossible, ε-greedy rarely explores enough).\n",
    "\n",
    "**Solution: Dyna-Q+**:\n",
    "- Track time $τ$ since last real try of $(s,a)$.\n",
    "- Add exploration bonus: Simulated reward $r + κ\\sqrt{τ}$ (encourages testing stale transitions).\n",
    "- Solves shortcut by exploring long-untried actions.\n",
    "\n",
    "![Blocking Maze Performance](../img/fig8_4.png)\n",
    "\n",
    "![Shortcut Maze Performance](../img/fig8_5.png)\n",
    "\n",
    "**Intuition**: Wrong models cause \"model traps.\" Add curiosity (bonus for uncertainty) to detect changes.\n",
    "\n",
    "**Sidenote**: Stochastic envs: Average model over experiences. Changing envs: Weight recent experiences more.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.4 Prioritized Sweeping\n",
    "\n",
    "**Issue**: Uniform random planning (Dyna-Q) wastes updates on low-impact states.\n",
    "\n",
    "**Solution**: Focus backups backward from high-change states (like reverse BFS).\n",
    "\n",
    "**Prioritized Sweeping Algorithm** (for deterministic env, pseudocode on page 170):\n",
    "- Use priority queue: State-action pairs prioritized by update magnitude $P = |R + \\gamma \\max_a Q(S',a) - Q(S,A)|$.\n",
    "- If $P > θ$ (small threshold), insert/update queue.\n",
    "- Update top pair; propagate changes to predecessors.\n",
    "\n",
    "**Intuition**: Chain reactions—big changes in one state affect predecessors. Queue ensures efficient backward focus.\n",
    "\n",
    "**Advantages**:\n",
    "- Faster than uniform (e.g., maze: 3x fewer backups for same performance).\n",
    "- Stochastic extension: Sample successors.\n",
    "\n",
    "**Example: Rod Maneuvering** (Figure 8.6): Solves large (14k states) deterministic task efficiently.\n",
    "\n",
    "![Prioritized Sweeping on Maze](../img/fig8_6.png)  <!-- Assuming this is the rod figure or maze comparison -->\n",
    "\n",
    "**Sidenote**: Like heuristic search but retains values long-term. \"Small backups\" (van Seijen & Sutton, 2013) take this further: Probability-weighted single transitions without sampling.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.5 Expected vs. Sample Updates\n",
    "\n",
    "**Dimensions of Updates**:\n",
    "- State vs. action values.\n",
    "- Optimal vs. arbitrary policy.\n",
    "- Expected (all successors) vs. sample (one successor).\n",
    "\n",
    "**Trade-off**: Expected: Accurate but compute-heavy (branching factor $b$). Sample: Noisy but cheap.\n",
    "\n",
    "**Efficiency Insight** (Figure 8.7): Sample often better for large $b$ (reduces error faster per computation).\n",
    "\n",
    "![Expected vs Sample Efficiency](../img/fig8_7.png)\n",
    "\n",
    "**General Rule**: Sample wins for large $b$, deep backups; expected for small $b$, shallow.\n",
    "\n",
    "**Sidenote**: Planning favors samples (easy from model); real experience favors expected (to reduce variance).\n",
    "\n",
    "---\n",
    "\n",
    "## 8.6 Trajectory Sampling\n",
    "\n",
    "**Update Distribution**: Where to focus backups?\n",
    "- **Uniform**: All states equally (like DP sweeps)—wastes on irrelevant states.\n",
    "- **On-policy**: Sample trajectories following current policy—focuses on likely states.\n",
    "\n",
    "**Advantages of On-Policy** (Figure 8.8): Faster initial learning (relevant states); but uniform may win long-term.\n",
    "\n",
    "![Uniform vs On-Policy](../img/fig8_8.png)\n",
    "\n",
    "**Intuition**: Like importance sampling—prioritize states under current behavior.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.7 Real-time Dynamic Programming (RTDP)\n",
    "\n",
    "**RTDP**: Asynchronous value iteration with on-policy trajectory sampling.\n",
    "- Update only visited states greedily.\n",
    "- Converges to optimal (with exploring starts).\n",
    "\n",
    "**Advantages**: Ignores irrelevant states; policy improves during computation.\n",
    "\n",
    "**Example: Racetrack** (Exercise 5.12): RTDP updates fewer states, finds near-optimal policy faster than full sweeps.\n",
    "\n",
    "**Intuition**: \"Relevant states\" are reachable under optimal policy—RTDP naturally focuses there without visiting all.\n",
    "\n",
    "**Sidenote**: Partial policies (optimal only on relevant states) suffice; RTDP guarantees this without infinite visits everywhere.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.8 Planning at Decision Time\n",
    "\n",
    "**Background Planning**: Improve overall policy/value (e.g., Dyna)—unfocused.\n",
    "- **Decision-time Planning**: From current state, search deeply to select one action (discard backups after).\n",
    "\n",
    "**Intuition**: Focus computation on now; useful when fast responses aren't needed (e.g., games).\n",
    "\n",
    "---\n",
    "\n",
    "## 8.9 Heuristic Search\n",
    "\n",
    "**Classical AI Method**: From current state (root), build search tree; evaluate leaves with heuristic $v$; backup maxes to root; pick best action.\n",
    "\n",
    "**RL Twist**: Improve heuristic $v$ over time via backups—blends with learning.\n",
    "\n",
    "![Heuristic Search as Backups](../img/fig8_9.png)\n",
    "\n",
    "**Intuition**: Deep search = many small backups focused downstream. Better than unfocused for current decision.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.10 Rollout Algorithms\n",
    "\n",
    "**Method**: Monte Carlo control from current state: Simulate trajectories per action using rollout policy $\\pi$; average returns for $q(s,a)$; pick $\\arg\\max_a$.\n",
    "\n",
    "**No Storage**: Discard estimates after action choice.\n",
    "\n",
    "**Improvement**: Policy iteration—rollouts improve over $\\pi$ (if $\\pi$ good, rollouts better).\n",
    "\n",
    "**Intuition**: Like MC but decision-time; balances trials vs. depth (truncate + bootstrap if needed).\n",
    "\n",
    "**Sidenote**: Parallelizable; prune poor actions. Enhances weak $\\pi$ via lookahead.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.11 Monte Carlo Tree Search (MCTS)\n",
    "\n",
    "**Method**: Build asymmetric tree from current state via iterations:\n",
    "1. **Selection**: Tree policy (e.g., UCB) to leaf.\n",
    "2. **Expansion**: Add child(ren).\n",
    "3. **Simulation**: Rollout policy to end; get return.\n",
    "4. **Backup**: Update $Q$ along path (average returns).\n",
    "\n",
    "Pick action with best root $Q$.\n",
    "\n",
    "![MCTS Process](../img/fig8_10.png)\n",
    "\n",
    "**Intuition**: Tree focuses on promising branches; rollouts explore beyond. Balances exploration/exploitation via tree policy.\n",
    "\n",
    "**RL View**: MC control with partial $q$ table (tree); discards between moves.\n",
    "\n",
    "**Sidenote**: AlphaGo (Ch. 16) extends with neural nets. MCTS revolutionized AI games by smart sampling.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.12 Summary of the Chapter\n",
    "\n",
    "**Recap**: Models enable planning (simulated backups). Dyna integrates all; prioritized/trajectory sampling focuses efficiently. Expected/sample, background/decision-time are key axes. Rollout/MCTS: Powerful decision-time methods.\n",
    "\n",
    "**Bridge to Part II**: Part I assumed tabular (exact values per state)—limits to small problems. Part II scales via function approximation: Approximate values over large spaces, enabling real-world RL.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.13 Summary of Part I: Dimensions\n",
    "\n",
    "**Core Dimensions** (Figure 8.11):\n",
    "- **Depth**: Shallow (one-step) to deep (full episodes).\n",
    "- **Width**: Expected (all branches) to sample (one branch).\n",
    "- **On/Off-Policy**: Learn current vs. optimal policy.\n",
    "\n",
    "![Dimensions of RL Methods](../img/fig8_11.png)\n",
    "\n",
    "**Other Dimensions**:\n",
    "- Episodic/continuing, discounted/undiscounted.\n",
    "- State/action/afterstate values.\n",
    "- Exploration: ε-greedy, optimistic, UCB.\n",
    "- Synchronous/asynchronous.\n",
    "- Real/simulated experience.\n",
    "- Update location/timing/memory.\n",
    "\n",
    "**Intuition**: Vast design space—mix for task. Function approximation (Part II) adds orthogonal scaling dimension.\n",
    "\n",
    "**Part I Takeaway**: Tabular methods build foundations; understand backups, models, integration for intuition.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 8.1 Solution\n",
    "**Q**: Why might multi-step bootstrapping match Dyna?\n",
    "\n",
    "**A**: Multi-step uses real multi-transitions; Dyna simulates many one-steps. If model accurate, similar. But Dyna flexible (arbitrary simulations); multi-step limited by real data—Dyna often better for planning.\n",
    "\n",
    "### Exercise 8.2 Solution\n",
    "**Q**: Why does nonplanning look poor in Fig 8.3?\n",
    "\n",
    "**A**: It's one-step. Multi-step could propagate faster, potentially matching Dyna. But Dyna's simulations allow more flexible/backward focus.\n",
    "\n",
    "### Exercise 8.3 Solution\n",
    "**Q**: Alternate Dyna-Q+: Use $\\sqrt{\\tau}$ in action selection, not updates.\n",
    "\n",
    "**A**: \n",
    "- **Strength**: Direct exploration in real actions.\n",
    "- **Weakness**: Less in planning (may miss long chains). Gridworld: Good for simple changes; worse for deep dependencies.\n",
    "\n",
    "### Exercise 8.4 Solution\n",
    "**Q**: Modify Dyna-Q for stochastic envs; issues in changing envs; fix.\n",
    "\n",
    "**A**: Model averages experiences (e.g., count-based probabilities). Changing envs: Forgets slowly—use recency weighting (e.g., higher α for model updates) or detect changes/reset.\n",
    "\n",
    "### Exercise 8.5 Solution\n",
    "**Q**: Prioritized sweeping on rod maneuvering.\n",
    "\n",
    "**A**: (Descriptive; see text example—efficient for large deterministic tasks by focusing backups.)\n",
    "\n",
    "(Note: Some exercises are thought experiments; solutions are conceptual.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03362b9",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
