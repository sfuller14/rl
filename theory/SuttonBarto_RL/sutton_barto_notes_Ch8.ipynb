{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1841a00e",
   "metadata": {},
   "source": [
    "# Summary of Notation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402c54e",
   "metadata": {},
   "source": [
    "(full notation in Appendix A)\n",
    "\n",
    "> Whenever you see the word \"value\", think \"action's Expected Value of reward\" (AEVOR). This is usually a $P(win)$ or (in the bandit case) probability-weighted-avg $R_t$, given that that action is taken. It can be either a \"True\" value (i.e. the population mean of the action's distribution) or an estimate (i.e. sample statistic) of it.\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| $\\Pr\\{X = x\\}$ | Probability that random variable $X$ takes value $x$ |\n",
    "| $X \\sim p$ | $X$ is drawn from distribution $p$ |\n",
    "| $\\mathbb{E}[X]$ | Expectation of random variable $X$ |\n",
    "| $\\arg\\max_a f(a)$ | Value of $a$ that maximizes $f(a)$ |\n",
    "| $\\alpha$, $\\beta$, $\\epsilon$ | Step-size, decay-rate, and exploration parameters |\n",
    "| $s, s'$ | States |\n",
    "| $a$ | Action |\n",
    "| $r$ | Reward |\n",
    "| $S, A(s), R$ | Set of states, available actions, rewards |\n",
    "| $t, T$ | Discrete time step, final step |\n",
    "| $S_t, A_t, R_t$ | State, action, and reward at time $t$ |\n",
    "| $\\pi$ | Policy (decision-making rule) |\n",
    "| $\\pi(s)$ | action taken in state $s$ under _deterministic_ $\\pi$ |\n",
    "| $\\pi(a \\mid s)$ | Probability of taking action $a$ in state $s$ under _stochastic_ $\\pi$ |\n",
    "| $G_t$ | Return from time $t$ |\n",
    "| $h$ | horizon (the timestep one looks up to in a forward view) |\n",
    "| $v_\\pi(s)$ | Value of state $s$ under policy $\\pi$ |\n",
    "| $q_\\pi(s, a)$ | Value of state-action pair $(s,a)$ under $\\pi$ |\n",
    "| $p(s', r \\mid s, a)$ | Transition dynamics |\n",
    "| $\\hat{v}(s, \\mathbf{w})$ | Approximate value function using weight vector $\\mathbf{w}$ |\n",
    "| $\\delta_t$ | Temporal-difference error at time $t$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402856b7",
   "metadata": {},
   "source": [
    "# Part I: Tabular Solution Methods\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64caf4a",
   "metadata": {},
   "source": [
    "<img src=\"../img/6.png\" alt=\"tabularsolutionmethods\" width=\"40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0242610d",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c5e439",
   "metadata": {},
   "source": [
    "# Chapter 8: Planning and Learning with Tabular Methods\n",
    "\n",
    "**Chapter Overview**: This chapter unifies model-based methods (like dynamic programming, which use planning with an environment model) and model-free methods (like Monte Carlo and TD learning, which rely directly on experience). It introduces integrated architectures like Dyna that combine learning and planning. Key themes include using models to simulate experience, the trade-offs between expected and sample updates, focusing backups efficiently, and decision-time planning techniques like MCTS. As the capstone of Part I (tabular methods), it recaps core dimensions of RL methods and bridges to Part II's focus on scaling via function approximation.\n",
    "\n",
    "**Key Unification Insight**: All RL methods compute value functions via backups (looking ahead to future events). Planning uses simulated experience from a model; learning uses real experience. This allows seamless integration: apply learning algorithms to model-generated data for planning.\n",
    "\n",
    "**Sidenote on Planning vs. Learning**: Planning is \"deliberative\" (model-based foresight), learning is \"reactive\" (experience-based adaptation). But they're not opposites—many methods blend them. Intuitively, planning is like mentally rehearsing scenarios; learning is trial-and-error in the real world.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.1 Models and Planning\n",
    "\n",
    "**Models**: Anything an agent uses to predict environment responses to actions.\n",
    "\n",
    "- **Given**: State $s$, action $a$\n",
    "- **Predicts**: Next state $s'$ and reward $r$\n",
    "\n",
    "**Types of Models**:\n",
    "- **Distribution model**: Gives all possible $s', r$ and their probabilities (e.g., DP's $p(s', r|s, a)$ from Equation 3.2).\n",
    "- **Sample model**: Samples one possible $s', r$ according to probabilities (e.g., blackjack simulation in Chapter 5).\n",
    "\n",
    "**Intuition**: Distribution models are \"stronger\" (can generate samples and probabilities) but harder to obtain. Sample models are easier (e.g., simulate dice rolls) but may introduce sampling variance.\n",
    "\n",
    "**Sidenote**: Models simulate experience. From a state and policy, generate episodes (trajectories). This \"simulated experience\" can train RL methods just like real experience.\n",
    "\n",
    "**Planning**: Computation to improve policy/value function using a model.\n",
    "- **State-space planning**: Focus of RL (vs. plan-space planning like evolutionary methods).\n",
    "- **Core Structure**: Compute values via backups on simulated experience.\n",
    "\n",
    "**Diagram Insight**: All state-space planning fits: (1) Compute values as intermediate step; (2) Use backups on simulated experience.\n",
    "\n",
    "![Common Structure of Planning and Learning](../img/fig8_1.png)\n",
    "\n",
    "**Unification with Learning**: Replace real experience in learning methods (e.g., TD) with model-simulated experience for planning. Both estimate values via backups; difference is experience source.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.2 Dyna: Integrated Planning, Acting, and Learning\n",
    "\n",
    "**Dyna Architecture**: Online agent integrating planning, acting, model-learning, and direct RL (all in parallel).\n",
    "\n",
    "- **Direct RL**: Improve value/policy from real experience (e.g., Q-learning).\n",
    "- **Model-learning**: Update model from real experience.\n",
    "- **Planning**: Indirect RL—improve value/policy from simulated experience.\n",
    "\n",
    "**Diagram**:\n",
    "\n",
    "![Dyna Architecture](../img/fig8_1.png)  <!-- Note: This is Figure 8.1 from the text -->\n",
    "\n",
    "**Relationships** (see diagram on page 163):\n",
    "- Experience → Model (model-learning)\n",
    "- Experience → Value/Policy (direct RL)\n",
    "- Model → Simulated Experience → Value/Policy (planning)\n",
    "\n",
    "**Search Control**: Process selecting starting states/actions for simulated experiences (e.g., random from experienced pairs).\n",
    "\n",
    "**Tabular Dyna-Q Algorithm** (pseudocode on page 164):\n",
    "- Initialize $Q(s,a)$ and Model$(s,a)$\n",
    "- Loop forever:\n",
    "  - (a) $S$ ← current state\n",
    "  - (b) $A$ ← ε-greedy$(S, Q)$\n",
    "  - (c) Execute $A$, observe $R, S'$\n",
    "  - (d) $Q(S,A) \\leftarrow Q(S,A) + \\alpha [R + \\gamma \\max_{a'} Q(S',a') - Q(S,A)]$  (direct RL, one-step tabular Q-learning)\n",
    "  - (e) Model$(S,A) \\leftarrow R, S'$  (assuming deterministic env)\n",
    "  - (f) For $n$ planning steps: Random experienced $S_{rand}, A_{rand}$; Simulate $R, S'$ from Model; Update $Q$ as in (d)\n",
    "\n",
    "**Key Parameters**:\n",
    "- $n$: Planning steps per real step (balances computation).\n",
    "- Assumes deterministic model; queries only experienced pairs.\n",
    "\n",
    "**Example: Dyna Maze** (Figure 8.2):\n",
    "- Task: From S to G quickly.\n",
    "- Dyna-Q with more $n$ learns faster (e.g., $n=50$ builds extensive policy in one episode).\n",
    "\n",
    "![Dyna Maze Learning Curves](../img/fig8_2.png)\n",
    "\n",
    "![Policies During Learning](../img/fig8_3.png)\n",
    "\n",
    "**Intuition**: Planning builds \"mental models\" to accelerate learning. Without planning ($n=0$), policy grows one step per episode; with planning, it expands rapidly via backups.\n",
    "\n",
    "**Sidenote**: Dyna-Q unifies: Same Q-update for real/simulated experience. Agent is always acting, planning, learning—model improves as experience grows.\n",
    "\n",
    "**Exercise Insight**: Multi-step methods (Ch. 7) might match Dyna if model accurate, but Dyna's one-step simplicity + many simulations often wins.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.3 When the Model Is Wrong\n",
    "\n",
    "**Issue**: Models can be inaccurate (initially empty or environment changes).\n",
    "\n",
    "**Example: Blocking Maze** (Figure 8.4):\n",
    "- Path blocks after 1000 steps; new path opens.\n",
    "- Dyna-Q recovers but wanders initially.\n",
    "\n",
    "**Worse Case: Shortcut Maze** (Figure 8.5):\n",
    "- Shortcut opens; Dyna-Q misses it (model says impossible, ε-greedy rarely explores enough).\n",
    "\n",
    "**Solution: Dyna-Q+**:\n",
    "- Track time $τ$ since last real try of $(s,a)$.\n",
    "- Add exploration bonus: Simulated reward $r + κ\\sqrt{τ}$ (encourages testing stale transitions).\n",
    "- Solves shortcut by exploring long-untried actions.\n",
    "\n",
    "![Blocking Maze Performance](../img/fig8_4.png)\n",
    "\n",
    "![Shortcut Maze Performance](../img/fig8_5.png)\n",
    "\n",
    "**Intuition**: Wrong models cause \"model traps.\" Add curiosity (bonus for uncertainty) to detect changes.\n",
    "\n",
    "**Sidenote**: Stochastic envs: Average model over experiences. Changing envs: Weight recent experiences more.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.4 Prioritized Sweeping\n",
    "\n",
    "**Issue**: Uniform random planning (Dyna-Q) wastes updates on low-impact states.\n",
    "\n",
    "**Solution**: Focus backups backward from high-change states (like reverse BFS).\n",
    "\n",
    "**Prioritized Sweeping Algorithm** (for deterministic env, pseudocode on page 170):\n",
    "- Use priority queue: State-action pairs prioritized by update magnitude $P = |R + \\gamma \\max_a Q(S',a) - Q(S,A)|$.\n",
    "- If $P > θ$ (small threshold), insert/update queue.\n",
    "- Update top pair; propagate changes to predecessors.\n",
    "\n",
    "**Intuition**: Chain reactions—big changes in one state affect predecessors. Queue ensures efficient backward focus.\n",
    "\n",
    "**Advantages**:\n",
    "- Faster than uniform (e.g., maze: 3x fewer backups for same performance).\n",
    "- Stochastic extension: Sample successors.\n",
    "\n",
    "**Example: Rod Maneuvering** (Figure 8.6): Solves large (14k states) deterministic task efficiently.\n",
    "\n",
    "![Prioritized Sweeping on Maze](../img/fig8_6.png)  <!-- Assuming this is the rod figure or maze comparison -->\n",
    "\n",
    "**Sidenote**: Like heuristic search but retains values long-term. \"Small backups\" (van Seijen & Sutton, 2013) take this further: Probability-weighted single transitions without sampling.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.5 Expected vs. Sample Updates\n",
    "\n",
    "**Dimensions of Updates**:\n",
    "- State vs. action values.\n",
    "- Optimal vs. arbitrary policy.\n",
    "- Expected (all successors) vs. sample (one successor).\n",
    "\n",
    "**Trade-off**: Expected: Accurate but compute-heavy (branching factor $b$). Sample: Noisy but cheap.\n",
    "\n",
    "**Efficiency Insight** (Figure 8.7): Sample often better for large $b$ (reduces error faster per computation).\n",
    "\n",
    "![Expected vs Sample Efficiency](../img/fig8_7.png)\n",
    "\n",
    "**General Rule**: Sample wins for large $b$, deep backups; expected for small $b$, shallow.\n",
    "\n",
    "**Sidenote**: Planning favors samples (easy from model); real experience favors expected (to reduce variance).\n",
    "\n",
    "---\n",
    "\n",
    "## 8.6 Trajectory Sampling\n",
    "\n",
    "**Update Distribution**: Where to focus backups?\n",
    "- **Uniform**: All states equally (like DP sweeps)—wastes on irrelevant states.\n",
    "- **On-policy**: Sample trajectories following current policy—focuses on likely states.\n",
    "\n",
    "**Advantages of On-Policy** (Figure 8.8): Faster initial learning (relevant states); but uniform may win long-term.\n",
    "\n",
    "![Uniform vs On-Policy](../img/fig8_8.png)\n",
    "\n",
    "**Intuition**: Like importance sampling—prioritize states under current behavior.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.7 Real-time Dynamic Programming (RTDP)\n",
    "\n",
    "**RTDP**: Asynchronous value iteration with on-policy trajectory sampling.\n",
    "- Update only visited states greedily.\n",
    "- Converges to optimal (with exploring starts).\n",
    "\n",
    "**Advantages**: Ignores irrelevant states; policy improves during computation.\n",
    "\n",
    "**Example: Racetrack** (Exercise 5.12): RTDP updates fewer states, finds near-optimal policy faster than full sweeps.\n",
    "\n",
    "**Intuition**: \"Relevant states\" are reachable under optimal policy—RTDP naturally focuses there without visiting all.\n",
    "\n",
    "**Sidenote**: Partial policies (optimal only on relevant states) suffice; RTDP guarantees this without infinite visits everywhere.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.8 Planning at Decision Time\n",
    "\n",
    "**Background Planning**: Improve overall policy/value (e.g., Dyna)—unfocused.\n",
    "- **Decision-time Planning**: From current state, search deeply to select one action (discard backups after).\n",
    "\n",
    "**Intuition**: Focus computation on now; useful when fast responses aren't needed (e.g., games).\n",
    "\n",
    "---\n",
    "\n",
    "## 8.9 Heuristic Search\n",
    "\n",
    "**Classical AI Method**: From current state (root), build search tree; evaluate leaves with heuristic $v$; backup maxes to root; pick best action.\n",
    "\n",
    "**RL Twist**: Improve heuristic $v$ over time via backups—blends with learning.\n",
    "\n",
    "![Heuristic Search as Backups](../img/fig8_9.png)\n",
    "\n",
    "**Intuition**: Deep search = many small backups focused downstream. Better than unfocused for current decision.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.10 Rollout Algorithms\n",
    "\n",
    "**Method**: Monte Carlo control from current state: Simulate trajectories per action using rollout policy $\\pi$; average returns for $q(s,a)$; pick $\\arg\\max_a$.\n",
    "\n",
    "**No Storage**: Discard estimates after action choice.\n",
    "\n",
    "**Improvement**: Policy iteration—rollouts improve over $\\pi$ (if $\\pi$ good, rollouts better).\n",
    "\n",
    "**Intuition**: Like MC but decision-time; balances trials vs. depth (truncate + bootstrap if needed).\n",
    "\n",
    "**Sidenote**: Parallelizable; prune poor actions. Enhances weak $\\pi$ via lookahead.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.11 Monte Carlo Tree Search (MCTS)\n",
    "\n",
    "**Method**: Build asymmetric tree from current state via iterations:\n",
    "1. **Selection**: Tree policy (e.g., UCB) to leaf.\n",
    "2. **Expansion**: Add child(ren).\n",
    "3. **Simulation**: Rollout policy to end; get return.\n",
    "4. **Backup**: Update $Q$ along path (average returns).\n",
    "\n",
    "Pick action with best root $Q$.\n",
    "\n",
    "![MCTS Process](../img/fig8_10.png)\n",
    "\n",
    "**Intuition**: Tree focuses on promising branches; rollouts explore beyond. Balances exploration/exploitation via tree policy.\n",
    "\n",
    "**RL View**: MC control with partial $q$ table (tree); discards between moves.\n",
    "\n",
    "**Sidenote**: AlphaGo (Ch. 16) extends with neural nets. MCTS revolutionized AI games by smart sampling.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.12 Summary of the Chapter\n",
    "\n",
    "**Recap**: Models enable planning (simulated backups). Dyna integrates all; prioritized/trajectory sampling focuses efficiently. Expected/sample, background/decision-time are key axes. Rollout/MCTS: Powerful decision-time methods.\n",
    "\n",
    "**Bridge to Part II**: Part I assumed tabular (exact values per state)—limits to small problems. Part II scales via function approximation: Approximate values over large spaces, enabling real-world RL.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.13 Summary of Part I: Dimensions\n",
    "\n",
    "**Core Dimensions** (Figure 8.11):\n",
    "- **Depth**: Shallow (one-step) to deep (full episodes).\n",
    "- **Width**: Expected (all branches) to sample (one branch).\n",
    "- **On/Off-Policy**: Learn current vs. optimal policy.\n",
    "\n",
    "![Dimensions of RL Methods](../img/fig8_11.png)\n",
    "\n",
    "**Other Dimensions**:\n",
    "- Episodic/continuing, discounted/undiscounted.\n",
    "- State/action/afterstate values.\n",
    "- Exploration: ε-greedy, optimistic, UCB.\n",
    "- Synchronous/asynchronous.\n",
    "- Real/simulated experience.\n",
    "- Update location/timing/memory.\n",
    "\n",
    "**Intuition**: Vast design space—mix for task. Function approximation (Part II) adds orthogonal scaling dimension.\n",
    "\n",
    "**Part I Takeaway**: Tabular methods build foundations; understand backups, models, integration for intuition.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 8.1 Solution\n",
    "**Q**: Why might multi-step bootstrapping match Dyna?\n",
    "\n",
    "**A**: Multi-step uses real multi-transitions; Dyna simulates many one-steps. If model accurate, similar. But Dyna flexible (arbitrary simulations); multi-step limited by real data—Dyna often better for planning.\n",
    "\n",
    "### Exercise 8.2 Solution\n",
    "**Q**: Why does nonplanning look poor in Fig 8.3?\n",
    "\n",
    "**A**: It's one-step. Multi-step could propagate faster, potentially matching Dyna. But Dyna's simulations allow more flexible/backward focus.\n",
    "\n",
    "### Exercise 8.3 Solution\n",
    "**Q**: Alternate Dyna-Q+: Use $\\sqrt{\\tau}$ in action selection, not updates.\n",
    "\n",
    "**A**: \n",
    "- **Strength**: Direct exploration in real actions.\n",
    "- **Weakness**: Less in planning (may miss long chains). Gridworld: Good for simple changes; worse for deep dependencies.\n",
    "\n",
    "### Exercise 8.4 Solution\n",
    "**Q**: Modify Dyna-Q for stochastic envs; issues in changing envs; fix.\n",
    "\n",
    "**A**: Model averages experiences (e.g., count-based probabilities). Changing envs: Forgets slowly—use recency weighting (e.g., higher α for model updates) or detect changes/reset.\n",
    "\n",
    "### Exercise 8.5 Solution\n",
    "**Q**: Prioritized sweeping on rod maneuvering.\n",
    "\n",
    "**A**: (Descriptive; see text example—efficient for large deterministic tasks by focusing backups.)\n",
    "\n",
    "(Note: Some exercises are thought experiments; solutions are conceptual.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03362b9",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cf86eb",
   "metadata": {},
   "source": [
    "# Appendix A: Full Notation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c0755",
   "metadata": {},
   "source": [
    "### General Notation\n",
    "\n",
    "> Capital letters are used for random variables.\n",
    "\n",
    "> Lower case letters are used for the values of random variables and for scalar functions.\n",
    "\n",
    "> Quantities that are required to be real-valued vectors are written in bold and in lower case (even if random variables). \n",
    "\n",
    "> Matrices are bold capitals.\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **Standard Operators** |  |\n",
    "| $\\doteq$ | Equality relationship that is true by definition |\n",
    "| $\\approx$ | Approximately equal |\n",
    "| $\\propto$ | Proportional to |\n",
    "| $\\Pr\\{X = x\\}$ | Probability that random variable $X$ takes value $x$ |\n",
    "| $X \\sim p$ | $X$ drawn from distribution $p$ |\n",
    "| $\\mathbb{E}[X]$ | Expectation of random variable $X$ |\n",
    "| $\\arg\\max_a f(a)$ | Value of $a$ that maximizes $f(a)$ |\n",
    "| $\\ln x$ | Natural logarithm of $x$ |\n",
    "| $e^x$, $\\exp(x)$ | The base of the natural logarithm, $e \\approx 2.71828$, carried to power $x$ |\n",
    "| $\\mathbb{R}$ | Set of real numbers |\n",
    "| $f: \\mathcal{X} \\to \\mathcal{Y}$ | Function from elements of set $\\mathcal{X}$ to elements of set $\\mathcal{Y}$ |\n",
    "| $\\leftarrow$ | Assignment |\n",
    "| $(a, b]$ | Real interval between $a$ and $b$ including $b$ but not $a$ |\n",
    "| **Standard RL Algorithm Parameters** |  |\n",
    "| $\\epsilon$ | Probability of taking a random action in an $\\epsilon$-greedy policy |\n",
    "| $\\alpha, \\beta$ | Step-size parameters |\n",
    "| $\\gamma$ | Discount-rate parameter |\n",
    "| $\\lambda$ | Decay-rate parameter for eligibility traces |\n",
    "| $\\mathbf{1}_\\text{predicate}$ | Indicator function (1 if predicate is true, else 0) |\n",
    "\n",
    "---\n",
    "\n",
    "### Multi-Armed Bandit Notation\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| $k$ | Number of actions (arms) |\n",
    "| $t$ | Discrete time step or play number |\n",
    "| $q_*(a)$ | True value (expected reward) of action $a$ |\n",
    "| $Q_t(a)$ | Estimate at time $t$ of $q_*(a)$ |\n",
    "| $N_t(a)$ | Number of times action $a$ has been selected up to time $t$ |\n",
    "| $H_t(a)$ | Learned preference for selecting action $a$ at time $t$ |\n",
    "| $\\pi_t(a)$ | Probability of selecting action $a$ at time $t$ |\n",
    "| $\\bar{R}_t$ | Estimate at time $t$ of the expected reward given $\\pi_t$ |\n",
    "\n",
    "---\n",
    "\n",
    "### Markov Decision Process Notation\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **State & Action Sets** |  |\n",
    "| $s, s'$ | States |\n",
    "| $a$ | An action |\n",
    "| $r$ | A reward |\n",
    "| $\\mathcal{S}$ | Set of all nonterminal states |\n",
    "| $\\mathcal{S}^+$ | Set of all states, incl. terminal state |\n",
    "| $\\mathcal{A}(s)$ | Set of all actions available in state $s$ |\n",
    "| $\\mathcal{R}$ | Set of all possible rewards, a finite subset of $\\mathbb{R}$ |\n",
    "| $\\mathcal{C}$ | Subset of (e.g., $\\mathcal{R} \\subset \\mathbb{R}$) |\n",
    "| $\\in$ | Is an element of (e.g. $s \\in \\mathcal{S}$, $r \\in \\mathcal{R}$) |\n",
    "| $\\lvert\\mathcal{S}\\rvert$ | Number of elements in set $\\mathcal{S}$ |\n",
    "| **Time & Policy** |  |\n",
    "| $t$ | Discrete time step |\n",
    "| $T, T(t)$ | Final time step of episode, or including $t$ |\n",
    "| $A_t$ | Action at time $t$ |\n",
    "| $S_t$ | State at time $t$ |\n",
    "| $R_t$ | Reward at time $t$ |\n",
    "| $\\pi$ | Policy (decision-making rule) |\n",
    "| $\\pi(s)$ | Action taken in state $s$ under deterministic $\\pi$ |\n",
    "| $\\pi(a \\mid s)$ | Probability of taking action $a$ in state $s$ under stochastic $\\pi$ |\n",
    "| **Returns** |  |\n",
    "| $G_t$ | Return following time $t$ |\n",
    "| $h$ | Horizon (timestep looked up to in forward view) |\n",
    "| $G_{t:t+n}, G_{t:h}$ | $n$-step return from $t+1$ to $t+n$ or $h$ (discounted/corrected) |\n",
    "| $G_{t:h}$ | Flat return (undiscounted/unadjusted) from $t+1$ to $h$ |\n",
    "| $G^\\lambda_t$ | $\\lambda$-return |\n",
    "| $G^{\\wedge}_t$, $G^{\\wedge a}_t$ | Truncated, corrected $\\lambda$-return |\n",
    "| **Transition & Reward Probabilities** |  |\n",
    "| $p(s', r \\mid s, a)$ | Probability of transition to $s'$ with reward $r$ from $s, a$ |\n",
    "| $p(s' \\mid s, a)$ | Probability of transition to $s'$ from $s$ taking $a$ |\n",
    "| $r(s, a)$ | Expected immediate reward from $s$ after $a$ |\n",
    "| $r(s, a, s')$ | Expected reward on transition $s \\to s'$ under $a$ |\n",
    "| **Value Functions** |  |\n",
    "| $v_\\pi(s)$ | Value of $s$ under policy $\\pi$ (expected return) |\n",
    "| $v_*(s)$ | Value of $s$ under optimal policy |\n",
    "| $q_\\pi(s, a)$ | Value of taking $a$ in $s$ under $\\pi$ |\n",
    "| $q_*(s, a)$ | Value of taking $a$ in $s$ under optimal policy |\n",
    "| **Estimators & TD Error** |  |\n",
    "| $V, V_t$ | Array estimates of $v_\\pi$ or $v_*$ |\n",
    "| $Q, Q_t$ | Array estimates of $q_\\pi$ or $q_*$ |\n",
    "| $\\hat{V}(s)$ | Expected approximate action value; e.g. $\\hat{V}_t(s) \\doteq \\sum_a \\pi(a|s) Q_t(s, a)$ |\n",
    "| $U_t$ | Target for estimate at time $t$ |\n",
    "| $\\delta_t$ | Temporal-difference (TD) error at $t$ (a random variable) |\n",
    "| $\\delta_t^s$, $\\delta_t^a$ | State- and action-specific forms of TD error |\n",
    "| $n$ | In $n$-step methods, $n$ is number of steps of bootstrapping |\n",
    "---\n",
    "\n",
    "### Function Approximation, Policy Gradient, Advanced Notation\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **Weight & Parameter Notation** |  |\n",
    "| $d$ | Dimensionality—number of components of $\\mathbf{w}$ |\n",
    "| $d'$ | Alternate dimensionality—number of components of $\\theta$ |\n",
    "| $\\mathbf{w}, \\mathbf{w}_t$ | $d$-vector of weights underlying approximate value function |\n",
    "| $w_i, w_{t,i}$ | $i$-th component of learnable weight vector |\n",
    "| $\\hat{v}(s, \\mathbf{w})$ | Approximate value of $s$ given $\\mathbf{w}$ |\n",
    "| $v_\\mathbf{w}(s)$ | Alternate notation for $\\hat{v}(s, \\mathbf{w})$ |\n",
    "| $\\hat{q}(s, a, \\mathbf{w})$ | Approximate value of $(s, a)$ given $\\mathbf{w}$ |\n",
    "| $\\nabla \\hat{v}(s, \\mathbf{w})$ | Column vector of partial derivatives w.r.t. $\\mathbf{w}$ |\n",
    "| $\\nabla \\hat{q}(s, a, \\mathbf{w})$ | Column vector of partial derivatives w.r.t. $\\mathbf{w}$ |\n",
    "| **Feature Vectors & Eligibility Traces** |  |\n",
    "| $\\mathbf{x}(s)$ | Feature vector visible in $s$ |\n",
    "| $\\mathbf{x}(s, a)$ | Feature vector visible in $s$ taking $a$ |\n",
    "| $x_i(s), x_i(s, a)$ | $i$-th component of $\\mathbf{x}(s)$ or $\\mathbf{x}(s, a)$ |\n",
    "| $\\mathbf{x}_t$ | Shorthand for $\\mathbf{x}(S_t)$ or $\\mathbf{x}(S_t, A_t)$ |\n",
    "| $\\mathbf{w}^\\top \\mathbf{x}$ | Inner product of vectors |\n",
    "| $\\mathbf{v}, \\mathbf{v}_t$ | Secondary $d$-vector of weights, used to learn $\\mathbf{w}$ |\n",
    "| $\\mathbf{z}_t$ | $d$-vector of eligibility traces at $t$ |\n",
    "| **Policy Gradient Notation** |  |\n",
    "| $\\theta, \\theta_t$ | Parameter vector of target policy |\n",
    "| $\\pi(a \\mid s, \\theta)$ | Probability of taking $a$ in $s$ given $\\theta$ |\n",
    "| $\\pi_\\theta$ | Policy corresponding to parameter $\\theta$ |\n",
    "| $\\nabla \\pi(a \\mid s, \\theta)$ | Partial derivatives of $\\pi(a \\mid s, \\theta)$ w.r.t. $\\theta$ |\n",
    "| $J(\\theta)$ | Performance measure for policy $\\pi_\\theta$ |\n",
    "| $\\nabla J(\\theta)$ | Partial derivatives of $J(\\theta)$ w.r.t. $\\theta$ |\n",
    "| $h(s, a, \\theta)$ | Preference for $a$ in $s$ based on $\\theta$ |\n",
    "| **Behavior Policy, Baselines, Importance Sampling** |  |\n",
    "| $b(a \\mid s)$ | Behavior policy used to select actions while learning target $\\pi$ |\n",
    "| $b(s)$ | Baseline function $b: \\mathcal{S} \\to \\mathbb{R}$ for policy-gradient methods |\n",
    "| $b$ | Branching factor for MDP/search tree |\n",
    "| $\\rho_{t:h}$ | Importance sampling ratio for $t$ through $h$ |\n",
    "| $\\rho_t$ | Importance sampling ratio for time $t$ alone, $\\rho_t \\doteq \\rho_{t:t}$ |\n",
    "| $r(\\pi)$ | Average reward (reward rate) for policy $\\pi$ |\n",
    "| $\\bar{R}_t$ | Estimate of $r(\\pi)$ at time $t$ |\n",
    "| **State Distributions & Operators** |  |\n",
    "| $\\mu(s)$ | On-policy distribution over states |\n",
    "| $\\mu$ | $\\lvert\\mathcal{S}\\rvert$-vector of the $\\mu(s)$ for $s \\in \\mathcal{S}$ |\n",
    "| $\\|v\\|^2_\\mu$ | $\\mu$-weighted squared norm of $v$, i.e., $\\|v\\|^2_\\mu \\doteq \\sum_{s \\in \\mathcal{S}} \\mu(s)v(s)^2$ |\n",
    "| $\\eta(s)$ | Expected number of visits to $s$ per episode |\n",
    "| $\\Pi$ | Projection operator for value functions |\n",
    "| $B_\\pi$ | Bellman operator for value functions |\n",
    "\n",
    "---\n",
    "\n",
    "### Matrices, Bellman Error, & Error Metrics\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **Matrix Notation & Linear Algebra** |  |\n",
    "| $\\mathbf{A}$ | $d \\times d$ matrix: $\\mathbf{A} \\doteq \\mathbb{E}\\left[ \\mathbf{x}_t(\\mathbf{x}_t - \\gamma \\mathbf{x}_{t+1})^\\top \\right]$ |\n",
    "| $\\mathbf{b}$ | $d$-dimensional vector: $\\mathbf{b} \\doteq \\mathbb{E}[R_{t+1} \\mathbf{x}_t]$ |\n",
    "| $\\mathbf{w}_{TD}$ | TD fixed point: $\\mathbf{w}_{TD} \\doteq \\mathbf{A}^{-1}\\mathbf{b}$ |\n",
    "| $\\mathbf{I}$ | Identity matrix |\n",
    "| $\\mathbf{P}$ | $\\lvert\\mathcal{S}\\rvert \\times \\lvert\\mathcal{S}\\rvert$ matrix of state-transition probabilities under $\\pi$ |\n",
    "| $\\mathbf{D}$ | $\\lvert\\mathcal{S}\\rvert \\times \\lvert\\mathcal{S}\\rvert$ diagonal matrix with $\\mu$ on its diagonal |\n",
    "| $\\mathbf{X}$ | $\\lvert\\mathcal{S}\\rvert \\times d$ matrix with the $\\mathbf{x}(s)$ as its rows |\n",
    "| **Bellman Error & Value Error Metrics** |  |\n",
    "| $\\bar{\\delta}_\\mathbf{w}(s)$ | Bellman error (expected TD error) for $v_\\mathbf{w}$ at $s$ |\n",
    "| $\\bar{\\delta}_\\mathbf{w}$, BE | Bellman error vector (with components $\\bar{\\delta}_\\mathbf{w}(s)$) |\n",
    "| $\\text{VE}(\\mathbf{w})$ | Mean square value error: $\\text{VE}(\\mathbf{w}) \\doteq \\|v_\\mathbf{w} - v_\\pi\\|^2_\\mu$ |\n",
    "| $\\text{BE}(\\mathbf{w})$ | Mean square Bellman error: $\\text{BE}(\\mathbf{w}) \\doteq \\|\\bar{\\delta}_\\mathbf{w}\\|^2_\\mu$ |\n",
    "| $\\text{PBE}(\\mathbf{w})$ | Mean square projected Bellman error: $\\text{PBE}(\\mathbf{w}) \\doteq \\|\\Pi \\bar{\\delta}_\\mathbf{w}\\|^2_\\mu$ |\n",
    "| $\\text{TDE}(\\mathbf{w})$ | Mean square temporal-difference error: $\\text{TDE}(\\mathbf{w}) \\doteq \\mathbb{E}_b[\\rho_t \\delta_t^2]$ |\n",
    "| $\\text{RE}(\\mathbf{w})$ | Mean square return error |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
