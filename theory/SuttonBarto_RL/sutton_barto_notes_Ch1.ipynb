{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69aa605b",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63844a28",
   "metadata": {},
   "source": [
    "## Key Realization:\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74146c31",
   "metadata": {},
   "source": [
    "> The point in RL is generally to find an optimal policy $\\pi^*$ that maximizes total rewards across timesteps (a.k.a. \"**Return**\").  \n",
    "> __**BUT**__  \n",
    "> **KEY REALIZATION**: ðŸ†In Part I, you're not trying to directly estimate the policy. You're trying to adjust the value function (your estimate of $\\mathbb{E}[PV(r^{\\pi}_{t+})]$ for all states). ðŸ†   \n",
    "> (i.e. Part I is \"Value-based methods\". Part II is \"Policy-based methods\".)  \n",
    "> In TD cases like tic-tac-toe, the next state's Value estimate is used as a proxy for the true Return.  \n",
    ">\n",
    "> ---\n",
    ">\n",
    "> \"Return\" ($G_t \\approx PV(r_{t+})$) is the cumulative reward from time $t$ onward (often discounted):\n",
    "> $$G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\dots$$\n",
    "> \"Value\" ($v_\\pi(s)$) is the _expected_ $G_t$ (i.e., expected cumulative future reward) starting from $s$ following policy $\\pi$.  \n",
    "> A \"Value Function\" ($v_\\pi$) is the \"function\" (or table) containing \"Values\" at each state.\n",
    ">\n",
    "> ---\n",
    ">\n",
    "> The point in RL is generally to find an optimal policy $\\pi^*$ that maximizes total rewards across timesteps (a.k.a. \"**Return**\").    \n",
    "> So:  \n",
    "> The point in RL is generally to find an optimal policy $\\pi^*$ that maximizes **Return** $G_t$.  \n",
    "> Return can be thought of as $PV(r_{t+})$.\n",
    "> \n",
    "> More precisely,  \n",
    "> \n",
    "> The point in RL training is to make policy $\\pi$ approximate optimal policy $\\pi^*$ by maximizing **_Expected_ Return** $\\mathbb{E}[G_t]$ (a.k.a. \"**Value**\").  \n",
    "> So:  \n",
    "> The point in RL training is to make policy $\\pi$ approximate optimal policy $\\pi^*$ by maximizing **Value** $v_{\\pi^*}(s)$ **at each state $s$**.  \n",
    "> Value can be thought of as $\\mathbb{E}[PV(r^{\\pi}_{t+})]$.  \n",
    "> So:  \n",
    "> The point in RL training is to make policy $\\pi$ approximate optimal policy $\\pi^*$ by maximizing  **Value** $\\mathbb{E}[PV(r^{\\pi}_{t+})]$ **at each state $s$**.  \n",
    ">\n",
    "> ---\n",
    ">\n",
    "> Part I focuses on methods for estimating \"**value functions**\" ($v_\\pi$), which output \"values\" ($v_{\\pi^*}(s)$) for all states, to which policies can be applied.  \n",
    "> So:  \n",
    "> ***ðŸ†RL training in Part I is estimating \"Value Functions\" that, at each state, output \"Values\" that accurately reflect $\\mathbb{E}[PV(\\text{future rewards yielded by following }\\pi)]$ðŸ†.  \n",
    "> $\\pi$ is taken as a given and the policy's Value Function (i.e. mapping of states to values) is what is actually learned.  \n",
    "> For example, the policy can simply be pre-specified as \"pick next state as the one with the highest available Value\". So the choice of a policy (like $\\epsilon$-greedy) can result in a better Value Function.  \n",
    "> But \"inference\" is just an algorithm that references the learned Value Table.***  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cf4162",
   "metadata": {},
   "source": [
    "## 1.1 Reinforcement Learning\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19948b10",
   "metadata": {},
   "source": [
    "- **Definition**: Learning how to map situations to actions to maximize cumulative reward.\n",
    "- **Core elements**:\n",
    "  - **Trial-and-error search**: The agent explores to learn.\n",
    "  - **Delayed reward**: Actions impact long-term outcomes, not just immediate feedback.\n",
    "- **Distinguishing features**:\n",
    "  - Not told *what* actions to take, only gets feedback via rewards.\n",
    "  - Must balance **exploration vs. exploitation**.\n",
    "- **Compared to other paradigms**:\n",
    "  - **Supervised learning**: Learns from labeled examples.\n",
    "  - **Unsupervised learning**: Learns hidden structure from unlabeled data.\n",
    "  - **RL**: Maximizes rewards via interaction; often needs to explore unknown state spaces.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee3ffe5",
   "metadata": {},
   "source": [
    "## 1.2 Examples\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd328bee",
   "metadata": {},
   "source": [
    "\n",
    "Real-world analogies include:\n",
    "- **Chess**: Intuition + planning, learning from position values.\n",
    "- **Adaptive control**: Tuning refinery settings in real time.\n",
    "- **Animals**: Gazelle calf learning to run soon after birth.\n",
    "- **Robots**: Battery-aware navigation decisions.\n",
    "- **Daily tasks**: Preparing breakfast involves conditional behavior, goals, and sensory feedback.\n",
    "\n",
    "**Key Takeaway**: RL applies broadly wherever an agent interacts with an environment, learns over time, and must adapt to uncertainty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf1ab1",
   "metadata": {},
   "source": [
    "## 1.3 The 3 Elements of Reinforcement Learning (policy, value, and model)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1f6d65",
   "metadata": {},
   "source": [
    "\n",
    "Four main components:\n",
    "\n",
    "1. **Policy ($\\pi$)**: The agent's way of behaving at a given time by mapping from perceived environment states to actions to be taken (or action probabilities) when in those states. \n",
    "   - $\\pi$ is the agent's decision-making algorithm (akin to \"stimulus-response rules\")\n",
    "   - $\\pi(s)$ is the action that policy $\\pi$ outputs when given state $s$ as input\n",
    "   - So $\\pi$ is the agent's \"mapping\" from environment states to actions $\\pi(s)$ (deterministic) (or $\\pi(a|s)$ (stochastic))\n",
    "      - In the stochastic case ($\\pi(a|s)$), a policy specifies probabilities for each action\n",
    "   - Can be a lookup table or simple function or an extensive computation (like search)\n",
    "   - Policy alone is is sufficient to determine agent behavior\n",
    "   - Notation:\n",
    "      - $\\pi(s)$: Action taken in state $s$ under deterministic policy $\\pi$\n",
    "         - Example:\n",
    "            - If $\\pi(s_1) = a_3$, this means \"when in state $s_1$, the policy chooses action $a_3$\"\n",
    "               - The policy $\\pi$ is the overall mapping/function, but $\\pi(s)$ is the concrete action value it returns\n",
    "         - This is analogous to how $f(x)$ represents the output value of function $f$ when given input $x$, not the function itself.\n",
    "      - $\\pi(a \\mid s)$: Probability of taking action $a$ in state $s$ under stochastic policy $\\pi$\n",
    "         - Example:\n",
    "            - If $\\pi(left \\mid s_1) = 0.7$ and $\\pi(right \\mid s_1) = 0.3$, this means \"when in state $s_1$, choose left with 70% probability and right with 30% probability\"\n",
    "            - The agent samples from this probability distribution to decide which action to take\n",
    "\n",
    "2. **Reward Signal ($r$)**: The immediate environmental feedback signal received by the agent at each time step. It is a single number. The agent's sole objective is to maximize the _total_ reward it receives over the long term.\n",
    "   - $r_t$ is the reward received at time step $t$ (akin to pain/pleasure from taking an action).\n",
    "   - Reward $r$ is a key variable in defining **Return ($G_t$)**, which is the **cumulative future reward, often discounted**: \n",
    "      $$G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\dots$$\n",
    "      - To compare to financial theory, $r_t$ is like a future cashflow and $G_t$ is like the present value of future cash flows.\n",
    "      - Note that Return $G_t$ is a random variable (since future rewards are uncertain) whose true expected value we want to estimate.\n",
    "         - **In RL, the point is generally to find an optimal policy $\\pi^*$ that maximizes the _EXPECTED_ Return from each state.**\n",
    "         - The _EXPECTED_ Return is what the value functions estimate. Your estimate of expected long-term reward is called the \"value function\".\n",
    "            - So your overall goal is to find the optimal policy $\\pi^*$. You do this by estimating value functions (expected cumulative reward) for your current policy $\\pi$, then using those estimates to improve $\\pi$ toward $\\pi^*$.\n",
    "   - See the note below on learnable parameters\n",
    "\n",
    "3. **Value Function ($v(s)$, $q(s,a)$)**:\n",
    "   - Estimates long-term reward (expected return) from states or state-action pairs.\n",
    "   - The value function is the agent's \"learned intuition\" about how good different situations are\n",
    "   - **$v_\\pi(s)$**: Expected return from state $s$ under policy $\\pi$ - \"How good is this state if I follow my current policy?\"\n",
    "      - V-value: $V(s)$ = expected return from state $s$ following the policy\n",
    "      - \"How good is this state?\"\n",
    "   - **$q_\\pi(s, a)$**: Expected return from taking action $a$ in state $s$, then following policy $\\pi$ - \"How good is this specific action in this state?\"\n",
    "      - Q-value: $Q(s,a)$ = expected return from taking action $a$ in state $s$, then following the policy\n",
    "      - \"How good is this specific action in this state?\"\n",
    "   - Value functions guide decision-making more than immediate rewards because they capture long-term consequences\n",
    "   - Unlike rewards (which are given by the environment), value functions are learned estimates that improve over time\n",
    "   - Think of value functions as the agent's \"experience-based predictions\" about future success\n",
    "   - In practice, value functions help answer: \"Should I be optimistic or pessimistic about my current situation?\"\n",
    "   - Notation:\n",
    "      - $v_\\pi(s)$: State value function - expected cumulative reward starting from state $s$\n",
    "      - $q_\\pi(s, a)$: Action-value function (Q-function) - expected cumulative reward for taking action $a$ in state $s$\n",
    "      - Both represent the same underlying concept (expected return) but from different starting points\n",
    "\n",
    "4. **Model (optional)**:\n",
    "   - Simulates environment behavior: $p(s', r \\mid s, a)$ - \"If I take action $a$ in state $s$, what state will I end up in and what reward will I get?\"\n",
    "   - The model is the agent's internal representation of how the world works\n",
    "   - Enables **planning** (model-based) vs. **direct interaction** (model-free)\n",
    "   - **Model-based RL**: Agent learns a model of the environment, then uses it to plan optimal actions (like playing out scenarios in your head)\n",
    "   - **Model-free RL**: Agent learns directly from experience without explicitly modeling the environment (like learning to ride a bike through practice)\n",
    "   - Models can be:\n",
    "      - **Perfect**: Complete knowledge of environment dynamics (rare in practice)\n",
    "      - **Learned**: Estimated from experience (common in model-based RL)\n",
    "      - **Absent**: No model used (model-free methods like Q-learning)\n",
    "   - Trade-off: Models enable faster learning through planning, but require additional computational resources and can be inaccurate\n",
    "   - Example: A chess program might model \"if I move my queen here, my opponent will likely respond with these\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cda6283",
   "metadata": {},
   "source": [
    "## An Aside: What are the \"weights\" (learnable parameters) in RL -- the Policy or the Value Function?\n",
    "---\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97688412",
   "metadata": {},
   "source": [
    "> #### Value-Based vs. Policy-Based Methods\n",
    "> \n",
    "> **Value-based methods**: Learn a value function (estimates of expected returns) and derive the policy from those values (e.g., act greedily with respect to learned values).\n",
    "> - The value function contains the learnable parameters\n",
    "> - Policy is implicit - derived from value estimates\n",
    "> - Examples: \n",
    ">   - Choose action with highest Q-value: $\\pi(s) = \\arg\\max_a Q(s,a)$\n",
    ">   - Îµ-greedy based on V(s): estimate Q-values from V(s) using one-step lookahead, then act Îµ-greedily\n",
    "> \n",
    "> **Policy-based methods**: Directly learn a parameterized policy without necessarily learning explicit value functions.\n",
    "> - The policy contains the learnable parameters  \n",
    "> - Value functions may be used as auxiliary tools but aren't the primary learned component\n",
    "> - Examples: Neural network that directly outputs action probabilities\n",
    "> \n",
    "> **Actor-Critic methods**: Hybrid approach that learns both value functions (critic) and policy (actor) with separate parameters.\n",
    "> \n",
    "> #### Classification of Sutton & Barto Methods\n",
    ">\n",
    "> **<u>NOTE: Part I methods are all value-based because they focus on learning value tables, with policies derived from those values.</u>**\n",
    "> \n",
    "> **Value-Based Methods (Part I - Tabular):**\n",
    "> - Multi-armed Bandits: Learn action-value estimates $Q_t(a)$\n",
    "> - Dynamic Programming: Learn $v_\\pi(s)$ or $q_\\pi(s,a)$ tables\n",
    "> - Monte Carlo Methods: Learn value tables $V(s)$ or $Q(s,a)$\n",
    "> - Temporal Difference Methods: Learn $V(s)$ or $Q(s,a)$ tables\n",
    "> - Hybrid Methods (TD(Î»)): Learn value tables with eligibility traces\n",
    "> \n",
    "> **Value-Based Methods (Part II - Function Approximation):**\n",
    "> - Deep Q-Networks (DQN)\n",
    "> - Value function approximation with neural networks\n",
    "> \n",
    "> **Policy-Based Methods (Part II):**\n",
    "> - REINFORCE (policy gradient)\n",
    "> - Actor-only policy gradient methods\n",
    "> \n",
    "> **Actor-Critic Methods (Part II):**\n",
    "> - Actor-Critic with function approximation\n",
    "> - Advanced methods like A3C, PPO, SAC (though these are beyond the book's scope)\n",
    "\n",
    "---\n",
    "\n",
    "#### Tic-Tac-Toe Example from Chapter 1\n",
    "In Sutton & Barto's tic-tac-toe example:\n",
    "\n",
    "* State ($s$): A board configuration (arrangement of X's and O's)\n",
    "* Action ($a$): A move (placing X in an available square)\n",
    "* Policy ($\\pi$): Rule for selecting moves, mostly greedy (choose move leading to highest-value board) with occasional exploration\n",
    "* Reward ($r_t$): +1 for win, 0 for draw, -1 for loss (received only at game end)\n",
    "* Return ($G_t$): Same as final reward since it's an episodic task with rewards only at termination\n",
    "   * See **KEY** note below\n",
    "* Value function ($V(s)$): Estimated probability of winning from each board state\n",
    "   * See **KEY** note below\n",
    "* Model: Not explicitly used (model-free approach)\n",
    "\n",
    "**Learnable parameters**: The value table $V(s)$ containing win probability estimates for each possible board configuration. These values are updated using the temporal difference rule: $$V(S_t) \\leftarrow V(S_t) + \\alpha [V(S_{t+1}) - V(S_t)]$$\n",
    "\n",
    "The value function serves the same role as model weights in supervised learning - it's what gets updated during training to improve performance.\n",
    "\n",
    "> <u>**KEY**</u>\n",
    "> \n",
    "> The Return ($G_t$) IS meaningful, but what gets \"backpropagated\" are value estimate updates, not the return itself. Here's the distinction:\n",
    "> * Return ($G_t$): The actual cumulative reward from time $t$ onward. In tic-tac-toe:\n",
    ">     * From any non-terminal state: $G_t$ = final reward (+1, 0, or -1)\n",
    ">     * So yes, $G_t$ equals the final reward since there are no intermediate rewards\n",
    "> * What gets updated: The value estimates $V(S_t)$ using the TD rule: $$V(S_t) \\leftarrow V(S_t) + \\alpha [V(S_{t+1}) - V(S_t)]$$\n",
    "> \n",
    "> Key insight: The TD update uses the next state's value estimate $V(S_{t+1})$ as a proxy for the true return $G_t$. This is called \"bootstrappingâ€ \" - using one estimate to update another estimate. \n",
    ">\n",
    "> So the algorithm is trying to make $V(S_t)$ approximate $\\mathbb{E}[G_t]$ (the expected return), but it updates using $V(S_{t+1})$ rather than waiting for the actual return.\n",
    ">\n",
    "> ---\n",
    ">\n",
    "> Aside: The tic-tac-toe example is 1-step bootstrappingâ€¡ (using $V(S_{t+1})$) within the general framework of bootstrappingâ€  (using estimates rather than true returns).\n",
    ">  \n",
    "> â€  \"bootstrapping\" here is used in the general statistical/RL step (sometimes seen in finance, specifically in MC methods):\n",
    ">     - Definition: Using estimates to update other estimates, rather than waiting for true/final values.\n",
    ">\n",
    "> â€¡ \"bootstrapping\" here is used as per \"n-step bootstrapping\":\n",
    ">     - Definition: How many steps ahead you look before using an estimate as your target.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de1ecb3",
   "metadata": {},
   "source": [
    "## 1.4 Limitations and Scope\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef046c26",
   "metadata": {},
   "source": [
    "- Assumes a **state signal** is given (e.g., as preprocessed input).\n",
    "- Reinforcement learning does not inherently solve **state representation**.\n",
    "- Evolutionary methods can solve RL problems without value functions, but often less efficient.\n",
    "- Focus of this book: **model-free and model-based RL** with value estimation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436f4aa9",
   "metadata": {},
   "source": [
    "## 1.5 An Extended Example: Tic-Tac-Toe\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8282106f",
   "metadata": {},
   "source": [
    "Goal: Learn to beat an imperfect opponent without knowing its strategy a priori.\n",
    "\n",
    "> About the best one can do on this problem is first to learn a model of the opponentâ€™s behavior, up to some level of confidence, and then apply dynamic programming to compute an optimal solution given the approximate opponent model. In the end, this is not that different from some of the reinforcement learning methods we examine later in this book.\n",
    "\n",
    "KEY: In this example, \"value\" is the estimated probability of winning in a given board state. The \"value function\" is the learned $P(win)$ estimates for ALL board states.\n",
    "\n",
    "### **Approach _using a value function_**:\n",
    "- Initialize a \"value table\" $V(s)$ that maps board states to win probabilities:\n",
    "   - Each \"cell\" in the value table corresponds to a possible board state $s$. \n",
    "   - Each cell's value is the latest estimate of the probability of winning from that state (that state's _value_ $V(s_i)$).\n",
    "   - The whole table is the learned _value function_.\n",
    "   - Initialize the table with:\n",
    "      - 1s for all states that have 3 Xs in a row.\n",
    "      - 0s for all states that have 3 Os in a row.\n",
    "      - 0.5 for all other states.\n",
    "- Play a bunch of games against the opponent and update the values.\n",
    "- Use mostly greedy moves (choose highest available $V(s)$), but occasionally explore (choose non-highest $V(s)$).\n",
    "- Once the game is finished, **after greedy moves only**, update the value of the previous state ($V(S_t)$) to be closer to the current state ($V(S_{t+1})$):\n",
    "  $$\n",
    "  V(S_t) \\leftarrow V(S_t) + \\alpha \\left[V(S_{t+1}) - V(S_t)\\right]\n",
    "  $$\n",
    "   - where $\\alpha$ is a step-size parameter (small positive fraction).\n",
    "   - **START AT THE TERMINAL STATE AND MOVE BACKWARDS, BACKPROPAGATING THE UPDATES TO $S_0$ FOR GREEDY MOVES ONLY**.\n",
    "   - Since value updates occur **only after greedy moves**, exploration is not used for learning.\n",
    "- This is an example of a temporal-difference update rule.\n",
    "\n",
    "<img src=\"../img/3.png\" alt=\"tictactoe-backprop\" width=\"50%\"/>\n",
    "\n",
    "#### **Note on mechanics**:\n",
    "- It can be assumed that Sutton & Barto's tic-tac-toe example is \"episodic TD(0)\", where the update rule is applied after the game concludes.\n",
    "   - This means that the post-game update applied to the penultimate move causes the update rule to backprop over multiple timesteps (in reverse order...\"one-step bootstrapping\"), sweeping from $S_{T-1}$ back to $S_0$ (but not applied for any exploration moves made along the way).\n",
    "      - \"One-step bootstrapping\": Each update only uses the immediate next state's value\n",
    "\n",
    "#### **Why Exploration Helps Even Without Direct Updates**:\n",
    "\n",
    "Exploration moves don't get updated directly, but they indirectly improve learning by leading the agent into new board states. When the agent eventually makes greedy moves from these newly discovered states, those greedy moves do get updated based on the outcomes. This allows exploration to:\n",
    "- **Discover better strategies**: Find high-value states that pure greedy play would never encounter\n",
    "- **Correct overconfident estimates**: Reveal that seemingly good states actually lead to losses\n",
    "\n",
    "So exploration expands the \"training data\" that the value function learns from, even though only greedy moves directly update the table.\n",
    "\n",
    "### **Takeaways**:\n",
    "- RL learns from interaction without a model of the opponent.\n",
    "- It backs up values from future to past states.\n",
    "- Unlike evolutionary methods, RL learns online, and credit assignment is finer-grained.\n",
    "\n",
    "### **Model-Free Nature**:\n",
    "- Doesnâ€™t require modeling opponent or future states.\n",
    "- Works via **temporal-difference (TD) learning**.\n",
    "- Can generalize using function approximators (e.g., Tesauroâ€™s neural network for Backgammon).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596e06b2",
   "metadata": {},
   "source": [
    "## 1.6 Summary\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27ecabc",
   "metadata": {},
   "source": [
    "- RL is a third paradigm alongside supervised and unsupervised learning.\n",
    "- Uses **Markov Decision Processes (MDPs)** to formalize interaction.\n",
    "- Central tools:\n",
    "  - **Value functions**\n",
    "  - **Trial-and-error updates**\n",
    "- RL = learning by *interacting*, *adapting*, and *optimizing long-term outcomes*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f42f99f",
   "metadata": {},
   "source": [
    "## 1.7 Early History of Reinforcement Learning\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4fbab6",
   "metadata": {},
   "source": [
    "**Two foundational threads**:\n",
    "1. **Trial-and-error learning**:\n",
    "   - Roots in psychology: Thorndikeâ€™s *Law of Effect*, Skinnerâ€™s reinforcement, Pavlovâ€™s conditioning.\n",
    "   - Turing (1948) described pleasure/pain driven learning machines.\n",
    "   - Shannonâ€™s *Theseus*, Michieâ€™s *MENACE*, Samuelâ€™s *checkers program* were early digital examples.\n",
    "\n",
    "2. **Optimal control and dynamic programming**:\n",
    "   - Bellmanâ€™s **DP** and **Bellman equations** in 1950s.\n",
    "   - MDPs (Howard, 1960) formalized stochastic control problems.\n",
    "   - Combined with function approximation: \"neurodynamic programming\".\n",
    "\n",
    "**Modern RL** = Integration of:\n",
    "- Dynamic programming theory (optimal control)\n",
    "- Psychological theories of learning\n",
    "- Computational models like TD-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41167df8",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2b8262",
   "metadata": {},
   "source": [
    "\n",
    "## Chapter 1 Exercises\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c741e5",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../img/4.png\" alt=\"4\" width=\"50%\"/>\n",
    "<br>\n",
    "<img src=\"../img/5.png\" alt=\"5\" width=\"50%\"/>\n",
    "\n",
    "### Answers & Intuition\n",
    "\n",
    "---\n",
    "\n",
    "#### **Exercise 1.1: Self-Play**\n",
    "**Q:** What happens if the RL algorithm plays against itself, both sides learning? Would it learn a different policy for selecting moves?\n",
    "\n",
    "**A:**  \n",
    "If both sides learn via self-play, the algorithm can converge to an optimal (or near-optimal) policy for both players, potentially leading to a policy that is robust to strong opponents, not just random ones. The learned policy may differ from what is learned against a random opponent, as the distribution of experiences will be more challenging and realistic.\n",
    "\n",
    "**Intuition:**  \n",
    "Self-play is foundational in RL for learning robust strategies, as seen in AlphaGo/AlphaZero. This question builds intuition for learning from interaction with non-stationary or improving opponents.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Exercise 1.2: Symmetries**\n",
    "**Q:** How might we amend the learning process to exploit board symmetries? How would this help? Should we still do this if the opponent does not exploit symmetries? Do symmetric positions necessarily have the same value?\n",
    "\n",
    "**A:**  \n",
    "We can exploit symmetries by treating symmetric board positions as equivalent, updating their values together, which reduces the state space and improves data efficiency. This accelerates learning because knowledge from one symmetric state transfers to others.  \n",
    "If the opponent does *not* exploit symmetries, we may still benefit, but symmetric positions could end up having different practical values due to asymmetric play. Symmetric positions *should* have the same value only if the opponent's policy is itself symmetric.\n",
    "\n",
    "**Intuition:**  \n",
    "This introduces the idea of state abstraction and function approximationâ€”core to generalization and sample efficiency in RL.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Exercise 1.3: Greedy Play**\n",
    "**Q:** What happens if the player always acts greedily (never explores)? Might it learn to play better or worse than a non-greedy player? What problems can occur?\n",
    "\n",
    "**A:**  \n",
    "A purely greedy player risks converging to suboptimal policies due to lack of exploration (\"getting stuck\" in local optima), and may never discover better moves. In practice, this can lead to worse performance compared to a player who explores and updates values more broadly. Problems include incomplete learning and overfitting to initial experiences.\n",
    "\n",
    "**Intuition:**  \n",
    "This exercise motivates the importance of exploration in RL (vs. exploitation), foundational for concepts like $\\epsilon$-greedy, softmax, and exploration-exploitation tradeoff.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Exercise 1.4: Learning from Exploration**\n",
    "**Q:** What if learning updates occur after all moves (including exploratory)? What are the two sets of probabilities? Which method is better for learning/winning?\n",
    "\n",
    "**A:**  \n",
    "- **If we update after all moves (including exploratory):** The value estimates reflect the actual probabilities with which moves are taken, blending greedy and exploratory choices.\n",
    "- **If we update only after greedy moves:** The values reflect the policy if only the best-known moves are always chosen.\n",
    "- Continuing to make exploratory moves but learning only from greedy moves focuses learning on the policy we intend to follow for winning, which usually results in better performance than learning from all moves (which blends exploration into the policy).\n",
    "\n",
    "**Intuition:**  \n",
    "This foreshadows \"on-policy\" vs. \"off-policy\" learning distinctions and highlights how the target of updates matters in RL algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Exercise 1.5: Other Improvements**\n",
    "**Q:** What other ways can we improve the RL player? Any better way to solve the tic-tac-toe problem?\n",
    "\n",
    "**A:**  \n",
    "Possible improvements:\n",
    "- Use deeper lookahead (planning) or tree search.\n",
    "- Incorporate opponent modeling.\n",
    "- Use value function approximation (e.g., neural nets).\n",
    "- Exploit more domain knowledge (e.g., forced moves).\n",
    "- Enhance exploration strategy (e.g., UCB, optimism).\n",
    "For tic-tac-toe, full minimax search can solve the game exactly, but the RL approach generalizes to much harder problems.\n",
    "\n",
    "**Intuition:**  \n",
    "This encourages thinking about RL's flexibility and limitations, foreshadowing the use of model-based planning, opponent modeling, and advanced function approximation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03362b9",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
