{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1841a00e",
   "metadata": {},
   "source": [
    "# Summary of Notation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402c54e",
   "metadata": {},
   "source": [
    "(full notation in Appendix A)\n",
    "\n",
    "> Whenever you see the word \"value\", think \"action's Expected Value of reward\" (AEVOR). This is usually a $P(win)$ or (in the bandit case) probability-weighted-avg $R_t$, given that that action is taken. It can be either a \"True\" value (i.e. the population mean of the action's distribution) or an estimate (i.e. sample statistic) of it.\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| $\\Pr\\{X = x\\}$ | Probability that random variable $X$ takes value $x$ |\n",
    "| $X \\sim p$ | $X$ is drawn from distribution $p$ |\n",
    "| $\\mathbb{E}[X]$ | Expectation of random variable $X$ |\n",
    "| $\\arg\\max_a f(a)$ | Value of $a$ that maximizes $f(a)$ |\n",
    "| $\\alpha$, $\\beta$, $\\epsilon$ | Step-size, decay-rate, and exploration parameters |\n",
    "| $s, s'$ | States |\n",
    "| $a$ | Action |\n",
    "| $r$ | Reward |\n",
    "| $S, A(s), R$ | Set of states, available actions, rewards |\n",
    "| $t, T$ | Discrete time step, final step |\n",
    "| $S_t, A_t, R_t$ | State, action, and reward at time $t$ |\n",
    "| $\\pi$ | Policy (decision-making rule) |\n",
    "| $\\pi(s)$ | action taken in state $s$ under _deterministic_ $\\pi$ |\n",
    "| $\\pi(a \\mid s)$ | Probability of taking action $a$ in state $s$ under _stochastic_ $\\pi$ |\n",
    "| $G_t$ | Return from time $t$ |\n",
    "| $h$ | horizon (the timestep one looks up to in a forward view) |\n",
    "| $v_\\pi(s)$ | Value of state $s$ under policy $\\pi$ |\n",
    "| $q_\\pi(s, a)$ | Value of state-action pair $(s,a)$ under $\\pi$ |\n",
    "| $p(s', r \\mid s, a)$ | Transition dynamics |\n",
    "| $\\hat{v}(s, \\mathbf{w})$ | Approximate value function using weight vector $\\mathbf{w}$ |\n",
    "| $\\delta_t$ | Temporal-difference error at time $t$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa605b",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cf4162",
   "metadata": {},
   "source": [
    "## 1.1 Reinforcement Learning\n",
    "\n",
    "- **Definition**: Learning how to map situations to actions to maximize cumulative reward.\n",
    "- **Core elements**:\n",
    "  - **Trial-and-error search**: The agent explores to learn.\n",
    "  - **Delayed reward**: Actions impact long-term outcomes, not just immediate feedback.\n",
    "- **Distinguishing features**:\n",
    "  - Not told *what* actions to take, only gets feedback via rewards.\n",
    "  - Must balance **exploration vs. exploitation**.\n",
    "- **Compared to other paradigms**:\n",
    "  - **Supervised learning**: Learns from labeled examples.\n",
    "  - **Unsupervised learning**: Learns hidden structure from unlabeled data.\n",
    "  - **RL**: Maximizes rewards via interaction; often needs to explore unknown state spaces.\n",
    "\n",
    "---\n",
    "## 1.2 Examples\n",
    "\n",
    "Real-world analogies include:\n",
    "- **Chess**: Intuition + planning, learning from position values.\n",
    "- **Adaptive control**: Tuning refinery settings in real time.\n",
    "- **Animals**: Gazelle calf learning to run soon after birth.\n",
    "- **Robots**: Battery-aware navigation decisions.\n",
    "- **Daily tasks**: Preparing breakfast involves conditional behavior, goals, and sensory feedback.\n",
    "\n",
    "**Key Takeaway**: RL applies broadly wherever an agent interacts with an environment, learns over time, and must adapt to uncertainty.\n",
    "\n",
    "---\n",
    "## 1.3 Elements of Reinforcement Learning\n",
    "\n",
    "Four main components:\n",
    "\n",
    "1. **Policy ($\\pi$)**:\n",
    "   - Mapping from states to actions: $\\pi(s)$ or $\\pi(a|s)$.\n",
    "   - Can be deterministic or stochastic.\n",
    "\n",
    "2. **Reward Signal ($r$)**:\n",
    "   - Defines the goal.\n",
    "   - Immediate signal from the environment, tells what’s good or bad.\n",
    "\n",
    "3. **Value Function ($v(s)$, $q(s,a)$)**:\n",
    "   - Estimates long-term reward.\n",
    "   - **$v_\\pi(s)$**: Expected return from state $s$ under policy $\\pi$.\n",
    "   - Guides decision-making more than immediate rewards.\n",
    "\n",
    "4. **Model (optional)**:\n",
    "   - Simulates environment behavior: $p(s', r \\mid s, a)$.\n",
    "   - Enables **planning** (model-based) vs. **direct interaction** (model-free).\n",
    "\n",
    "---\n",
    "\n",
    "## 1.4 Limitations and Scope\n",
    "\n",
    "- Assumes a **state signal** is given (e.g., as preprocessed input).\n",
    "- Reinforcement learning does not inherently solve **state representation**.\n",
    "- Evolutionary methods can solve RL problems without value functions, but often less efficient.\n",
    "- Focus of this book: **model-free and model-based RL** with value estimation.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.5 An Extended Example: Tic-Tac-Toe\n",
    "\n",
    "Goal: Learn to beat an imperfect opponent without knowing its strategy a priori.\n",
    "\n",
    "> About the best one can do on this problem is first to learn a model of the opponent’s behavior, up to some level of confidence, and then apply dynamic programming to compute an optimal solution given the approximate opponent model. In the end, this is not that different from some of the reinforcement learning methods we examine later in this book.\n",
    "\n",
    "KEY: In this example, \"value\" is the estimated probability of winning in a given board state. The \"value function\" is the learned $P(win)$ estimates for ALL board states.\n",
    "\n",
    "### **Approach _using a value function_**:\n",
    "- Initialize a \"value table\" $V(s)$ that maps board states to win probabilities:\n",
    "   - Each \"cell\" in the value table corresponds to a possible board state $s$. \n",
    "   - Each cell's value is the latest estimate of the probability of winning from that state (that state's _value_ $V(s_i)$).\n",
    "   - The whole table is the learned _value function_.\n",
    "   - Initialize the table with:\n",
    "      - 1s for all states that have 3 Xs in a row.\n",
    "      - 0s for all states that have 3 Os in a row.\n",
    "      - 0.5 for all other states.\n",
    "- Play a bunch of games against the opponent and update the values.\n",
    "- Use mostly greedy moves (choose highest available $V(s)$), but occasionally explore (choose non-highest $V(s)$).\n",
    "- Once the game is finished, **after greedy moves only**, update the value of the previous state ($V(S_t)$) to be closer to the current state ($V(S_{t+1})$):\n",
    "  $$\n",
    "  V(S_t) \\leftarrow V(S_t) + \\alpha \\left[V(S_{t+1}) - V(S_t)\\right]\n",
    "  $$\n",
    "   - where $\\alpha$ is a step-size parameter (small positive fraction).\n",
    "   - **START AT THE TERMINAL STATE AND MOVE BACKWARDS, BACKPROPAGATING THE UPDATES TO $S_0$ FOR GREEDY MOVES ONLY**.\n",
    "   - Since value updates occur **only after greedy moves**, exploration is not used for learning.\n",
    "- This is an example of a temporal-difference update rule.\n",
    "\n",
    "<img src=\"../img/3.png\" alt=\"tictactoe-backprop\" width=\"50%\"/>\n",
    "\n",
    "#### **Note on mechanics**:\n",
    "- It can (I think) be assumed that Sutton & Barto's tic-tac-toe example is \"episodic TD(0)\", where the update rule is applied after the game concludes.\n",
    "   - This means that the post-game update applied to the penultimate move causes the update rule to backprop over multiple timesteps (in reverse order...\"one-step bootsrapping\"), sweeping from $S_{T-1}$ back to $S_0$ (but not applied for any exploration moves made along the way).\n",
    "- \"Online TD(0)\": each update happens immediately after a step. \n",
    "   - In the first few games, the only signal comes from terminal states, as there is no gradient information to drive updates between intermediate timesteps.\n",
    "      - In game 1, since all non-final board states are initiated with 0.5, $V(S_{t+1})$ does not impact the update to the previous state's value (because $V(S_{t+1}) - V(S_t)$ = 0) for all intermediate timesteps. However, the second-to-last board state does get updated with either $\\alpha \\left[1 - 0.5\\right]$ or $\\alpha \\left[0 - 0.5\\right]$ based on win or loss. In future games, the third from last board state _may_ (or may not) get updated because the penultimate is now updated.\n",
    "\n",
    "#### **Why Exploration Helps Even Without Direct Updates**:\n",
    "\n",
    "Exploration moves don’t directly cause value updates, but they indirectly shape which states you’ll encounter later in the game. By choosing exploratory actions, the learner encounters and evaluates board states that would never appear if playing purely greedily. This leads to two key impacts on learning:\n",
    "1.\tIndirect discovery of better strategies:\n",
    "   - Exploration moves lead the agent into unfamiliar board states. While these states themselves aren’t updated immediately, the greedy moves taken afterwards (once back on the greedy path) do get updated based on these newly discovered outcomes.\n",
    "   - Thus, exploratory moves can uncover states with higher true values than currently known, indirectly reshaping the value function.\n",
    "2.\tIndirect correction of overly optimistic values:\n",
    "   - Conversely, exploration can also reveal states that initially appeared promising (due to limited experience), but actually lead to losses.\n",
    "   - Even though the exploratory moves themselves aren’t updated directly, they still “set up” future greedy moves to be evaluated realistically, correcting inflated values indirectly.\n",
    "\n",
    "### **Takeaways**:\n",
    "- RL learns from interaction without a model of the opponent.\n",
    "- It backs up values from future to past states.\n",
    "- Unlike evolutionary methods, RL learns online, and credit assignment is finer-grained.\n",
    "\n",
    "### **Model-Free Nature**:\n",
    "- Doesn’t require modeling opponent or future states.\n",
    "- Works via **temporal-difference (TD) learning**.\n",
    "- Can generalize using function approximators (e.g., Tesauro’s neural network for Backgammon).\n",
    "\n",
    "---\n",
    "\n",
    "## 1.6 Summary\n",
    "\n",
    "- RL is a third paradigm alongside supervised and unsupervised learning.\n",
    "- Uses **Markov Decision Processes (MDPs)** to formalize interaction.\n",
    "- Central tools:\n",
    "  - **Value functions**\n",
    "  - **Trial-and-error updates**\n",
    "- RL = learning by *interacting*, *adapting*, and *optimizing long-term outcomes*.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.7 Early History of Reinforcement Learning\n",
    "\n",
    "**Two foundational threads**:\n",
    "1. **Trial-and-error learning**:\n",
    "   - Roots in psychology: Thorndike’s *Law of Effect*, Skinner’s reinforcement, Pavlov’s conditioning.\n",
    "   - Turing (1948) described pleasure/pain driven learning machines.\n",
    "   - Shannon’s *Theseus*, Michie’s *MENACE*, Samuel’s *checkers program* were early digital examples.\n",
    "\n",
    "2. **Optimal control and dynamic programming**:\n",
    "   - Bellman’s **DP** and **Bellman equations** in 1950s.\n",
    "   - MDPs (Howard, 1960) formalized stochastic control problems.\n",
    "   - Combined with function approximation: \"neurodynamic programming\".\n",
    "\n",
    "**Modern RL** = Integration of:\n",
    "- Dynamic programming theory (optimal control)\n",
    "- Psychological theories of learning\n",
    "- Computational models like TD-learning\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter 1 Exercises\n",
    "\n",
    "<img src=\"../img/4.png\" alt=\"4\" width=\"50%\"/>\n",
    "<br>\n",
    "<img src=\"../img/5.png\" alt=\"5\" width=\"50%\"/>\n",
    "\n",
    "### Answers & Intuition\n",
    "\n",
    "---\n",
    "\n",
    "#### **Exercise 1.1: Self-Play**\n",
    "**Q:** What happens if the RL algorithm plays against itself, both sides learning? Would it learn a different policy for selecting moves?\n",
    "\n",
    "**A:**  \n",
    "If both sides learn via self-play, the algorithm can converge to an optimal (or near-optimal) policy for both players, potentially leading to a policy that is robust to strong opponents, not just random ones. The learned policy may differ from what is learned against a random opponent, as the distribution of experiences will be more challenging and realistic.\n",
    "\n",
    "**Intuition:**  \n",
    "Self-play is foundational in RL for learning robust strategies, as seen in AlphaGo/AlphaZero. This question builds intuition for learning from interaction with non-stationary or improving opponents.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Exercise 1.2: Symmetries**\n",
    "**Q:** How might we amend the learning process to exploit board symmetries? How would this help? Should we still do this if the opponent does not exploit symmetries? Do symmetric positions necessarily have the same value?\n",
    "\n",
    "**A:**  \n",
    "We can exploit symmetries by treating symmetric board positions as equivalent, updating their values together, which reduces the state space and improves data efficiency. This accelerates learning because knowledge from one symmetric state transfers to others.  \n",
    "If the opponent does *not* exploit symmetries, we may still benefit, but symmetric positions could end up having different practical values due to asymmetric play. Symmetric positions *should* have the same value only if the opponent's policy is itself symmetric.\n",
    "\n",
    "**Intuition:**  \n",
    "This introduces the idea of state abstraction and function approximation—core to generalization and sample efficiency in RL.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Exercise 1.3: Greedy Play**\n",
    "**Q:** What happens if the player always acts greedily (never explores)? Might it learn to play better or worse than a non-greedy player? What problems can occur?\n",
    "\n",
    "**A:**  \n",
    "A purely greedy player risks converging to suboptimal policies due to lack of exploration (\"getting stuck\" in local optima), and may never discover better moves. In practice, this can lead to worse performance compared to a player who explores and updates values more broadly. Problems include incomplete learning and overfitting to initial experiences.\n",
    "\n",
    "**Intuition:**  \n",
    "This exercise motivates the importance of exploration in RL (vs. exploitation), foundational for concepts like $\\epsilon$-greedy, softmax, and exploration-exploitation tradeoff.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Exercise 1.4: Learning from Exploration**\n",
    "**Q:** What if learning updates occur after all moves (including exploratory)? What are the two sets of probabilities? Which method is better for learning/winning?\n",
    "\n",
    "**A:**  \n",
    "- **If we update after all moves (including exploratory):** The value estimates reflect the actual probabilities with which moves are taken, blending greedy and exploratory choices.\n",
    "- **If we update only after greedy moves:** The values reflect the policy if only the best-known moves are always chosen.\n",
    "- Continuing to make exploratory moves but learning only from greedy moves focuses learning on the policy we intend to follow for winning, which usually results in better performance than learning from all moves (which blends exploration into the policy).\n",
    "\n",
    "**Intuition:**  \n",
    "This foreshadows \"on-policy\" vs. \"off-policy\" learning distinctions and highlights how the target of updates matters in RL algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Exercise 1.5: Other Improvements**\n",
    "**Q:** What other ways can we improve the RL player? Any better way to solve the tic-tac-toe problem?\n",
    "\n",
    "**A:**  \n",
    "Possible improvements:\n",
    "- Use deeper lookahead (planning) or tree search.\n",
    "- Incorporate opponent modeling.\n",
    "- Use value function approximation (e.g., neural nets).\n",
    "- Exploit more domain knowledge (e.g., forced moves).\n",
    "- Enhance exploration strategy (e.g., UCB, optimism).\n",
    "For tic-tac-toe, full minimax search can solve the game exactly, but the RL approach generalizes to much harder problems.\n",
    "\n",
    "**Intuition:**  \n",
    "This encourages thinking about RL's flexibility and limitations, foreshadowing the use of model-based planning, opponent modeling, and advanced function approximation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03362b9",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cf86eb",
   "metadata": {},
   "source": [
    "# Appendix A: Full Notation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c0755",
   "metadata": {},
   "source": [
    "### General Notation\n",
    "\n",
    "> Capital letters are used for random variables.\n",
    "\n",
    "> Lower case letters are used for the values of random variables and for scalar functions.\n",
    "\n",
    "> Quantities that are required to be real-valued vectors are written in bold and in lower case (even if random variables). \n",
    "\n",
    "> Matrices are bold capitals.\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **Standard Operators** |  |\n",
    "| $\\doteq$ | Equality relationship that is true by definition |\n",
    "| $\\approx$ | Approximately equal |\n",
    "| $\\propto$ | Proportional to |\n",
    "| $\\Pr\\{X = x\\}$ | Probability that random variable $X$ takes value $x$ |\n",
    "| $X \\sim p$ | $X$ drawn from distribution $p$ |\n",
    "| $\\mathbb{E}[X]$ | Expectation of random variable $X$ |\n",
    "| $\\arg\\max_a f(a)$ | Value of $a$ that maximizes $f(a)$ |\n",
    "| $\\ln x$ | Natural logarithm of $x$ |\n",
    "| $e^x$, $\\exp(x)$ | The base of the natural logarithm, $e \\approx 2.71828$, carried to power $x$ |\n",
    "| $\\mathbb{R}$ | Set of real numbers |\n",
    "| $f: \\mathcal{X} \\to \\mathcal{Y}$ | Function from elements of set $\\mathcal{X}$ to elements of set $\\mathcal{Y}$ |\n",
    "| $\\leftarrow$ | Assignment |\n",
    "| $(a, b]$ | Real interval between $a$ and $b$ including $b$ but not $a$ |\n",
    "| **Standard RL Algorithm Parameters** |  |\n",
    "| $\\epsilon$ | Probability of taking a random action in an $\\epsilon$-greedy policy |\n",
    "| $\\alpha, \\beta$ | Step-size parameters |\n",
    "| $\\gamma$ | Discount-rate parameter |\n",
    "| $\\lambda$ | Decay-rate parameter for eligibility traces |\n",
    "| $\\mathbf{1}_\\text{predicate}$ | Indicator function (1 if predicate is true, else 0) |\n",
    "\n",
    "---\n",
    "\n",
    "### Multi-Armed Bandit Notation\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| $k$ | Number of actions (arms) |\n",
    "| $t$ | Discrete time step or play number |\n",
    "| $q_*(a)$ | True value (expected reward) of action $a$ |\n",
    "| $Q_t(a)$ | Estimate at time $t$ of $q_*(a)$ |\n",
    "| $N_t(a)$ | Number of times action $a$ has been selected up to time $t$ |\n",
    "| $H_t(a)$ | Learned preference for selecting action $a$ at time $t$ |\n",
    "| $\\pi_t(a)$ | Probability of selecting action $a$ at time $t$ |\n",
    "| $\\bar{R}_t$ | Estimate at time $t$ of the expected reward given $\\pi_t$ |\n",
    "\n",
    "---\n",
    "\n",
    "### Markov Decision Process Notation\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **State & Action Sets** |  |\n",
    "| $s, s'$ | States |\n",
    "| $a$ | An action |\n",
    "| $r$ | A reward |\n",
    "| $\\mathcal{S}$ | Set of all nonterminal states |\n",
    "| $\\mathcal{S}^+$ | Set of all states, incl. terminal state |\n",
    "| $\\mathcal{A}(s)$ | Set of all actions available in state $s$ |\n",
    "| $\\mathcal{R}$ | Set of all possible rewards, a finite subset of $\\mathbb{R}$ |\n",
    "| $\\mathcal{C}$ | Subset of (e.g., $\\mathcal{R} \\subset \\mathbb{R}$) |\n",
    "| $\\in$ | Is an element of (e.g. $s \\in \\mathcal{S}$, $r \\in \\mathcal{R}$) |\n",
    "| $\\lvert\\mathcal{S}\\rvert$ | Number of elements in set $\\mathcal{S}$ |\n",
    "| **Time & Policy** |  |\n",
    "| $t$ | Discrete time step |\n",
    "| $T, T(t)$ | Final time step of episode, or including $t$ |\n",
    "| $A_t$ | Action at time $t$ |\n",
    "| $S_t$ | State at time $t$ |\n",
    "| $R_t$ | Reward at time $t$ |\n",
    "| $\\pi$ | Policy (decision-making rule) |\n",
    "| $\\pi(s)$ | Action taken in state $s$ under deterministic $\\pi$ |\n",
    "| $\\pi(a \\mid s)$ | Probability of taking action $a$ in state $s$ under stochastic $\\pi$ |\n",
    "| **Returns** |  |\n",
    "| $G_t$ | Return following time $t$ |\n",
    "| $h$ | Horizon (timestep looked up to in forward view) |\n",
    "| $G_{t:t+n}, G_{t:h}$ | $n$-step return from $t+1$ to $t+n$ or $h$ (discounted/corrected) |\n",
    "| $G_{t:h}$ | Flat return (undiscounted/unadjusted) from $t+1$ to $h$ |\n",
    "| $G^\\lambda_t$ | $\\lambda$-return |\n",
    "| $G^{\\wedge}_t$, $G^{\\wedge a}_t$ | Truncated, corrected $\\lambda$-return |\n",
    "| **Transition & Reward Probabilities** |  |\n",
    "| $p(s', r \\mid s, a)$ | Probability of transition to $s'$ with reward $r$ from $s, a$ |\n",
    "| $p(s' \\mid s, a)$ | Probability of transition to $s'$ from $s$ taking $a$ |\n",
    "| $r(s, a)$ | Expected immediate reward from $s$ after $a$ |\n",
    "| $r(s, a, s')$ | Expected reward on transition $s \\to s'$ under $a$ |\n",
    "| **Value Functions** |  |\n",
    "| $v_\\pi(s)$ | Value of $s$ under policy $\\pi$ (expected return) |\n",
    "| $v_*(s)$ | Value of $s$ under optimal policy |\n",
    "| $q_\\pi(s, a)$ | Value of taking $a$ in $s$ under $\\pi$ |\n",
    "| $q_*(s, a)$ | Value of taking $a$ in $s$ under optimal policy |\n",
    "| **Estimators & TD Error** |  |\n",
    "| $V, V_t$ | Array estimates of $v_\\pi$ or $v_*$ |\n",
    "| $Q, Q_t$ | Array estimates of $q_\\pi$ or $q_*$ |\n",
    "| $\\hat{V}(s)$ | Expected approximate action value; e.g. $\\hat{V}_t(s) \\doteq \\sum_a \\pi(a|s) Q_t(s, a)$ |\n",
    "| $U_t$ | Target for estimate at time $t$ |\n",
    "| $\\delta_t$ | Temporal-difference (TD) error at $t$ (a random variable) |\n",
    "| $\\delta_t^s$, $\\delta_t^a$ | State- and action-specific forms of TD error |\n",
    "| $n$ | In $n$-step methods, $n$ is number of steps of bootstrapping |\n",
    "---\n",
    "\n",
    "### Function Approximation, Policy Gradient, Advanced Notation\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **Weight & Parameter Notation** |  |\n",
    "| $d$ | Dimensionality—number of components of $\\mathbf{w}$ |\n",
    "| $d'$ | Alternate dimensionality—number of components of $\\theta$ |\n",
    "| $\\mathbf{w}, \\mathbf{w}_t$ | $d$-vector of weights underlying approximate value function |\n",
    "| $w_i, w_{t,i}$ | $i$-th component of learnable weight vector |\n",
    "| $\\hat{v}(s, \\mathbf{w})$ | Approximate value of $s$ given $\\mathbf{w}$ |\n",
    "| $v_\\mathbf{w}(s)$ | Alternate notation for $\\hat{v}(s, \\mathbf{w})$ |\n",
    "| $\\hat{q}(s, a, \\mathbf{w})$ | Approximate value of $(s, a)$ given $\\mathbf{w}$ |\n",
    "| $\\nabla \\hat{v}(s, \\mathbf{w})$ | Column vector of partial derivatives w.r.t. $\\mathbf{w}$ |\n",
    "| $\\nabla \\hat{q}(s, a, \\mathbf{w})$ | Column vector of partial derivatives w.r.t. $\\mathbf{w}$ |\n",
    "| **Feature Vectors & Eligibility Traces** |  |\n",
    "| $\\mathbf{x}(s)$ | Feature vector visible in $s$ |\n",
    "| $\\mathbf{x}(s, a)$ | Feature vector visible in $s$ taking $a$ |\n",
    "| $x_i(s), x_i(s, a)$ | $i$-th component of $\\mathbf{x}(s)$ or $\\mathbf{x}(s, a)$ |\n",
    "| $\\mathbf{x}_t$ | Shorthand for $\\mathbf{x}(S_t)$ or $\\mathbf{x}(S_t, A_t)$ |\n",
    "| $\\mathbf{w}^\\top \\mathbf{x}$ | Inner product of vectors |\n",
    "| $\\mathbf{v}, \\mathbf{v}_t$ | Secondary $d$-vector of weights, used to learn $\\mathbf{w}$ |\n",
    "| $\\mathbf{z}_t$ | $d$-vector of eligibility traces at $t$ |\n",
    "| **Policy Gradient Notation** |  |\n",
    "| $\\theta, \\theta_t$ | Parameter vector of target policy |\n",
    "| $\\pi(a \\mid s, \\theta)$ | Probability of taking $a$ in $s$ given $\\theta$ |\n",
    "| $\\pi_\\theta$ | Policy corresponding to parameter $\\theta$ |\n",
    "| $\\nabla \\pi(a \\mid s, \\theta)$ | Partial derivatives of $\\pi(a \\mid s, \\theta)$ w.r.t. $\\theta$ |\n",
    "| $J(\\theta)$ | Performance measure for policy $\\pi_\\theta$ |\n",
    "| $\\nabla J(\\theta)$ | Partial derivatives of $J(\\theta)$ w.r.t. $\\theta$ |\n",
    "| $h(s, a, \\theta)$ | Preference for $a$ in $s$ based on $\\theta$ |\n",
    "| **Behavior Policy, Baselines, Importance Sampling** |  |\n",
    "| $b(a \\mid s)$ | Behavior policy used to select actions while learning target $\\pi$ |\n",
    "| $b(s)$ | Baseline function $b: \\mathcal{S} \\to \\mathbb{R}$ for policy-gradient methods |\n",
    "| $b$ | Branching factor for MDP/search tree |\n",
    "| $\\rho_{t:h}$ | Importance sampling ratio for $t$ through $h$ |\n",
    "| $\\rho_t$ | Importance sampling ratio for time $t$ alone, $\\rho_t \\doteq \\rho_{t:t}$ |\n",
    "| $r(\\pi)$ | Average reward (reward rate) for policy $\\pi$ |\n",
    "| $\\bar{R}_t$ | Estimate of $r(\\pi)$ at time $t$ |\n",
    "| **State Distributions & Operators** |  |\n",
    "| $\\mu(s)$ | On-policy distribution over states |\n",
    "| $\\mu$ | $\\lvert\\mathcal{S}\\rvert$-vector of the $\\mu(s)$ for $s \\in \\mathcal{S}$ |\n",
    "| $\\|v\\|^2_\\mu$ | $\\mu$-weighted squared norm of $v$, i.e., $\\|v\\|^2_\\mu \\doteq \\sum_{s \\in \\mathcal{S}} \\mu(s)v(s)^2$ |\n",
    "| $\\eta(s)$ | Expected number of visits to $s$ per episode |\n",
    "| $\\Pi$ | Projection operator for value functions |\n",
    "| $B_\\pi$ | Bellman operator for value functions |\n",
    "\n",
    "---\n",
    "\n",
    "### Matrices, Bellman Error, & Error Metrics\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **Matrix Notation & Linear Algebra** |  |\n",
    "| $\\mathbf{A}$ | $d \\times d$ matrix: $\\mathbf{A} \\doteq \\mathbb{E}\\left[ \\mathbf{x}_t(\\mathbf{x}_t - \\gamma \\mathbf{x}_{t+1})^\\top \\right]$ |\n",
    "| $\\mathbf{b}$ | $d$-dimensional vector: $\\mathbf{b} \\doteq \\mathbb{E}[R_{t+1} \\mathbf{x}_t]$ |\n",
    "| $\\mathbf{w}_{TD}$ | TD fixed point: $\\mathbf{w}_{TD} \\doteq \\mathbf{A}^{-1}\\mathbf{b}$ |\n",
    "| $\\mathbf{I}$ | Identity matrix |\n",
    "| $\\mathbf{P}$ | $\\lvert\\mathcal{S}\\rvert \\times \\lvert\\mathcal{S}\\rvert$ matrix of state-transition probabilities under $\\pi$ |\n",
    "| $\\mathbf{D}$ | $\\lvert\\mathcal{S}\\rvert \\times \\lvert\\mathcal{S}\\rvert$ diagonal matrix with $\\mu$ on its diagonal |\n",
    "| $\\mathbf{X}$ | $\\lvert\\mathcal{S}\\rvert \\times d$ matrix with the $\\mathbf{x}(s)$ as its rows |\n",
    "| **Bellman Error & Value Error Metrics** |  |\n",
    "| $\\bar{\\delta}_\\mathbf{w}(s)$ | Bellman error (expected TD error) for $v_\\mathbf{w}$ at $s$ |\n",
    "| $\\bar{\\delta}_\\mathbf{w}$, BE | Bellman error vector (with components $\\bar{\\delta}_\\mathbf{w}(s)$) |\n",
    "| $\\text{VE}(\\mathbf{w})$ | Mean square value error: $\\text{VE}(\\mathbf{w}) \\doteq \\|v_\\mathbf{w} - v_\\pi\\|^2_\\mu$ |\n",
    "| $\\text{BE}(\\mathbf{w})$ | Mean square Bellman error: $\\text{BE}(\\mathbf{w}) \\doteq \\|\\bar{\\delta}_\\mathbf{w}\\|^2_\\mu$ |\n",
    "| $\\text{PBE}(\\mathbf{w})$ | Mean square projected Bellman error: $\\text{PBE}(\\mathbf{w}) \\doteq \\|\\Pi \\bar{\\delta}_\\mathbf{w}\\|^2_\\mu$ |\n",
    "| $\\text{TDE}(\\mathbf{w})$ | Mean square temporal-difference error: $\\text{TDE}(\\mathbf{w}) \\doteq \\mathbb{E}_b[\\rho_t \\delta_t^2]$ |\n",
    "| $\\text{RE}(\\mathbf{w})$ | Mean square return error |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
