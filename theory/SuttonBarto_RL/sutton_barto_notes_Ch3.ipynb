{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7521318",
   "metadata": {},
   "source": [
    "# Chapter 3: Finite Markov Decision Processes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27a5db8",
   "metadata": {},
   "source": [
    "**Chapter Overview**: This chapter introduces the formal mathematical framework that underlies most of reinforcement learning. We transition from simple bandits (Chapter 2) to the full RL problem where actions affect not just immediate rewards, but also future states and opportunities.\n",
    "\n",
    "**Key Distinction from Bandits**: \n",
    "- **Bandits**: Actions only affect immediate rewards\n",
    "- **MDPs**: Actions affect both immediate rewards AND future states (and thus future rewards)\n",
    "\n",
    "![Agent-Environment Interface](../img/fig3_1.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 The Agent–Environment Interface\n",
    "\n",
    "**Core Framework**: The MDP framework models interaction between an **agent** (learner/decision-maker) and **environment** (everything outside the agent).\n",
    "\n",
    "### The Interaction Loop\n",
    "\n",
    "**Basic sequence** at discrete time steps $t = 0, 1, 2, 3, ...$:\n",
    "1. Agent observes state $S_t$\n",
    "2. Agent selects action $A_t$ \n",
    "3. Environment responds with reward $R_{t+1}$ and new state $S_{t+1}$\n",
    "\n",
    "**Trajectory**: $S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, ...$\n",
    "\n",
    "### MDP Dynamics Function\n",
    "\n",
    "**Four-argument dynamics function** (_Equation 3.2_):\n",
    "\n",
    "$$p(s', r|s, a) \\doteq \\Pr\\{S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a\\}$$\n",
    "\n",
    "**Components**:\n",
    "- $s, s'$: Current and next states\n",
    "- $a$: Action taken  \n",
    "- $r$: Reward received\n",
    "- $p(s', r|s, a)$: Probability of transitioning to state $s'$ and receiving reward $r$\n",
    "\n",
    "**Mathematical intuition**: This function completely characterizes the environment's behavior. Given current state and action, it tells us the probability of every possible outcome.\n",
    "\n",
    "**Probability constraint** (_Equation 3.3_):\n",
    "$$\\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r|s, a) = 1, \\text{ for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)$$\n",
    "\n",
    "### Derived Functions\n",
    "\n",
    "From the four-argument $p$ function, we can compute:\n",
    "\n",
    "**State transition probabilities** (_Equation 3.4_):\n",
    "$$p(s'|s, a) = \\sum_{r \\in \\mathcal{R}} p(s', r|s, a)$$\n",
    "\n",
    "**Expected rewards for state-action pairs** (_Equation 3.5_):\n",
    "$$r(s, a) = \\sum_{r \\in \\mathcal{R}} r \\sum_{s' \\in \\mathcal{S}} p(s', r|s, a)$$\n",
    "\n",
    "**Expected rewards for state-action-next-state triples** (_Equation 3.6_):\n",
    "$$r(s, a, s') = \\sum_{r \\in \\mathcal{R}} r \\frac{p(s', r|s, a)}{p(s'|s, a)}$$\n",
    "\n",
    "### The Markov Property\n",
    "\n",
    "**Definition**: The state must include all information about the past that affects the future. \n",
    "\n",
    "**Mathematical statement**: $P(S_{t+1}, R_{t+1} | S_t, A_t, S_{t-1}, A_{t-1}, ...) = P(S_{t+1}, R_{t+1} | S_t, A_t)$\n",
    "\n",
    "**Intuition**: If you know the current state, the past doesn't matter for predicting the future.\n",
    "\n",
    "### Agent-Environment Boundary\n",
    "\n",
    "**Key principle**: The boundary represents the limit of the agent's **absolute control**, not its knowledge.\n",
    "\n",
    "**Examples**:\n",
    "- **Robot arm**: Motors and sensors are part of environment, not agent\n",
    "- **Chess program**: Board position is state, but opponent's strategy is part of environment\n",
    "- **Human**: Muscles and sensory organs are environment\n",
    "\n",
    "**Practical guideline**: Everything the agent cannot arbitrarily change belongs to the environment.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2 Goals and Rewards\n",
    "\n",
    "### The Reward Hypothesis\n",
    "\n",
    "> **Reward Hypothesis**: All goals and purposes can be thought of as maximizing the expected value of the cumulative sum of a received scalar signal (reward).\n",
    "\n",
    "**Key insights**:\n",
    "- Rewards define the goal, not how to achieve it\n",
    "- Don't reward intermediate steps—reward the actual objective\n",
    "- Example: Chess agent should be rewarded for winning, not for taking pieces\n",
    "\n",
    "**Bad example**: Rewarding a chess agent for taking opponent pieces might lead it to take pieces while losing the game.\n",
    "\n",
    "### Reward Signal Design\n",
    "\n",
    "**Critical principle**: Use rewards to communicate **what** you want achieved, not **how** to achieve it.\n",
    "\n",
    "**Examples of good reward design**:\n",
    "- **Walking robot**: +1 for each step forward\n",
    "- **Maze escape**: -1 per time step until escape (encourages speed)\n",
    "- **Game playing**: +1 win, -1 loss, 0 draw\n",
    "- **Can collection**: +1 per can collected\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3 Returns and Episodes\n",
    "\n",
    "### Episodes vs. Continuing Tasks\n",
    "\n",
    "**Episodic tasks**: Natural breaking points (episodes)\n",
    "- Examples: Games, maze runs, conversations\n",
    "- Each episode ends in **terminal state**\n",
    "- Episodes are independent\n",
    "\n",
    "**Continuing tasks**: No natural endpoints\n",
    "- Examples: Process control, life-long learning\n",
    "- Interaction continues indefinitely\n",
    "\n",
    "### Return Definitions\n",
    "\n",
    "**Simple return for episodic tasks** (_Equation 3.7_):\n",
    "$$G_t = R_{t+1} + R_{t+2} + R_{t+3} + \\cdots + R_T$$\n",
    "\n",
    "**Components**:\n",
    "- $G_t$: Return starting from time $t$\n",
    "- $T$: Final time step of episode\n",
    "- Simple sum of all future rewards in episode\n",
    "\n",
    "### Discounted Return\n",
    "\n",
    "**Discounted return** (_Equation 3.8_):\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n",
    "\n",
    "**Components**:\n",
    "- $\\gamma$: Discount rate, $0 \\leq \\gamma \\leq 1$\n",
    "- $\\gamma^k$: Discount factor for reward $k$ steps in future\n",
    "\n",
    "**Mathematical intuition**: Future rewards are worth less than immediate rewards. The discount rate $\\gamma$ controls how much we value the future.\n",
    "\n",
    "**Special cases**:\n",
    "- $\\gamma = 0$: Only immediate reward matters (myopic)\n",
    "- $\\gamma = 1$: All rewards equally important (far-sighted)\n",
    "- $\\gamma < 1$: Ensures finite return even for infinite sequences\n",
    "\n",
    "### Recursive Return Relationship\n",
    "\n",
    "**Fundamental recursion** (_Equation 3.9_):\n",
    "$$G_t = R_{t+1} + \\gamma G_{t+1}$$\n",
    "\n",
    "**Mathematical intuition**: Today's return equals immediate reward plus discounted future return. This recursion is the foundation of all RL algorithms.\n",
    "\n",
    "**Example calculation**: If rewards are constant +1 and $\\gamma < 1$:\n",
    "$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1-\\gamma}$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4 Unified Notation for Episodic and Continuing Tasks\n",
    "\n",
    "**Key insight**: We can treat episodic tasks as continuing tasks with absorbing terminal states that give zero reward.\n",
    "\n",
    "![State Transition Diagram](../img/fig3_2.png)\n",
    "\n",
    "**Unified return formula** (_Equation 3.11_):\n",
    "$$G_t = \\sum_{k=t+1}^{T} \\gamma^{k-t-1} R_k$$\n",
    "\n",
    "where $T = \\infty$ or $\\gamma = 1$ (but not both).\n",
    "\n",
    "---\n",
    "\n",
    "## 3.5 Policies and Value Functions\n",
    "\n",
    "### Policies\n",
    "\n",
    "**Definition**: A policy $\\pi$ is a mapping from states to probabilities of selecting each action.\n",
    "\n",
    "**Stochastic policy**: $\\pi(a|s) = $ probability of taking action $a$ in state $s$\n",
    "\n",
    "**Mathematical constraint**: $\\sum_{a} \\pi(a|s) = 1$ for all $s$\n",
    "\n",
    "### State-Value Functions\n",
    "\n",
    "**State-value function** (_Equation 3.12_):\n",
    "$$v_\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s] = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\bigg| S_t = s\\right]$$\n",
    "\n",
    "**Components**:\n",
    "- $v_\\pi(s)$: Expected return starting from state $s$ following policy $\\pi$\n",
    "- $\\mathbb{E}_\\pi[\\cdot]$: Expectation when following policy $\\pi$\n",
    "\n",
    "**Mathematical intuition**: How good is it to be in state $s$ if we follow policy $\\pi$ from here on?\n",
    "\n",
    "### Action-Value Functions\n",
    "\n",
    "**Action-value function** (_Equation 3.13_):\n",
    "$$q_\\pi(s, a) = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a] = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\bigg| S_t = s, A_t = a\\right]$$\n",
    "\n",
    "**Mathematical intuition**: How good is it to take action $a$ in state $s$, then follow policy $\\pi$?\n",
    "\n",
    "### The Bellman Equation for $v_\\pi$\n",
    "\n",
    "**🏆 FOUNDATIONAL FORMULA** (_Equation 3.14_):\n",
    "$$v_\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s', r|s, a)[r + \\gamma v_\\pi(s')]$$\n",
    "\n",
    "**Components**:\n",
    "- $\\pi(a|s)$: Probability of taking action $a$ in state $s$\n",
    "- $p(s', r|s, a)$: Environment dynamics\n",
    "- $r$: Immediate reward\n",
    "- $\\gamma v_\\pi(s')$: Discounted future value\n",
    "\n",
    "**Mathematical intuition**: The value of a state equals the expected immediate reward plus the expected discounted value of the next state. This captures the recursive nature of value.\n",
    "\n",
    "**Derivation insight**:\n",
    "1. Start with definition: $v_\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s]$\n",
    "2. Use recursion: $G_t = R_{t+1} + \\gamma G_{t+1}$\n",
    "3. Apply law of total expectation over actions and next states\n",
    "\n",
    "![Backup Diagram for v_π](../img/fig3_3.png)\n",
    "\n",
    "### Backup Diagrams\n",
    "\n",
    "**Backup diagrams** show the relationship between a state (or state-action pair) and its successors:\n",
    "- **Open circles**: States\n",
    "- **Solid circles**: State-action pairs\n",
    "- **Arrows**: Possible transitions\n",
    "\n",
    "---\n",
    "\n",
    "## 3.6 Optimal Policies and Optimal Value Functions\n",
    "\n",
    "### Optimal Value Functions\n",
    "\n",
    "**Optimal state-value function** (_Equation 3.15_):\n",
    "$$v_*(s) = \\max_\\pi v_\\pi(s)$$\n",
    "\n",
    "**Optimal action-value function** (_Equation 3.16_):\n",
    "$$q_*(s, a) = \\max_\\pi q_\\pi(s, a)$$\n",
    "\n",
    "**Mathematical intuition**: These represent the best possible performance achievable from each state or state-action pair.\n",
    "\n",
    "**Relationship** (_Equation 3.17_):\n",
    "$$q_*(s, a) = \\mathbb{E}[R_{t+1} + \\gamma v_*(S_{t+1}) | S_t = s, A_t = a]$$\n",
    "\n",
    "### Bellman Optimality Equations\n",
    "\n",
    "**🏆 FOUNDATIONAL FORMULA - Bellman Optimality Equation for $v_*$** (_Equations 3.18-3.19_):\n",
    "$$v_*(s) = \\max_a \\mathbb{E}[R_{t+1} + \\gamma v_*(S_{t+1}) | S_t = s, A_t = a]$$\n",
    "$$v_*(s) = \\max_a \\sum_{s',r} p(s', r|s, a)[r + \\gamma v_*(s')]$$\n",
    "\n",
    "**Components**:\n",
    "- $\\max_a$: Choose the best action\n",
    "- Rest is same as Bellman equation, but now we optimize over actions\n",
    "\n",
    "**Mathematical intuition**: The value of a state under optimal policy equals the value of the best action from that state. We replace expectation over policy with maximization over actions.\n",
    "\n",
    "**🏆 FOUNDATIONAL FORMULA - Bellman Optimality Equation for $q_*$** (_Equation 3.20_):\n",
    "$$q_*(s, a) = \\sum_{s',r} p(s', r|s, a)[r + \\gamma \\max_{a'} q_*(s', a')]$$\n",
    "\n",
    "**Mathematical intuition**: The value of taking action $a$ in state $s$ equals the expected immediate reward plus the discounted value of the best action in the next state.\n",
    "\n",
    "![Backup Diagrams for Optimal Value Functions](../img/fig3_4.png)\n",
    "\n",
    "### Finding Optimal Policies\n",
    "\n",
    "**Key insight**: Once you have $v_*$, finding optimal policy is easy:\n",
    "\n",
    "**Greedy policy extraction**:\n",
    "$$\\pi_*(s) = \\arg\\max_a \\sum_{s',r} p(s', r|s, a)[r + \\gamma v_*(s')]$$\n",
    "\n",
    "**Why this works**: $v_*$ already accounts for all future consequences, so a greedy one-step lookahead gives the optimal action.\n",
    "\n",
    "**With $q_*$, it's even easier**:\n",
    "$$\\pi_*(s) = \\arg\\max_a q_*(s, a)$$\n",
    "\n",
    "### Solving Bellman Optimality Equations\n",
    "\n",
    "**In principle**: Can solve the system of Bellman optimality equations directly\n",
    "- For $n$ states, have $n$ equations in $n$ unknowns\n",
    "- Requires knowing environment dynamics $p(s', r|s, a)$\n",
    "\n",
    "**In practice**: Usually computationally intractable\n",
    "- Example: Backgammon has ~$10^{20}$ states\n",
    "- Most RL methods approximate the solution\n",
    "\n",
    "---\n",
    "\n",
    "## 3.7 Optimality and Approximation\n",
    "\n",
    "### Computational Reality\n",
    "\n",
    "**The fundamental challenge**: Computing optimal policies exactly is usually impossible due to:\n",
    "1. **Computational limits**: Not enough computation per time step\n",
    "2. **Memory constraints**: Cannot store values for all states  \n",
    "3. **Unknown dynamics**: Don't know $p(s', r|s, a)$\n",
    "\n",
    "### Tabular vs. Function Approximation\n",
    "\n",
    "**Tabular methods**: Store separate value for each state\n",
    "- Feasible only for small state spaces\n",
    "- Can find exact solutions\n",
    "\n",
    "**Function approximation**: Use parameterized functions to approximate values\n",
    "- Necessary for large state spaces\n",
    "- Can only find approximate solutions\n",
    "\n",
    "### Approximation Opportunities\n",
    "\n",
    "**Key insight**: RL allows focusing computational resources on frequently encountered states\n",
    "\n",
    "**Example**: Tesauro's backgammon player\n",
    "- Exceptional performance despite potentially poor decisions on rare board positions\n",
    "- Focuses learning on states that actually occur in expert play\n",
    "\n",
    "---\n",
    "\n",
    "## 3.8 Summary\n",
    "\n",
    "### Key Concepts Introduced\n",
    "\n",
    "**MDPs provide the mathematical foundation for RL**:\n",
    "- **States**: Basis for decision-making\n",
    "- **Actions**: Choices available to agent  \n",
    "- **Rewards**: Basis for evaluation\n",
    "- **Policy**: Rule for selecting actions\n",
    "- **Value functions**: Expected future reward\n",
    "\n",
    "**Value functions are central to RL**:\n",
    "- $v_\\pi(s)$: Expected return from state $s$ under policy $\\pi$\n",
    "- $q_\\pi(s,a)$: Expected return from taking action $a$ in state $s$ under policy $\\pi$\n",
    "- $v_*(s)$: Best possible return from state $s$\n",
    "- $q_*(s,a)$: Best possible return from taking action $a$ in state $s$\n",
    "\n",
    "**Bellman equations provide recursive structure**:\n",
    "- Connect value of state to values of successor states\n",
    "- Foundation for most RL algorithms\n",
    "- Optimality equations characterize optimal behavior\n",
    "\n",
    "### Fundamental Takeaways\n",
    "\n",
    "1. **Actions affect both immediate rewards and future opportunities**\n",
    "2. **Value functions capture long-term consequences of decisions**  \n",
    "3. **Optimal policies are greedy with respect to optimal value functions**\n",
    "4. **Exact solutions usually impossible; approximation necessary**\n",
    "5. **RL can focus learning on frequently encountered states**\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 3.1 Solution\n",
    "**Q**: Devise three example tasks that fit the MDP framework.\n",
    "\n",
    "**A**: \n",
    "1. **Autonomous driving**:\n",
    "   - States: (position, speed, traffic conditions, weather)\n",
    "   - Actions: (accelerate, brake, steer left/right, change lanes)\n",
    "   - Rewards: +1 for progress toward destination, -100 for accidents, -1 for traffic violations\n",
    "\n",
    "2. **Stock trading**:\n",
    "   - States: (portfolio value, market indicators, time of day, news sentiment)\n",
    "   - Actions: (buy stock X, sell stock Y, hold, set stop-loss)\n",
    "   - Rewards: Change in portfolio value each day\n",
    "\n",
    "3. **Chatbot conversation**:\n",
    "   - States: (conversation history, user sentiment, topic, user profile)\n",
    "   - Actions: (different response templates, ask question, provide information, end conversation)\n",
    "   - Rewards: +1 for positive user feedback, -1 for user ending conversation early\n",
    "\n",
    "### Exercise 3.5 Solution\n",
    "**Q**: Modify equation (3.3) for episodic tasks.\n",
    "\n",
    "**A**: For episodic tasks, we need to account for transitions to the terminal state:\n",
    "$$\\sum_{s' \\in \\mathcal{S}^+} \\sum_{r \\in \\mathcal{R}} p(s', r|s, a) = 1$$\n",
    "where $\\mathcal{S}^+$ includes the terminal state.\n",
    "\n",
    "### Exercise 3.8 Solution  \n",
    "**Q**: Given $\\gamma = 0.5$ and rewards $R_1 = -1, R_2 = 2, R_3 = 6, R_4 = 3, R_5 = 2$ with $T = 5$, find $G_0, G_1, ..., G_5$.\n",
    "\n",
    "**A**: Working backwards:\n",
    "- $G_5 = 0$ (terminal)\n",
    "- $G_4 = R_5 = 2$  \n",
    "- $G_3 = R_4 + \\gamma G_4 = 3 + 0.5(2) = 4$\n",
    "- $G_2 = R_3 + \\gamma G_3 = 6 + 0.5(4) = 8$\n",
    "- $G_1 = R_2 + \\gamma G_2 = 2 + 0.5(8) = 6$ \n",
    "- $G_0 = R_1 + \\gamma G_1 = -1 + 0.5(6) = 2$\n",
    "\n",
    "### Exercise 3.12 Solution\n",
    "**Q**: Give equation for $v_\\pi$ in terms of $q_\\pi$ and $\\pi$.\n",
    "\n",
    "**A**: \n",
    "$$v_\\pi(s) = \\sum_a \\pi(a|s) q_\\pi(s, a)$$\n",
    "\n",
    "**Intuition**: State value is weighted average of action values under the policy.\n",
    "\n",
    "### Exercise 3.13 Solution\n",
    "**Q**: Give equation for $q_\\pi$ in terms of $v_\\pi$ and $p$.\n",
    "\n",
    "**A**:\n",
    "$$q_\\pi(s, a) = \\sum_{s',r} p(s', r|s, a)[r + \\gamma v_\\pi(s')]$$\n",
    "\n",
    "**Intuition**: Action value equals expected immediate reward plus discounted next state value.\n",
    "\n",
    "### Exercise 3.17 Solution\n",
    "**Q**: What is the Bellman equation for action values $q_\\pi$?\n",
    "\n",
    "**A**:\n",
    "$$q_\\pi(s, a) = \\sum_{s',r} p(s', r|s, a)\\left[r + \\gamma \\sum_{a'} \\pi(a'|s') q_\\pi(s', a')\\right]$$\n",
    "\n",
    "**Components**:\n",
    "- Expected immediate reward: $r$\n",
    "- Expected future value: $\\gamma \\sum_{a'} \\pi(a'|s') q_\\pi(s', a')$\n",
    "\n",
    "### Exercise 3.25 Solution\n",
    "**Q**: Give equation for $v_*$ in terms of $q_*$.\n",
    "\n",
    "**A**:\n",
    "$$v_*(s) = \\max_a q_*(s, a)$$\n",
    "\n",
    "### Exercise 3.27 Solution  \n",
    "**Q**: Give equation for $\\pi_*$ in terms of $q_*$.\n",
    "\n",
    "**A**:\n",
    "$$\\pi_*(a|s) = \\begin{cases} \n",
    "1 & \\text{if } a \\in \\arg\\max_{a'} q_*(s, a') \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Intuition**: Optimal policy puts all probability on action(s) with highest $q_*$ value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66df9a5",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
