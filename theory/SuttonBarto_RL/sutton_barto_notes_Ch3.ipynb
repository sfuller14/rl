{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1841a00e",
   "metadata": {},
   "source": [
    "# Summary of Notation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402c54e",
   "metadata": {},
   "source": [
    "(full notation in Appendix A)\n",
    "\n",
    "> Whenever you see the word \"value\", think \"action's Expected Value of reward\" (AEVOR). This is usually a $P(win)$ or (in the bandit case) probability-weighted-avg $R_t$, given that that action is taken. It can be either a \"True\" value (i.e. the population mean of the action's distribution) or an estimate (i.e. sample statistic) of it.\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| $\\Pr\\{X = x\\}$ | Probability that random variable $X$ takes value $x$ |\n",
    "| $X \\sim p$ | $X$ is drawn from distribution $p$ |\n",
    "| $\\mathbb{E}[X]$ | Expectation of random variable $X$ |\n",
    "| $\\arg\\max_a f(a)$ | Value of $a$ that maximizes $f(a)$ |\n",
    "| $\\alpha$, $\\beta$, $\\epsilon$ | Step-size, decay-rate, and exploration parameters |\n",
    "| $s, s'$ | States |\n",
    "| $a$ | Action |\n",
    "| $r$ | Reward |\n",
    "| $S, A(s), R$ | Set of states, available actions, rewards |\n",
    "| $t, T$ | Discrete time step, final step |\n",
    "| $S_t, A_t, R_t$ | State, action, and reward at time $t$ |\n",
    "| $\\pi$ | Policy (decision-making rule) |\n",
    "| $\\pi(s)$ | action taken in state $s$ under _deterministic_ $\\pi$ |\n",
    "| $\\pi(a \\mid s)$ | Probability of taking action $a$ in state $s$ under _stochastic_ $\\pi$ |\n",
    "| $G_t$ | Return from time $t$ |\n",
    "| $h$ | horizon (the timestep one looks up to in a forward view) |\n",
    "| $v_\\pi(s)$ | Value of state $s$ under policy $\\pi$ |\n",
    "| $q_\\pi(s, a)$ | Value of state-action pair $(s,a)$ under $\\pi$ |\n",
    "| $p(s', r \\mid s, a)$ | Transition dynamics |\n",
    "| $\\hat{v}(s, \\mathbf{w})$ | Approximate value function using weight vector $\\mathbf{w}$ |\n",
    "| $\\delta_t$ | Temporal-difference error at time $t$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402856b7",
   "metadata": {},
   "source": [
    "# Part I: Tabular Solution Methods\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64caf4a",
   "metadata": {},
   "source": [
    "<img src=\"../img/6.png\" alt=\"tabularsolutionmethods\" width=\"40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87706ebe",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7521318",
   "metadata": {},
   "source": [
    "# Chapter 3: Finite Markov Decision Processes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27a5db8",
   "metadata": {},
   "source": [
    "**Chapter Overview**: This chapter introduces the formal mathematical framework that underlies most of reinforcement learning. We transition from simple bandits (Chapter 2) to the full RL problem where actions affect not just immediate rewards, but also future states and opportunities.\n",
    "\n",
    "**Key Distinction from Bandits**: \n",
    "- **Bandits**: Actions only affect immediate rewards\n",
    "- **MDPs**: Actions affect both immediate rewards AND future states (and thus future rewards)\n",
    "\n",
    "![Agent-Environment Interface](../img/fig3_1.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 The Agentâ€“Environment Interface\n",
    "\n",
    "**Core Framework**: The MDP framework models interaction between an **agent** (learner/decision-maker) and **environment** (everything outside the agent).\n",
    "\n",
    "### The Interaction Loop\n",
    "\n",
    "**Basic sequence** at discrete time steps $t = 0, 1, 2, 3, ...$:\n",
    "1. Agent observes state $S_t$\n",
    "2. Agent selects action $A_t$ \n",
    "3. Environment responds with reward $R_{t+1}$ and new state $S_{t+1}$\n",
    "\n",
    "**Trajectory**: $S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, ...$\n",
    "\n",
    "### MDP Dynamics Function\n",
    "\n",
    "**Four-argument dynamics function** (_Equation 3.2_):\n",
    "$$p(s', r|s, a) \\doteq \\Pr\\{S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a\\}$$\n",
    "\n",
    "**Components**:\n",
    "- $s, s'$: Current and next states\n",
    "- $a$: Action taken  \n",
    "- $r$: Reward received\n",
    "- $p(s', r|s, a)$: Probability of transitioning to state $s'$ and receiving reward $r$\n",
    "\n",
    "**Mathematical intuition**: This function completely characterizes the environment's behavior. Given current state and action, it tells us the probability of every possible outcome.\n",
    "\n",
    "**Probability constraint** (_Equation 3.3_):\n",
    "$$\\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r|s, a) = 1, \\text{ for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)$$\n",
    "\n",
    "### Derived Functions\n",
    "\n",
    "From the four-argument $p$ function, we can compute:\n",
    "\n",
    "**State transition probabilities** (_Equation 3.4_):\n",
    "$$p(s'|s, a) = \\sum_{r \\in \\mathcal{R}} p(s', r|s, a)$$\n",
    "\n",
    "**Expected rewards for state-action pairs** (_Equation 3.5_):\n",
    "$$r(s, a) = \\sum_{r \\in \\mathcal{R}} r \\sum_{s' \\in \\mathcal{S}} p(s', r|s, a)$$\n",
    "\n",
    "**Expected rewards for state-action-next-state triples** (_Equation 3.6_):\n",
    "$$r(s, a, s') = \\sum_{r \\in \\mathcal{R}} r \\frac{p(s', r|s, a)}{p(s'|s, a)}$$\n",
    "\n",
    "### The Markov Property\n",
    "\n",
    "**Definition**: The state must include all information about the past that affects the future. \n",
    "\n",
    "**Mathematical statement**: $P(S_{t+1}, R_{t+1} | S_t, A_t, S_{t-1}, A_{t-1}, ...) = P(S_{t+1}, R_{t+1} | S_t, A_t)$\n",
    "\n",
    "**Intuition**: If you know the current state, the past doesn't matter for predicting the future.\n",
    "\n",
    "### Agent-Environment Boundary\n",
    "\n",
    "**Key principle**: The boundary represents the limit of the agent's **absolute control**, not its knowledge.\n",
    "\n",
    "**Examples**:\n",
    "- **Robot arm**: Motors and sensors are part of environment, not agent\n",
    "- **Chess program**: Board position is state, but opponent's strategy is part of environment\n",
    "- **Human**: Muscles and sensory organs are environment\n",
    "\n",
    "**Practical guideline**: Everything the agent cannot arbitrarily change belongs to the environment.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2 Goals and Rewards\n",
    "\n",
    "### The Reward Hypothesis\n",
    "\n",
    "> **Reward Hypothesis**: All goals and purposes can be thought of as maximizing the expected value of the cumulative sum of a received scalar signal (reward).\n",
    "\n",
    "**Key insights**:\n",
    "- Rewards define the goal, not how to achieve it\n",
    "- Don't reward intermediate stepsâ€”reward the actual objective\n",
    "- Example: Chess agent should be rewarded for winning, not for taking pieces\n",
    "\n",
    "**Bad example**: Rewarding a chess agent for taking opponent pieces might lead it to take pieces while losing the game.\n",
    "\n",
    "### Reward Signal Design\n",
    "\n",
    "**Critical principle**: Use rewards to communicate **what** you want achieved, not **how** to achieve it.\n",
    "\n",
    "**Examples of good reward design**:\n",
    "- **Walking robot**: +1 for each step forward\n",
    "- **Maze escape**: -1 per time step until escape (encourages speed)\n",
    "- **Game playing**: +1 win, -1 loss, 0 draw\n",
    "- **Can collection**: +1 per can collected\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3 Returns and Episodes\n",
    "\n",
    "### Episodes vs. Continuing Tasks\n",
    "\n",
    "**Episodic tasks**: Natural breaking points (episodes)\n",
    "- Examples: Games, maze runs, conversations\n",
    "- Each episode ends in **terminal state**\n",
    "- Episodes are independent\n",
    "\n",
    "**Continuing tasks**: No natural endpoints\n",
    "- Examples: Process control, life-long learning\n",
    "- Interaction continues indefinitely\n",
    "\n",
    "### Return Definitions\n",
    "\n",
    "**Simple return for episodic tasks** (_Equation 3.7_):\n",
    "$$G_t = R_{t+1} + R_{t+2} + R_{t+3} + \\cdots + R_T$$\n",
    "\n",
    "**Components**:\n",
    "- $G_t$: Return starting from time $t$\n",
    "- $T$: Final time step of episode\n",
    "- Simple sum of all future rewards in episode\n",
    "\n",
    "### Discounted Return\n",
    "\n",
    "**Discounted return** (_Equation 3.8_):\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n",
    "\n",
    "**Components**:\n",
    "- $\\gamma$: Discount rate, $0 \\leq \\gamma \\leq 1$\n",
    "- $\\gamma^k$: Discount factor for reward $k$ steps in future\n",
    "\n",
    "**Mathematical intuition**: Future rewards are worth less than immediate rewards. The discount rate $\\gamma$ controls how much we value the future.\n",
    "\n",
    "**Special cases**:\n",
    "- $\\gamma = 0$: Only immediate reward matters (myopic)\n",
    "- $\\gamma = 1$: All rewards equally important (far-sighted)\n",
    "- $\\gamma < 1$: Ensures finite return even for infinite sequences\n",
    "\n",
    "### Recursive Return Relationship\n",
    "\n",
    "**Fundamental recursion** (_Equation 3.9_):\n",
    "$$G_t = R_{t+1} + \\gamma G_{t+1}$$\n",
    "\n",
    "**Mathematical intuition**: Today's return equals immediate reward plus discounted future return. This recursion is the foundation of all RL algorithms.\n",
    "\n",
    "**Example calculation**: If rewards are constant +1 and $\\gamma < 1$:\n",
    "$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k = \\frac{1}{1-\\gamma}$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4 Unified Notation for Episodic and Continuing Tasks\n",
    "\n",
    "**Key insight**: We can treat episodic tasks as continuing tasks with absorbing terminal states that give zero reward.\n",
    "\n",
    "![State Transition Diagram](../img/fig3_2.png)\n",
    "\n",
    "**Unified return formula** (_Equation 3.11_):\n",
    "$$G_t = \\sum_{k=t+1}^{T} \\gamma^{k-t-1} R_k$$\n",
    "\n",
    "where $T = \\infty$ or $\\gamma = 1$ (but not both).\n",
    "\n",
    "---\n",
    "\n",
    "## 3.5 Policies and Value Functions\n",
    "\n",
    "### Policies\n",
    "\n",
    "**Definition**: A policy $\\pi$ is a mapping from states to probabilities of selecting each action.\n",
    "\n",
    "**Stochastic policy**: $\\pi(a|s) = $ probability of taking action $a$ in state $s$\n",
    "\n",
    "**Mathematical constraint**: $\\sum_{a} \\pi(a|s) = 1$ for all $s$\n",
    "\n",
    "### State-Value Functions\n",
    "\n",
    "**State-value function** (_Equation 3.12_):\n",
    "$$v_\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s] = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\bigg| S_t = s\\right]$$\n",
    "\n",
    "**Components**:\n",
    "- $v_\\pi(s)$: Expected return starting from state $s$ following policy $\\pi$\n",
    "- $\\mathbb{E}_\\pi[\\cdot]$: Expectation when following policy $\\pi$\n",
    "\n",
    "**Mathematical intuition**: How good is it to be in state $s$ if we follow policy $\\pi$ from here on?\n",
    "\n",
    "### Action-Value Functions\n",
    "\n",
    "**Action-value function** (_Equation 3.13_):\n",
    "$$q_\\pi(s, a) = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a] = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\bigg| S_t = s, A_t = a\\right]$$\n",
    "\n",
    "**Mathematical intuition**: How good is it to take action $a$ in state $s$, then follow policy $\\pi$?\n",
    "\n",
    "### The Bellman Equation for $v_\\pi$\n",
    "\n",
    "**ðŸ† FOUNDATIONAL FORMULA** (_Equation 3.14_):\n",
    "$$v_\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s', r|s, a)[r + \\gamma v_\\pi(s')]$$\n",
    "\n",
    "**Components**:\n",
    "- $\\pi(a|s)$: Probability of taking action $a$ in state $s$\n",
    "- $p(s', r|s, a)$: Environment dynamics\n",
    "- $r$: Immediate reward\n",
    "- $\\gamma v_\\pi(s')$: Discounted future value\n",
    "\n",
    "**Mathematical intuition**: The value of a state equals the expected immediate reward plus the expected discounted value of the next state. This captures the recursive nature of value.\n",
    "\n",
    "**Derivation insight**:\n",
    "1. Start with definition: $v_\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s]$\n",
    "2. Use recursion: $G_t = R_{t+1} + \\gamma G_{t+1}$\n",
    "3. Apply law of total expectation over actions and next states\n",
    "\n",
    "![Backup Diagram for v_Ï€](../img/fig3_3.png)\n",
    "\n",
    "### Backup Diagrams\n",
    "\n",
    "**Backup diagrams** show the relationship between a state (or state-action pair) and its successors:\n",
    "- **Open circles**: States\n",
    "- **Solid circles**: State-action pairs\n",
    "- **Arrows**: Possible transitions\n",
    "\n",
    "---\n",
    "\n",
    "## 3.6 Optimal Policies and Optimal Value Functions\n",
    "\n",
    "### Optimal Value Functions\n",
    "\n",
    "**Optimal state-value function** (_Equation 3.15_):\n",
    "$$v_*(s) = \\max_\\pi v_\\pi(s)$$\n",
    "\n",
    "**Optimal action-value function** (_Equation 3.16_):\n",
    "$$q_*(s, a) = \\max_\\pi q_\\pi(s, a)$$\n",
    "\n",
    "**Mathematical intuition**: These represent the best possible performance achievable from each state or state-action pair.\n",
    "\n",
    "**Relationship** (_Equation 3.17_):\n",
    "$$q_*(s, a) = \\mathbb{E}[R_{t+1} + \\gamma v_*(S_{t+1}) | S_t = s, A_t = a]$$\n",
    "\n",
    "### Bellman Optimality Equations\n",
    "\n",
    "**ðŸ† FOUNDATIONAL FORMULA - Bellman Optimality Equation for $v_*$** (_Equations 3.18-3.19_):\n",
    "$$v_*(s) = \\max_a \\mathbb{E}[R_{t+1} + \\gamma v_*(S_{t+1}) | S_t = s, A_t = a]$$\n",
    "$$v_*(s) = \\max_a \\sum_{s',r} p(s', r|s, a)[r + \\gamma v_*(s')]$$\n",
    "\n",
    "**Components**:\n",
    "- $\\max_a$: Choose the best action\n",
    "- Rest is same as Bellman equation, but now we optimize over actions\n",
    "\n",
    "**Mathematical intuition**: The value of a state under optimal policy equals the value of the best action from that state. We replace expectation over policy with maximization over actions.\n",
    "\n",
    "**ðŸ† FOUNDATIONAL FORMULA - Bellman Optimality Equation for $q_*$** (_Equation 3.20_):\n",
    "$$q_*(s, a) = \\sum_{s',r} p(s', r|s, a)[r + \\gamma \\max_{a'} q_*(s', a')]$$\n",
    "\n",
    "**Mathematical intuition**: The value of taking action $a$ in state $s$ equals the expected immediate reward plus the discounted value of the best action in the next state.\n",
    "\n",
    "![Backup Diagrams for Optimal Value Functions](../img/fig3_4.png)\n",
    "\n",
    "### Finding Optimal Policies\n",
    "\n",
    "**Key insight**: Once you have $v_*$, finding optimal policy is easy:\n",
    "\n",
    "**Greedy policy extraction**:\n",
    "$$\\pi_*(s) = \\arg\\max_a \\sum_{s',r} p(s', r|s, a)[r + \\gamma v_*(s')]$$\n",
    "\n",
    "**Why this works**: $v_*$ already accounts for all future consequences, so a greedy one-step lookahead gives the optimal action.\n",
    "\n",
    "**With $q_*$, it's even easier**:\n",
    "$$\\pi_*(s) = \\arg\\max_a q_*(s, a)$$\n",
    "\n",
    "### Solving Bellman Optimality Equations\n",
    "\n",
    "**In principle**: Can solve the system of Bellman optimality equations directly\n",
    "- For $n$ states, have $n$ equations in $n$ unknowns\n",
    "- Requires knowing environment dynamics $p(s', r|s, a)$\n",
    "\n",
    "**In practice**: Usually computationally intractable\n",
    "- Example: Backgammon has ~$10^{20}$ states\n",
    "- Most RL methods approximate the solution\n",
    "\n",
    "---\n",
    "\n",
    "## 3.7 Optimality and Approximation\n",
    "\n",
    "### Computational Reality\n",
    "\n",
    "**The fundamental challenge**: Computing optimal policies exactly is usually impossible due to:\n",
    "1. **Computational limits**: Not enough computation per time step\n",
    "2. **Memory constraints**: Cannot store values for all states  \n",
    "3. **Unknown dynamics**: Don't know $p(s', r|s, a)$\n",
    "\n",
    "### Tabular vs. Function Approximation\n",
    "\n",
    "**Tabular methods**: Store separate value for each state\n",
    "- Feasible only for small state spaces\n",
    "- Can find exact solutions\n",
    "\n",
    "**Function approximation**: Use parameterized functions to approximate values\n",
    "- Necessary for large state spaces\n",
    "- Can only find approximate solutions\n",
    "\n",
    "### Approximation Opportunities\n",
    "\n",
    "**Key insight**: RL allows focusing computational resources on frequently encountered states\n",
    "\n",
    "**Example**: Tesauro's backgammon player\n",
    "- Exceptional performance despite potentially poor decisions on rare board positions\n",
    "- Focuses learning on states that actually occur in expert play\n",
    "\n",
    "---\n",
    "\n",
    "## 3.8 Summary\n",
    "\n",
    "### Key Concepts Introduced\n",
    "\n",
    "**MDPs provide the mathematical foundation for RL**:\n",
    "- **States**: Basis for decision-making\n",
    "- **Actions**: Choices available to agent  \n",
    "- **Rewards**: Basis for evaluation\n",
    "- **Policy**: Rule for selecting actions\n",
    "- **Value functions**: Expected future reward\n",
    "\n",
    "**Value functions are central to RL**:\n",
    "- $v_\\pi(s)$: Expected return from state $s$ under policy $\\pi$\n",
    "- $q_\\pi(s,a)$: Expected return from taking action $a$ in state $s$ under policy $\\pi$\n",
    "- $v_*(s)$: Best possible return from state $s$\n",
    "- $q_*(s,a)$: Best possible return from taking action $a$ in state $s$\n",
    "\n",
    "**Bellman equations provide recursive structure**:\n",
    "- Connect value of state to values of successor states\n",
    "- Foundation for most RL algorithms\n",
    "- Optimality equations characterize optimal behavior\n",
    "\n",
    "### Fundamental Takeaways\n",
    "\n",
    "1. **Actions affect both immediate rewards and future opportunities**\n",
    "2. **Value functions capture long-term consequences of decisions**  \n",
    "3. **Optimal policies are greedy with respect to optimal value functions**\n",
    "4. **Exact solutions usually impossible; approximation necessary**\n",
    "5. **RL can focus learning on frequently encountered states**\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 3.1 Solution\n",
    "**Q**: Devise three example tasks that fit the MDP framework.\n",
    "\n",
    "**A**: \n",
    "1. **Autonomous driving**:\n",
    "   - States: (position, speed, traffic conditions, weather)\n",
    "   - Actions: (accelerate, brake, steer left/right, change lanes)\n",
    "   - Rewards: +1 for progress toward destination, -100 for accidents, -1 for traffic violations\n",
    "\n",
    "2. **Stock trading**:\n",
    "   - States: (portfolio value, market indicators, time of day, news sentiment)\n",
    "   - Actions: (buy stock X, sell stock Y, hold, set stop-loss)\n",
    "   - Rewards: Change in portfolio value each day\n",
    "\n",
    "3. **Chatbot conversation**:\n",
    "   - States: (conversation history, user sentiment, topic, user profile)\n",
    "   - Actions: (different response templates, ask question, provide information, end conversation)\n",
    "   - Rewards: +1 for positive user feedback, -1 for user ending conversation early\n",
    "\n",
    "### Exercise 3.5 Solution\n",
    "**Q**: Modify equation (3.3) for episodic tasks.\n",
    "\n",
    "**A**: For episodic tasks, we need to account for transitions to the terminal state:\n",
    "$$\\sum_{s' \\in \\mathcal{S}^+} \\sum_{r \\in \\mathcal{R}} p(s', r|s, a) = 1$$\n",
    "where $\\mathcal{S}^+$ includes the terminal state.\n",
    "\n",
    "### Exercise 3.8 Solution  \n",
    "**Q**: Given $\\gamma = 0.5$ and rewards $R_1 = -1, R_2 = 2, R_3 = 6, R_4 = 3, R_5 = 2$ with $T = 5$, find $G_0, G_1, ..., G_5$.\n",
    "\n",
    "**A**: Working backwards:\n",
    "- $G_5 = 0$ (terminal)\n",
    "- $G_4 = R_5 = 2$  \n",
    "- $G_3 = R_4 + \\gamma G_4 = 3 + 0.5(2) = 4$\n",
    "- $G_2 = R_3 + \\gamma G_3 = 6 + 0.5(4) = 8$\n",
    "- $G_1 = R_2 + \\gamma G_2 = 2 + 0.5(8) = 6$ \n",
    "- $G_0 = R_1 + \\gamma G_1 = -1 + 0.5(6) = 2$\n",
    "\n",
    "### Exercise 3.12 Solution\n",
    "**Q**: Give equation for $v_\\pi$ in terms of $q_\\pi$ and $\\pi$.\n",
    "\n",
    "**A**: \n",
    "$$v_\\pi(s) = \\sum_a \\pi(a|s) q_\\pi(s, a)$$\n",
    "\n",
    "**Intuition**: State value is weighted average of action values under the policy.\n",
    "\n",
    "### Exercise 3.13 Solution\n",
    "**Q**: Give equation for $q_\\pi$ in terms of $v_\\pi$ and $p$.\n",
    "\n",
    "**A**:\n",
    "$$q_\\pi(s, a) = \\sum_{s',r} p(s', r|s, a)[r + \\gamma v_\\pi(s')]$$\n",
    "\n",
    "**Intuition**: Action value equals expected immediate reward plus discounted next state value.\n",
    "\n",
    "### Exercise 3.17 Solution\n",
    "**Q**: What is the Bellman equation for action values $q_\\pi$?\n",
    "\n",
    "**A**:\n",
    "$$q_\\pi(s, a) = \\sum_{s',r} p(s', r|s, a)\\left[r + \\gamma \\sum_{a'} \\pi(a'|s') q_\\pi(s', a')\\right]$$\n",
    "\n",
    "**Components**:\n",
    "- Expected immediate reward: $r$\n",
    "- Expected future value: $\\gamma \\sum_{a'} \\pi(a'|s') q_\\pi(s', a')$\n",
    "\n",
    "### Exercise 3.25 Solution\n",
    "**Q**: Give equation for $v_*$ in terms of $q_*$.\n",
    "\n",
    "**A**:\n",
    "$$v_*(s) = \\max_a q_*(s, a)$$\n",
    "\n",
    "### Exercise 3.27 Solution  \n",
    "**Q**: Give equation for $\\pi_*$ in terms of $q_*$.\n",
    "\n",
    "**A**:\n",
    "$$\\pi_*(a|s) = \\begin{cases} \n",
    "1 & \\text{if } a \\in \\arg\\max_{a'} q_*(s, a') \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Intuition**: Optimal policy puts all probability on action(s) with highest $q_*$ value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66df9a5",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cf86eb",
   "metadata": {},
   "source": [
    "# Appendix A: Full Notation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c0755",
   "metadata": {},
   "source": [
    "### General Notation\n",
    "\n",
    "> Capital letters are used for random variables.\n",
    "\n",
    "> Lower case letters are used for the values of random variables and for scalar functions.\n",
    "\n",
    "> Quantities that are required to be real-valued vectors are written in bold and in lower case (even if random variables). \n",
    "\n",
    "> Matrices are bold capitals.\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **Standard Operators** |  |\n",
    "| $\\doteq$ | Equality relationship that is true by definition |\n",
    "| $\\approx$ | Approximately equal |\n",
    "| $\\propto$ | Proportional to |\n",
    "| $\\Pr\\{X = x\\}$ | Probability that random variable $X$ takes value $x$ |\n",
    "| $X \\sim p$ | $X$ drawn from distribution $p$ |\n",
    "| $\\mathbb{E}[X]$ | Expectation of random variable $X$ |\n",
    "| $\\arg\\max_a f(a)$ | Value of $a$ that maximizes $f(a)$ |\n",
    "| $\\ln x$ | Natural logarithm of $x$ |\n",
    "| $e^x$, $\\exp(x)$ | The base of the natural logarithm, $e \\approx 2.71828$, carried to power $x$ |\n",
    "| $\\mathbb{R}$ | Set of real numbers |\n",
    "| $f: \\mathcal{X} \\to \\mathcal{Y}$ | Function from elements of set $\\mathcal{X}$ to elements of set $\\mathcal{Y}$ |\n",
    "| $\\leftarrow$ | Assignment |\n",
    "| $(a, b]$ | Real interval between $a$ and $b$ including $b$ but not $a$ |\n",
    "| **Standard RL Algorithm Parameters** |  |\n",
    "| $\\epsilon$ | Probability of taking a random action in an $\\epsilon$-greedy policy |\n",
    "| $\\alpha, \\beta$ | Step-size parameters |\n",
    "| $\\gamma$ | Discount-rate parameter |\n",
    "| $\\lambda$ | Decay-rate parameter for eligibility traces |\n",
    "| $\\mathbf{1}_\\text{predicate}$ | Indicator function (1 if predicate is true, else 0) |\n",
    "\n",
    "---\n",
    "\n",
    "### Multi-Armed Bandit Notation\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| $k$ | Number of actions (arms) |\n",
    "| $t$ | Discrete time step or play number |\n",
    "| $q_*(a)$ | True value (expected reward) of action $a$ |\n",
    "| $Q_t(a)$ | Estimate at time $t$ of $q_*(a)$ |\n",
    "| $N_t(a)$ | Number of times action $a$ has been selected up to time $t$ |\n",
    "| $H_t(a)$ | Learned preference for selecting action $a$ at time $t$ |\n",
    "| $\\pi_t(a)$ | Probability of selecting action $a$ at time $t$ |\n",
    "| $\\bar{R}_t$ | Estimate at time $t$ of the expected reward given $\\pi_t$ |\n",
    "\n",
    "---\n",
    "\n",
    "### Markov Decision Process Notation\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **State & Action Sets** |  |\n",
    "| $s, s'$ | States |\n",
    "| $a$ | An action |\n",
    "| $r$ | A reward |\n",
    "| $\\mathcal{S}$ | Set of all nonterminal states |\n",
    "| $\\mathcal{S}^+$ | Set of all states, incl. terminal state |\n",
    "| $\\mathcal{A}(s)$ | Set of all actions available in state $s$ |\n",
    "| $\\mathcal{R}$ | Set of all possible rewards, a finite subset of $\\mathbb{R}$ |\n",
    "| $\\mathcal{C}$ | Subset of (e.g., $\\mathcal{R} \\subset \\mathbb{R}$) |\n",
    "| $\\in$ | Is an element of (e.g. $s \\in \\mathcal{S}$, $r \\in \\mathcal{R}$) |\n",
    "| $\\lvert\\mathcal{S}\\rvert$ | Number of elements in set $\\mathcal{S}$ |\n",
    "| **Time & Policy** |  |\n",
    "| $t$ | Discrete time step |\n",
    "| $T, T(t)$ | Final time step of episode, or including $t$ |\n",
    "| $A_t$ | Action at time $t$ |\n",
    "| $S_t$ | State at time $t$ |\n",
    "| $R_t$ | Reward at time $t$ |\n",
    "| $\\pi$ | Policy (decision-making rule) |\n",
    "| $\\pi(s)$ | Action taken in state $s$ under deterministic $\\pi$ |\n",
    "| $\\pi(a \\mid s)$ | Probability of taking action $a$ in state $s$ under stochastic $\\pi$ |\n",
    "| **Returns** |  |\n",
    "| $G_t$ | Return following time $t$ |\n",
    "| $h$ | Horizon (timestep looked up to in forward view) |\n",
    "| $G_{t:t+n}, G_{t:h}$ | $n$-step return from $t+1$ to $t+n$ or $h$ (discounted/corrected) |\n",
    "| $G_{t:h}$ | Flat return (undiscounted/unadjusted) from $t+1$ to $h$ |\n",
    "| $G^\\lambda_t$ | $\\lambda$-return |\n",
    "| $G^{\\wedge}_t$, $G^{\\wedge a}_t$ | Truncated, corrected $\\lambda$-return |\n",
    "| **Transition & Reward Probabilities** |  |\n",
    "| $p(s', r \\mid s, a)$ | Probability of transition to $s'$ with reward $r$ from $s, a$ |\n",
    "| $p(s' \\mid s, a)$ | Probability of transition to $s'$ from $s$ taking $a$ |\n",
    "| $r(s, a)$ | Expected immediate reward from $s$ after $a$ |\n",
    "| $r(s, a, s')$ | Expected reward on transition $s \\to s'$ under $a$ |\n",
    "| **Value Functions** |  |\n",
    "| $v_\\pi(s)$ | Value of $s$ under policy $\\pi$ (expected return) |\n",
    "| $v_*(s)$ | Value of $s$ under optimal policy |\n",
    "| $q_\\pi(s, a)$ | Value of taking $a$ in $s$ under $\\pi$ |\n",
    "| $q_*(s, a)$ | Value of taking $a$ in $s$ under optimal policy |\n",
    "| **Estimators & TD Error** |  |\n",
    "| $V, V_t$ | Array estimates of $v_\\pi$ or $v_*$ |\n",
    "| $Q, Q_t$ | Array estimates of $q_\\pi$ or $q_*$ |\n",
    "| $\\hat{V}(s)$ | Expected approximate action value; e.g. $\\hat{V}_t(s) \\doteq \\sum_a \\pi(a|s) Q_t(s, a)$ |\n",
    "| $U_t$ | Target for estimate at time $t$ |\n",
    "| $\\delta_t$ | Temporal-difference (TD) error at $t$ (a random variable) |\n",
    "| $\\delta_t^s$, $\\delta_t^a$ | State- and action-specific forms of TD error |\n",
    "| $n$ | In $n$-step methods, $n$ is number of steps of bootstrapping |\n",
    "---\n",
    "\n",
    "### Function Approximation, Policy Gradient, Advanced Notation\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **Weight & Parameter Notation** |  |\n",
    "| $d$ | Dimensionalityâ€”number of components of $\\mathbf{w}$ |\n",
    "| $d'$ | Alternate dimensionalityâ€”number of components of $\\theta$ |\n",
    "| $\\mathbf{w}, \\mathbf{w}_t$ | $d$-vector of weights underlying approximate value function |\n",
    "| $w_i, w_{t,i}$ | $i$-th component of learnable weight vector |\n",
    "| $\\hat{v}(s, \\mathbf{w})$ | Approximate value of $s$ given $\\mathbf{w}$ |\n",
    "| $v_\\mathbf{w}(s)$ | Alternate notation for $\\hat{v}(s, \\mathbf{w})$ |\n",
    "| $\\hat{q}(s, a, \\mathbf{w})$ | Approximate value of $(s, a)$ given $\\mathbf{w}$ |\n",
    "| $\\nabla \\hat{v}(s, \\mathbf{w})$ | Column vector of partial derivatives w.r.t. $\\mathbf{w}$ |\n",
    "| $\\nabla \\hat{q}(s, a, \\mathbf{w})$ | Column vector of partial derivatives w.r.t. $\\mathbf{w}$ |\n",
    "| **Feature Vectors & Eligibility Traces** |  |\n",
    "| $\\mathbf{x}(s)$ | Feature vector visible in $s$ |\n",
    "| $\\mathbf{x}(s, a)$ | Feature vector visible in $s$ taking $a$ |\n",
    "| $x_i(s), x_i(s, a)$ | $i$-th component of $\\mathbf{x}(s)$ or $\\mathbf{x}(s, a)$ |\n",
    "| $\\mathbf{x}_t$ | Shorthand for $\\mathbf{x}(S_t)$ or $\\mathbf{x}(S_t, A_t)$ |\n",
    "| $\\mathbf{w}^\\top \\mathbf{x}$ | Inner product of vectors |\n",
    "| $\\mathbf{v}, \\mathbf{v}_t$ | Secondary $d$-vector of weights, used to learn $\\mathbf{w}$ |\n",
    "| $\\mathbf{z}_t$ | $d$-vector of eligibility traces at $t$ |\n",
    "| **Policy Gradient Notation** |  |\n",
    "| $\\theta, \\theta_t$ | Parameter vector of target policy |\n",
    "| $\\pi(a \\mid s, \\theta)$ | Probability of taking $a$ in $s$ given $\\theta$ |\n",
    "| $\\pi_\\theta$ | Policy corresponding to parameter $\\theta$ |\n",
    "| $\\nabla \\pi(a \\mid s, \\theta)$ | Partial derivatives of $\\pi(a \\mid s, \\theta)$ w.r.t. $\\theta$ |\n",
    "| $J(\\theta)$ | Performance measure for policy $\\pi_\\theta$ |\n",
    "| $\\nabla J(\\theta)$ | Partial derivatives of $J(\\theta)$ w.r.t. $\\theta$ |\n",
    "| $h(s, a, \\theta)$ | Preference for $a$ in $s$ based on $\\theta$ |\n",
    "| **Behavior Policy, Baselines, Importance Sampling** |  |\n",
    "| $b(a \\mid s)$ | Behavior policy used to select actions while learning target $\\pi$ |\n",
    "| $b(s)$ | Baseline function $b: \\mathcal{S} \\to \\mathbb{R}$ for policy-gradient methods |\n",
    "| $b$ | Branching factor for MDP/search tree |\n",
    "| $\\rho_{t:h}$ | Importance sampling ratio for $t$ through $h$ |\n",
    "| $\\rho_t$ | Importance sampling ratio for time $t$ alone, $\\rho_t \\doteq \\rho_{t:t}$ |\n",
    "| $r(\\pi)$ | Average reward (reward rate) for policy $\\pi$ |\n",
    "| $\\bar{R}_t$ | Estimate of $r(\\pi)$ at time $t$ |\n",
    "| **State Distributions & Operators** |  |\n",
    "| $\\mu(s)$ | On-policy distribution over states |\n",
    "| $\\mu$ | $\\lvert\\mathcal{S}\\rvert$-vector of the $\\mu(s)$ for $s \\in \\mathcal{S}$ |\n",
    "| $\\|v\\|^2_\\mu$ | $\\mu$-weighted squared norm of $v$, i.e., $\\|v\\|^2_\\mu \\doteq \\sum_{s \\in \\mathcal{S}} \\mu(s)v(s)^2$ |\n",
    "| $\\eta(s)$ | Expected number of visits to $s$ per episode |\n",
    "| $\\Pi$ | Projection operator for value functions |\n",
    "| $B_\\pi$ | Bellman operator for value functions |\n",
    "\n",
    "---\n",
    "\n",
    "### Matrices, Bellman Error, & Error Metrics\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **Matrix Notation & Linear Algebra** |  |\n",
    "| $\\mathbf{A}$ | $d \\times d$ matrix: $\\mathbf{A} \\doteq \\mathbb{E}\\left[ \\mathbf{x}_t(\\mathbf{x}_t - \\gamma \\mathbf{x}_{t+1})^\\top \\right]$ |\n",
    "| $\\mathbf{b}$ | $d$-dimensional vector: $\\mathbf{b} \\doteq \\mathbb{E}[R_{t+1} \\mathbf{x}_t]$ |\n",
    "| $\\mathbf{w}_{TD}$ | TD fixed point: $\\mathbf{w}_{TD} \\doteq \\mathbf{A}^{-1}\\mathbf{b}$ |\n",
    "| $\\mathbf{I}$ | Identity matrix |\n",
    "| $\\mathbf{P}$ | $\\lvert\\mathcal{S}\\rvert \\times \\lvert\\mathcal{S}\\rvert$ matrix of state-transition probabilities under $\\pi$ |\n",
    "| $\\mathbf{D}$ | $\\lvert\\mathcal{S}\\rvert \\times \\lvert\\mathcal{S}\\rvert$ diagonal matrix with $\\mu$ on its diagonal |\n",
    "| $\\mathbf{X}$ | $\\lvert\\mathcal{S}\\rvert \\times d$ matrix with the $\\mathbf{x}(s)$ as its rows |\n",
    "| **Bellman Error & Value Error Metrics** |  |\n",
    "| $\\bar{\\delta}_\\mathbf{w}(s)$ | Bellman error (expected TD error) for $v_\\mathbf{w}$ at $s$ |\n",
    "| $\\bar{\\delta}_\\mathbf{w}$, BE | Bellman error vector (with components $\\bar{\\delta}_\\mathbf{w}(s)$) |\n",
    "| $\\text{VE}(\\mathbf{w})$ | Mean square value error: $\\text{VE}(\\mathbf{w}) \\doteq \\|v_\\mathbf{w} - v_\\pi\\|^2_\\mu$ |\n",
    "| $\\text{BE}(\\mathbf{w})$ | Mean square Bellman error: $\\text{BE}(\\mathbf{w}) \\doteq \\|\\bar{\\delta}_\\mathbf{w}\\|^2_\\mu$ |\n",
    "| $\\text{PBE}(\\mathbf{w})$ | Mean square projected Bellman error: $\\text{PBE}(\\mathbf{w}) \\doteq \\|\\Pi \\bar{\\delta}_\\mathbf{w}\\|^2_\\mu$ |\n",
    "| $\\text{TDE}(\\mathbf{w})$ | Mean square temporal-difference error: $\\text{TDE}(\\mathbf{w}) \\doteq \\mathbb{E}_b[\\rho_t \\delta_t^2]$ |\n",
    "| $\\text{RE}(\\mathbf{w})$ | Mean square return error |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
