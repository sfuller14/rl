{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe1fd84b",
   "metadata": {},
   "source": [
    "# Summary of Notation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e97302",
   "metadata": {},
   "source": [
    "(full notation in Appendix A)\n",
    "\n",
    "> Whenever you see the word \"value\", think \"Expected Present Value of Future ~~Cash Flows~~ Rewards\" $\\mathbb{E}[PV(r^{\\pi}_{t+})]$.  \n",
    "> \"Value\" is your best estimate of the cumulative discounted future rewards you'll receive from a given state or action. \n",
    ">\n",
    "> Financial analysts estimate the NPV of an investment.  \n",
    "> RL agents estimate the \"reward NPV\" of being in different states or taking different actions.  \n",
    "> Value is a forward-looking aggregation of all future consequences (not just immediate payoffs).\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| $\\Pr\\{X = x\\}$ | Probability that random variable $X$ takes value $x$ |\n",
    "| $X \\sim p$ | $X$ is drawn from distribution $p$ |\n",
    "| $\\mathbb{E}[X]$ | Expectation of random variable $X$ |\n",
    "| $\\arg\\max_a f(a)$ | Value of $a$ that maximizes $f(a)$ |\n",
    "| $\\alpha$, $\\beta$, $\\epsilon$ | Step-size, decay-rate, and exploration parameters |\n",
    "| $s, s'$ | States |\n",
    "| $a$ | Action |\n",
    "| $r$ | Reward |\n",
    "| $S, A(s), R$ | Set of states, available actions, rewards |\n",
    "| $t, T$ | Discrete time step, final step |\n",
    "| $S_t, A_t, R_t$ | State, action, and reward at time $t$ |\n",
    "| $\\pi$ | Policy (decision-making rule) |\n",
    "| $\\pi(s)$ | action taken in state $s$ under _deterministic_ $\\pi$ |\n",
    "| $\\pi(a \\mid s)$ | Probability of taking action $a$ in state $s$ under _stochastic_ $\\pi$ |\n",
    "| $G_t$ | Return from time $t$ |\n",
    "| $h$ | horizon (the timestep one looks up to in a forward view) |\n",
    "| $v_\\pi(s)$ | Value of state $s$ under policy $\\pi$ |\n",
    "| $q_\\pi(s, a)$ | Value of state-action pair $(s,a)$ under $\\pi$ |\n",
    "| $p(s', r \\mid s, a)$ | Transition dynamics |\n",
    "| $\\hat{v}(s, \\mathbf{w})$ | Approximate value function using weight vector $\\mathbf{w}$ |\n",
    "| $\\delta_t$ | Temporal-difference error at time $t$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a1da39",
   "metadata": {},
   "source": [
    "# Appendix A: Full Notation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1028f6a",
   "metadata": {},
   "source": [
    "### General Notation\n",
    "\n",
    "> Capital letters are used for random variables.\n",
    "\n",
    "> Lower case letters are used for the values of random variables and for scalar functions.\n",
    "\n",
    "> Quantities that are required to be real-valued vectors are written in bold and in lower case (even if random variables). \n",
    "\n",
    "> Matrices are bold capitals.\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **Standard Operators** |  |\n",
    "| $\\doteq$ | Equality relationship that is true by definition |\n",
    "| $\\approx$ | Approximately equal |\n",
    "| $\\propto$ | Proportional to |\n",
    "| $\\Pr\\{X = x\\}$ | Probability that random variable $X$ takes value $x$ |\n",
    "| $X \\sim p$ | $X$ drawn from distribution $p$ |\n",
    "| $\\mathbb{E}[X]$ | Expectation of random variable $X$ |\n",
    "| $\\arg\\max_a f(a)$ | Value of $a$ that maximizes $f(a)$ |\n",
    "| $\\ln x$ | Natural logarithm of $x$ |\n",
    "| $e^x$, $\\exp(x)$ | The base of the natural logarithm, $e \\approx 2.71828$, carried to power $x$ |\n",
    "| $\\mathbb{R}$ | Set of real numbers |\n",
    "| $f: \\mathcal{X} \\to \\mathcal{Y}$ | Function from elements of set $\\mathcal{X}$ to elements of set $\\mathcal{Y}$ |\n",
    "| $\\leftarrow$ | Assignment |\n",
    "| $(a, b]$ | Real interval between $a$ and $b$ including $b$ but not $a$ |\n",
    "| **Standard RL Algorithm Parameters** |  |\n",
    "| $\\epsilon$ | Probability of taking a random action in an $\\epsilon$-greedy policy |\n",
    "| $\\alpha, \\beta$ | Step-size parameters |\n",
    "| $\\gamma$ | Discount-rate parameter |\n",
    "| $\\lambda$ | Decay-rate parameter for eligibility traces |\n",
    "| $\\mathbf{1}_\\text{predicate}$ | Indicator function (1 if predicate is true, else 0) |\n",
    "\n",
    "---\n",
    "\n",
    "### Multi-Armed Bandit Notation\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| $k$ | Number of actions (arms) |\n",
    "| $t$ | Discrete time step or play number |\n",
    "| $q_*(a)$ | True value (expected reward) of action $a$ |\n",
    "| $Q_t(a)$ | Estimate at time $t$ of $q_*(a)$ |\n",
    "| $N_t(a)$ | Number of times action $a$ has been selected up to time $t$ |\n",
    "| $H_t(a)$ | Learned preference for selecting action $a$ at time $t$ |\n",
    "| $\\pi_t(a)$ | Probability of selecting action $a$ at time $t$ |\n",
    "| $\\bar{R}_t$ | Estimate at time $t$ of the expected reward given $\\pi_t$ |\n",
    "\n",
    "---\n",
    "\n",
    "### Markov Decision Process Notation\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **State & Action Sets** |  |\n",
    "| $s, s'$ | States |\n",
    "| $a$ | An action |\n",
    "| $r$ | A reward |\n",
    "| $\\mathcal{S}$ | Set of all nonterminal states |\n",
    "| $\\mathcal{S}^+$ | Set of all states, incl. terminal state |\n",
    "| $\\mathcal{A}(s)$ | Set of all actions available in state $s$ |\n",
    "| $\\mathcal{R}$ | Set of all possible rewards, a finite subset of $\\mathbb{R}$ |\n",
    "| $\\mathcal{C}$ | Subset of (e.g., $\\mathcal{R} \\subset \\mathbb{R}$) |\n",
    "| $\\in$ | Is an element of (e.g. $s \\in \\mathcal{S}$, $r \\in \\mathcal{R}$) |\n",
    "| $\\lvert\\mathcal{S}\\rvert$ | Number of elements in set $\\mathcal{S}$ |\n",
    "| **Time & Policy** |  |\n",
    "| $t$ | Discrete time step |\n",
    "| $T, T(t)$ | Final time step of episode, or including $t$ |\n",
    "| $A_t$ | Action at time $t$ |\n",
    "| $S_t$ | State at time $t$ |\n",
    "| $R_t$ | Reward at time $t$ |\n",
    "| $\\pi$ | Policy (decision-making rule) |\n",
    "| $\\pi(s)$ | Action taken in state $s$ under deterministic $\\pi$ |\n",
    "| $\\pi(a \\mid s)$ | Probability of taking action $a$ in state $s$ under stochastic $\\pi$ |\n",
    "| **Returns** |  |\n",
    "| $G_t$ | Return following time $t$ |\n",
    "| $h$ | Horizon (timestep looked up to in forward view) |\n",
    "| $G_{t:t+n}, G_{t:h}$ | $n$-step return from $t+1$ to $t+n$ or $h$ (discounted/corrected) |\n",
    "| $G_{t:h}$ | Flat return (undiscounted/unadjusted) from $t+1$ to $h$ |\n",
    "| $G^\\lambda_t$ | $\\lambda$-return |\n",
    "| $G^{\\wedge}_t$, $G^{\\wedge a}_t$ | Truncated, corrected $\\lambda$-return |\n",
    "| **Transition & Reward Probabilities** |  |\n",
    "| $p(s', r \\mid s, a)$ | Probability of transition to $s'$ with reward $r$ from $s, a$ |\n",
    "| $p(s' \\mid s, a)$ | Probability of transition to $s'$ from $s$ taking $a$ |\n",
    "| $r(s, a)$ | Expected immediate reward from $s$ after $a$ |\n",
    "| $r(s, a, s')$ | Expected reward on transition $s \\to s'$ under $a$ |\n",
    "| **Value Functions** |  |\n",
    "| $v_\\pi(s)$ | Value of $s$ under policy $\\pi$ (expected return) |\n",
    "| $v_*(s)$ | Value of $s$ under optimal policy |\n",
    "| $q_\\pi(s, a)$ | Value of taking $a$ in $s$ under $\\pi$ |\n",
    "| $q_*(s, a)$ | Value of taking $a$ in $s$ under optimal policy |\n",
    "| **Estimators & TD Error** |  |\n",
    "| $V, V_t$ | Array estimates of $v_\\pi$ or $v_*$ |\n",
    "| $Q, Q_t$ | Array estimates of $q_\\pi$ or $q_*$ |\n",
    "| $\\hat{V}(s)$ | Expected approximate action value; e.g. $\\hat{V}_t(s) \\doteq \\sum_a \\pi(a|s) Q_t(s, a)$ |\n",
    "| $U_t$ | Target for estimate at time $t$ |\n",
    "| $\\delta_t$ | Temporal-difference (TD) error at $t$ (a random variable) |\n",
    "| $\\delta_t^s$, $\\delta_t^a$ | State- and action-specific forms of TD error |\n",
    "| $n$ | In $n$-step methods, $n$ is number of steps of bootstrapping |\n",
    "---\n",
    "\n",
    "### Function Approximation, Policy Gradient, Advanced Notation\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **Weight & Parameter Notation** |  |\n",
    "| $d$ | Dimensionality—number of components of $\\mathbf{w}$ |\n",
    "| $d'$ | Alternate dimensionality—number of components of $\\theta$ |\n",
    "| $\\mathbf{w}, \\mathbf{w}_t$ | $d$-vector of weights underlying approximate value function |\n",
    "| $w_i, w_{t,i}$ | $i$-th component of learnable weight vector |\n",
    "| $\\hat{v}(s, \\mathbf{w})$ | Approximate value of $s$ given $\\mathbf{w}$ |\n",
    "| $v_\\mathbf{w}(s)$ | Alternate notation for $\\hat{v}(s, \\mathbf{w})$ |\n",
    "| $\\hat{q}(s, a, \\mathbf{w})$ | Approximate value of $(s, a)$ given $\\mathbf{w}$ |\n",
    "| $\\nabla \\hat{v}(s, \\mathbf{w})$ | Column vector of partial derivatives w.r.t. $\\mathbf{w}$ |\n",
    "| $\\nabla \\hat{q}(s, a, \\mathbf{w})$ | Column vector of partial derivatives w.r.t. $\\mathbf{w}$ |\n",
    "| **Feature Vectors & Eligibility Traces** |  |\n",
    "| $\\mathbf{x}(s)$ | Feature vector visible in $s$ |\n",
    "| $\\mathbf{x}(s, a)$ | Feature vector visible in $s$ taking $a$ |\n",
    "| $x_i(s), x_i(s, a)$ | $i$-th component of $\\mathbf{x}(s)$ or $\\mathbf{x}(s, a)$ |\n",
    "| $\\mathbf{x}_t$ | Shorthand for $\\mathbf{x}(S_t)$ or $\\mathbf{x}(S_t, A_t)$ |\n",
    "| $\\mathbf{w}^\\top \\mathbf{x}$ | Inner product of vectors |\n",
    "| $\\mathbf{v}, \\mathbf{v}_t$ | Secondary $d$-vector of weights, used to learn $\\mathbf{w}$ |\n",
    "| $\\mathbf{z}_t$ | $d$-vector of eligibility traces at $t$ |\n",
    "| **Policy Gradient Notation** |  |\n",
    "| $\\theta, \\theta_t$ | Parameter vector of target policy |\n",
    "| $\\pi(a \\mid s, \\theta)$ | Probability of taking $a$ in $s$ given $\\theta$ |\n",
    "| $\\pi_\\theta$ | Policy corresponding to parameter $\\theta$ |\n",
    "| $\\nabla \\pi(a \\mid s, \\theta)$ | Partial derivatives of $\\pi(a \\mid s, \\theta)$ w.r.t. $\\theta$ |\n",
    "| $J(\\theta)$ | Performance measure for policy $\\pi_\\theta$ |\n",
    "| $\\nabla J(\\theta)$ | Partial derivatives of $J(\\theta)$ w.r.t. $\\theta$ |\n",
    "| $h(s, a, \\theta)$ | Preference for $a$ in $s$ based on $\\theta$ |\n",
    "| **Behavior Policy, Baselines, Importance Sampling** |  |\n",
    "| $b(a \\mid s)$ | Behavior policy used to select actions while learning target $\\pi$ |\n",
    "| $b(s)$ | Baseline function $b: \\mathcal{S} \\to \\mathbb{R}$ for policy-gradient methods |\n",
    "| $b$ | Branching factor for MDP/search tree |\n",
    "| $\\rho_{t:h}$ | Importance sampling ratio for $t$ through $h$ |\n",
    "| $\\rho_t$ | Importance sampling ratio for time $t$ alone, $\\rho_t \\doteq \\rho_{t:t}$ |\n",
    "| $r(\\pi)$ | Average reward (reward rate) for policy $\\pi$ |\n",
    "| $\\bar{R}_t$ | Estimate of $r(\\pi)$ at time $t$ |\n",
    "| **State Distributions & Operators** |  |\n",
    "| $\\mu(s)$ | On-policy distribution over states |\n",
    "| $\\mu$ | $\\lvert\\mathcal{S}\\rvert$-vector of the $\\mu(s)$ for $s \\in \\mathcal{S}$ |\n",
    "| $\\|v\\|^2_\\mu$ | $\\mu$-weighted squared norm of $v$, i.e., $\\|v\\|^2_\\mu \\doteq \\sum_{s \\in \\mathcal{S}} \\mu(s)v(s)^2$ |\n",
    "| $\\eta(s)$ | Expected number of visits to $s$ per episode |\n",
    "| $\\Pi$ | Projection operator for value functions |\n",
    "| $B_\\pi$ | Bellman operator for value functions |\n",
    "\n",
    "---\n",
    "\n",
    "### Matrices, Bellman Error, & Error Metrics\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| **Matrix Notation & Linear Algebra** |  |\n",
    "| $\\mathbf{A}$ | $d \\times d$ matrix: $\\mathbf{A} \\doteq \\mathbb{E}\\left[ \\mathbf{x}_t(\\mathbf{x}_t - \\gamma \\mathbf{x}_{t+1})^\\top \\right]$ |\n",
    "| $\\mathbf{b}$ | $d$-dimensional vector: $\\mathbf{b} \\doteq \\mathbb{E}[R_{t+1} \\mathbf{x}_t]$ |\n",
    "| $\\mathbf{w}_{TD}$ | TD fixed point: $\\mathbf{w}_{TD} \\doteq \\mathbf{A}^{-1}\\mathbf{b}$ |\n",
    "| $\\mathbf{I}$ | Identity matrix |\n",
    "| $\\mathbf{P}$ | $\\lvert\\mathcal{S}\\rvert \\times \\lvert\\mathcal{S}\\rvert$ matrix of state-transition probabilities under $\\pi$ |\n",
    "| $\\mathbf{D}$ | $\\lvert\\mathcal{S}\\rvert \\times \\lvert\\mathcal{S}\\rvert$ diagonal matrix with $\\mu$ on its diagonal |\n",
    "| $\\mathbf{X}$ | $\\lvert\\mathcal{S}\\rvert \\times d$ matrix with the $\\mathbf{x}(s)$ as its rows |\n",
    "| **Bellman Error & Value Error Metrics** |  |\n",
    "| $\\bar{\\delta}_\\mathbf{w}(s)$ | Bellman error (expected TD error) for $v_\\mathbf{w}$ at $s$ |\n",
    "| $\\bar{\\delta}_\\mathbf{w}$, BE | Bellman error vector (with components $\\bar{\\delta}_\\mathbf{w}(s)$) |\n",
    "| $\\text{VE}(\\mathbf{w})$ | Mean square value error: $\\text{VE}(\\mathbf{w}) \\doteq \\|v_\\mathbf{w} - v_\\pi\\|^2_\\mu$ |\n",
    "| $\\text{BE}(\\mathbf{w})$ | Mean square Bellman error: $\\text{BE}(\\mathbf{w}) \\doteq \\|\\bar{\\delta}_\\mathbf{w}\\|^2_\\mu$ |\n",
    "| $\\text{PBE}(\\mathbf{w})$ | Mean square projected Bellman error: $\\text{PBE}(\\mathbf{w}) \\doteq \\|\\Pi \\bar{\\delta}_\\mathbf{w}\\|^2_\\mu$ |\n",
    "| $\\text{TDE}(\\mathbf{w})$ | Mean square temporal-difference error: $\\text{TDE}(\\mathbf{w}) \\doteq \\mathbb{E}_b[\\rho_t \\delta_t^2]$ |\n",
    "| $\\text{RE}(\\mathbf{w})$ | Mean square return error |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
