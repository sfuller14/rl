{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1841a00e",
   "metadata": {},
   "source": [
    "## Summary of Notation\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| $\\Pr\\{X = x\\}$ | Probability that random variable $X$ takes value $x$ |\n",
    "| $X \\sim p$ | $X$ is drawn from distribution $p$ |\n",
    "| $\\mathbb{E}[X]$ | Expectation of random variable $X$ |\n",
    "| $\\arg\\max_a f(a)$ | Value of $a$ that maximizes $f(a)$ |\n",
    "| $\\alpha$, $\\beta$, $\\epsilon$ | Step-size, decay-rate, and exploration parameters |\n",
    "| $s, s'$ | States |\n",
    "| $a$ | Action |\n",
    "| $r$ | Reward |\n",
    "| $S, A(s), R$ | Set of states, available actions, rewards |\n",
    "| $t, T$ | Discrete time step, final step |\n",
    "| $S_t, A_t, R_t$ | State, action, and reward at time $t$ |\n",
    "| $\\pi$ | Policy (decision-making rule) |\n",
    "| $\\pi(s)$ | action taken in state $s$ under _deterministic_ $\\pi$ |\n",
    "| $\\pi(a \\mid s)$ | Probability of taking action $a$ in state $s$ under _stochastic_ $\\pi$ |\n",
    "| $G_t$ | Return from time $t$ |\n",
    "| $h$ | horizon (the timestep one looks up to in a forward view) |\n",
    "| $v_\\pi(s)$ | Value of state $s$ under policy $\\pi$ |\n",
    "| $q_\\pi(s, a)$ | Value of state-action pair $(s,a)$ under $\\pi$ |\n",
    "| $p(s', r \\mid s, a)$ | Transition dynamics |\n",
    "| $\\hat{v}(s, \\mathbf{w})$ | Approximate value function using weight vector $\\mathbf{w}$ |\n",
    "| $\\delta_t$ | Temporal-difference error at time $t$ |\n",
    "\n",
    "---\n",
    "\n",
    "# Chapter 1: Introduction\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 Reinforcement Learning\n",
    "\n",
    "- **Definition**: Learning how to map situations to actions to maximize cumulative reward.\n",
    "- **Core elements**:\n",
    "  - **Trial-and-error search**: The agent explores to learn.\n",
    "  - **Delayed reward**: Actions impact long-term outcomes, not just immediate feedback.\n",
    "- **Distinguishing features**:\n",
    "  - Not told *what* actions to take, only gets feedback via rewards.\n",
    "  - Must balance **exploration vs. exploitation**.\n",
    "- **Compared to other paradigms**:\n",
    "  - **Supervised learning**: Learns from labeled examples.\n",
    "  - **Unsupervised learning**: Learns hidden structure from unlabeled data.\n",
    "  - **RL**: Maximizes rewards via interaction; often needs to explore unknown state spaces.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 Examples\n",
    "\n",
    "Real-world analogies include:\n",
    "- **Chess**: Intuition + planning, learning from position values.\n",
    "- **Adaptive control**: Tuning refinery settings in real time.\n",
    "- **Animals**: Gazelle calf learning to run soon after birth.\n",
    "- **Robots**: Battery-aware navigation decisions.\n",
    "- **Daily tasks**: Preparing breakfast involves conditional behavior, goals, and sensory feedback.\n",
    "\n",
    "**Key Takeaway**: RL applies broadly wherever an agent interacts with an environment, learns over time, and must adapt to uncertainty.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.3 Elements of Reinforcement Learning\n",
    "\n",
    "Four main components:\n",
    "\n",
    "1. **Policy ($\\pi$)**:\n",
    "   - Mapping from states to actions: $\\pi(s)$ or $\\pi(a|s)$.\n",
    "   - Can be deterministic or stochastic.\n",
    "\n",
    "2. **Reward Signal ($r$)**:\n",
    "   - Defines the goal.\n",
    "   - Immediate signal from the environment, tells what’s good or bad.\n",
    "\n",
    "3. **Value Function ($v(s)$, $q(s,a)$)**:\n",
    "   - Estimates long-term reward.\n",
    "   - **$v_\\pi(s)$**: Expected return from state $s$ under policy $\\pi$.\n",
    "   - Guides decision-making more than immediate rewards.\n",
    "\n",
    "4. **Model (optional)**:\n",
    "   - Simulates environment behavior: $p(s', r \\mid s, a)$.\n",
    "   - Enables **planning** (model-based) vs. **direct interaction** (model-free).\n",
    "\n",
    "---\n",
    "\n",
    "## 1.4 Limitations and Scope\n",
    "\n",
    "- Assumes a **state signal** is given (e.g., as preprocessed input).\n",
    "- Reinforcement learning does not inherently solve **state representation**.\n",
    "- Evolutionary methods can solve RL problems without value functions, but often less efficient.\n",
    "- Focus of this book: **model-free and model-based RL** with value estimation.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.5 An Extended Example: Tic-Tac-Toe\n",
    "\n",
    "Goal: Learn to beat an imperfect opponent without knowing its strategy a priori.\n",
    "\n",
    "> About the best one can do on this problem is first to learn a model of the opponent’s behavior, up to some level of confidence, and then apply dynamic programming to compute an optimal solution given the approximate opponent model. In the end, this is not that different from some of the reinforcement learning methods we examine later in this book.\n",
    "\n",
    "**Approach _using a value function_**:\n",
    "- Maintain a value table $V(s)$ for each board state. Each \"cell\" in the value table is a board state. Each cell's value is the latest estimate of the probability of winning from that state (that state's _value_).\n",
    "   - The whole table is the learned _value function_.\n",
    "   - Initialize the table with:\n",
    "      - 1s for all states that have 3 Xs in a row.\n",
    "      - 0s for all states that have 3 Os in a row.\n",
    "      - 0.5 for all other states.\n",
    "- Update values using:\n",
    "  $$\n",
    "  V(S_t) \\leftarrow V(S_t) + \\alpha \\left[V(S_{t+1}) - V(S_t)\\right]\n",
    "  $$\n",
    "- Use mostly greedy moves (highest $V(s)$), with occasional exploration.\n",
    "- Value updates occur **only after greedy moves** (exploration not used for learning).\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "   <img src=\"img/1.png\" alt=\"1\" width=\"60%\"/>\n",
    "   <img src=\"img/2.png\" alt=\"2\" width=\"60%\"/>\n",
    "   <img src=\"img/3.png\" alt=\"3\" width=\"60%\"/>\n",
    "</div>\n",
    "\n",
    "**Takeaways**:\n",
    "- RL learns from interaction without a model of the opponent.\n",
    "- It backs up values from future to past states.\n",
    "- Unlike evolutionary methods, RL learns online, and credit assignment is finer-grained.\n",
    "\n",
    "**Model-Free Nature**:\n",
    "- Doesn’t require modeling opponent or future states.\n",
    "- Works via **temporal-difference (TD) learning**.\n",
    "- Can generalize using function approximators (e.g., Tesauro’s neural network for Backgammon).\n",
    "\n",
    "---\n",
    "\n",
    "## 1.6 Summary\n",
    "\n",
    "- RL is a third paradigm alongside supervised and unsupervised learning.\n",
    "- Uses **Markov Decision Processes (MDPs)** to formalize interaction.\n",
    "- Central tools:\n",
    "  - **Value functions**\n",
    "  - **Trial-and-error updates**\n",
    "- RL = learning by *interacting*, *adapting*, and *optimizing long-term outcomes*.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.7 Early History of Reinforcement Learning\n",
    "\n",
    "**Two foundational threads**:\n",
    "1. **Trial-and-error learning**:\n",
    "   - Roots in psychology: Thorndike’s *Law of Effect*, Skinner’s reinforcement, Pavlov’s conditioning.\n",
    "   - Turing (1948) described pleasure/pain driven learning machines.\n",
    "   - Shannon’s *Theseus*, Michie’s *MENACE*, Samuel’s *checkers program* were early digital examples.\n",
    "\n",
    "2. **Optimal control and dynamic programming**:\n",
    "   - Bellman’s **DP** and **Bellman equations** in 1950s.\n",
    "   - MDPs (Howard, 1960) formalized stochastic control problems.\n",
    "   - Combined with function approximation: \"neurodynamic programming\".\n",
    "\n",
    "**Modern RL** = Integration of:\n",
    "- Dynamic programming theory (optimal control)\n",
    "- Psychological theories of learning\n",
    "- Computational models like TD-learning\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter 1 Exercises\n",
    "\n",
    "<img src=\"img/4.png\" alt=\"4\" width=\"60%\"/>\n",
    "<img src=\"img/5.png\" alt=\"5\" width=\"60%\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
